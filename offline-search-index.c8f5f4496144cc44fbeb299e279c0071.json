





























[{"body":"On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.\nIn this livestream, we discussed a lot of Merbridge-related issues, including a live demo that will help you get a quick overview of Merbridge’s features and usage.\nAlso, the PPT is available here for download.\nIf you are interested, see:\n ","categories":"","description":"","excerpt":"On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.\nIn …","ref":"/blog/2022/03/29/solo-io-livestream/","tags":"","title":"Livestream with Solo.io"},{"body":"2022 年 3 月 29 日，Solo.io 和 Merbridge 共同举办了一场直播活动。\n在这次直播中，我们一起探讨了很多与 Merbridge 相关的问题，其中包含了一个线上 Demo，可以帮你快速了解 Merbridge 的功能和使用方法。\n同时，PPT 可以在这里下载。\n如果您有兴趣，可以查看：\n ","categories":"","description":"","excerpt":"2022 年 3 月 29 日，Solo.io 和 Merbridge 共同举办了一场直播活动。\n在这次直播中， …","ref":"/zh/blog/2022/03/29/solo-io-livestream/","tags":"","title":"与 Solo.io 一起举办的直播活动"},{"body":"一行代码使用 eBPF 代替 iptables 加速 Istio 介绍 以 Istio 为首的服务网格技术正在被越来越多的企业关注，其使用 Sidecar 借助 iptables 技术实现流量拦截，可以处理所有应用的出入口流量，以实现诸如治理、观测、加密等能力。\n但是使用 iptables 的方式进行拦截，由于需要对出入口都拦截，会让原本只需要在内核态处理两次的链路变成四次，会损失不少性能，这在一些要求高性能的场景下显然是有影响的。\n近两年，由于 eBPF 技术的兴起，不少围绕 eBPF 的项目也应声而出，eBPF 在可观测性和网络包的处理方面也有不少优秀的案例。如 Cilium、px.dev 等项目。\n借助 eBPF 的 sockops 和 redir 能力，可以高效的处理数据包，再结合实际场景，那么我们就可以使用 eBPF 去代替 iptables 为 Istio 进行加速。\n现在，我们开源了 Merbridge 项目，只需要在您的 Istio 集群执行以下命令，即可直接使用 eBPF 代替 iptables 实现网络加速！\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml  注意：当前仅支持在 5.7 版本及以上的内核下运行，请事先升级您的内核版本。\n eBPF 的 sockops 加速 网络连接本质上是 socket 的通讯，eBPF 提供了一个 bpf_msg_redirect_hash 函数，用来将应用发出的包，直接转发到对端的 socket 上面，可以极大的加速包在内核中的处理流程。\n这里需要一个 sock_map，需要根据当前的数据包信息，从 sock_map 中挑选一个存在的 socket 连接，转发请求，所以，需要在 sockops 的 hook 处或者其它地方将 socket 信息保存到 sock_map，并提供根据 key 查到 socket 的规则（一般为四元组）。\n原理 下面，将按照实际的场景，逐步的介绍 Merbridge 详细的设计和实现原理，这将让你对 Merbridge 或者 eBPF 有一个初步的了解。\nIstio 基于 iptables 的原理 如上图所示，当外部流量相应访问应用的端口时，会在 iptables 中被 PREROUTING 拦截，最后转发到 Sidecar 容器的 15006 端口，然后交给 Envoy 来进行处理。（图中红色 1 2 3 4 的路径）\nEnvoy 根据从控制平面下发的规则进行处理，处理完成后，会发送请求给实际的容器端口。\n当应用想要访问其它服务时，会在 iptables 中 OUTPUT 拦截，然后转发给 Sidecar 容器的 15001 端口（Envoy 监听）。（图中红色 9 10 11 12 的路径）然后和入口流量处理差不多。\n由此可以看到，原本流量可以直接到应用端口，但是中间需要通过 iptables 转发到 Sidecar，然后又让 Sidecar 发送给应用，这无疑增加了开销。并且，iptables 的通用性决定了它的性能没有很理想。会在整条链路上增加不少延迟。\n如果我们能使用 sockops 去直接连接 Sidecar 到应用的 Socket，这样可以使流量不经过 iptables，可以提高性能。\n出口流量处理 如上所述，我们希望使用 eBPF 的 sockops 来绕过 iptables 以加速网络请求。同时，我们希望创造的是一个能够完全适配社区版 Istio，不做任何改造。所以，我们需要模拟 iptables 所做的操作。\n这个时候我们在看回 iptables 本身，其使用 DNAT 功能做流量转发。\n想要用 eBPF 模拟 iptables 的能力，那么就需要使用 eBPF 实现类似 iptables DNAT 的能力。\n这里主要有两个点：\n 修改连接发起时的目的地址，让流量能够发送到新的接口； 让 Envoy 能识别原始的目的地址，以能够识别流量；  对于其中第一点，我们可以使用 eBPF 的 connect 程序来做，通过修改 user_ip 和 user_port 实现。\n对于其中第二点，需要用到 ORIGINAL_DST 的概念。这个在内核中其实是在 netfilter 模块专属的。\n其原理就是，应用程序（包括 Envoy）会在收到连接之后，调用 get_sockopts 函数，获取 ORIGINAL_DST，如果经过了 iptables 的 DNAT，那么 iptables 就会给当前的 socket 设置这个值，并把原有的 IP + 端口写入这个值，应用程序就可以根据连接拿到原有的目的地址。\n那么我们就需要通过 eBPF 的 get_sockopt 程序来修改这个调用。（不用 **bpf_setsockopt** 的原因是因为目前这个参数并不支持 SO_ORIGINAL_DST` 的 optname）\n参见下图，在应用向外发起请求时，会经过如下阶段：\n 在应用向外发起连接时，connect 程序会将目标地址修改为 127.x.y.z:15001，并用 cookie_original_dst 保存原始目的地址。 在 sockops 程序中，将当前 sock 和四元组保存在 sock_pair_map 中。同时，将四元组信息和对应的原始目的地址写入 pair_original_dst 中（之所以不用 cookie，是因为在 get_sockopt 程序中无法获取当前 cookie）。 Envoy 收到连接之后会调用 getsockopt 获取当前连接的目的地址，get_sockopt 程序会根据四元组信息从 pair_original_dst 取出原始目的地址并返回，由此连接完全建立。 在发送数据阶段，redir 程序会根据四元组信息，从 sock_pair_map 中读取 sock，然后通过 bpf_msg_redirect_hash 进行直`接转发，加速请求。  其中，之所以在 connect 的时候，修改目的地址为 127.x.y.z 而不是 127.0.0.1，是因为在不同的 Pod 中，可能产生冲突的四元组，使用此方式即可巧妙的避开。（每个 Pod 间的目的 IP 就已经不同了，不存在冲突的情况）\n入口流量处理 入口流量处理基本和出口流量类似，唯一差别：只需要将目的地址的端口改成 15006 即可。\n但是，需要注意的是，由于 eBPF 不像 iptables 能在指定命名空间生效，它是全局的，这就造成如果我们将一个本来不是 Istio 所管理的 Pod，或者就是一个外部的 IP 地址，也做了这个操作的话，那就会引起严重问题，会请求直接无法建立连接。\n所以这里我们设计了一个小的控制平面（以 DaemonSet 方式部署），其通过 Watch 所有的 Pod，类似于像 kubelet 那样获取当前节点的 Pod 列表，将已经被注入了 Sidecar 的 Pod IP 地址写入 local_pod_ips 这个 map。\n当我们在做入口流量处理的时候，如果目的地址不在这个列表之中，我们就不做处理，让它走原来的逻辑，这样就可以比较灵活且简单的处理入口流量。\n其他的流程和出口流量流程一样。\n同节点加速 通过入口流量处理，理论上，我们已经可以直接加速同节点的 Envoy 到 Envoy 的加速。但是存在一个问题。就是在这种场景下，Envoy 访问当前 Pod 的应用的时候会出错。\n在 Istio 中，Envoy 访问应用的方式是使用当前 PodIP 加服务端口。经过上面入口流量处理章节，其实我们会发现，由于 PodIP 肯定也存在于 local_pod_ips 中，那么这个请求会被转发到 PodIP + 15006 端口，这显然是不行的，会造成无限递归。\n那么我们也没办法在 eBPF 中获取当前 ns 的 IP 地址信息，怎么办？\n为此，我们设计了一套反馈机制：\n即，在 Envoy 尝试建立连接的时候，我们还是会走重定向到 15006 端口，但是，在 sockops 阶段，我们会判断源 IP 和目的地址 IP是否一致，如果一致，代表发送了错误的请求，那么我们会在 sockops 丢弃这个连接，并将当前的 ProcessID 和 IP 地址信息写入 process_ip 这个 map，让 eBPF 支持进程和 IP 的对应关系。\n当下次请求发送时，我们直接从 process_ip 表检查目的地址是否和当前 IP 地址\n Envoy 会在请求失败的时候重试，且这个错误只会发生一次，后续的连接会非常快。\n 连接关系 在没有使用 Merbridge（eBPF） 优化之前，Pod 到 Pod 间的访问入下图所示：\n在使用 Merbridge（eBPF）优化之后，出入口流量会使用直接跳过很多内核模块，提高性能：\n同时，如果两个 Pod 在同一台机器上，那么他们之间的通讯将更加高效：\n以上，通过使用 eBPF 在主机上对相应的连接进行处理，可以大幅度的减少内核处理流量的流程，提升服务之间的通讯质量。\n加速效果  下面的测试只是一个基本的测试，不是非常严谨。\n 下图展示了使用 eBPF 代替 iptables 之后，整体延迟的情况（越低越好）：\n下图展示了使用 eBPF 代替 iptables 之后，整体 QPS 的情况（越高越好）：\n 以上数据使用 wrk 测试得出。\n Merbridge 项目 以上介绍的都是 Merbridge 项目的核心能力，其通过使用 eBPF 代替 iptables，可以在服务网格场景下，完全无感知的对流量通路进行加速。同时，我们不会对现有的 Istio 做任何修改，原有的逻辑依然畅通，这意味着，如果不再希望使用 eBPF，那么可以直接删除掉 DaemonSet，改为传统的 iptables 方式也不会出任何问题。\nMerbridge 是一个完全独立的开源项目，此时还处于早期阶段，我们希望可以有更多的用户或者开发者参与其中，使用先进的技术能力，优化我们的服务网格。\n项目地址：https://github.com/merbridge/merbridge\n参考文档：\n  https://ebpf.io/\n  https://cilium.io/\n  Merbridge on GitHub\n  Using eBPF instead of iptables to optimize the performance of service grid data plane by Liu Xu, Tencent\n  Sidecar injection and transparent traffic hijacking process in Istio explained in detail by Jimmy Song, Tetrate\n  Accelerate the Istio data plane with eBPF by Yizhou Xu, Intel\n  Envoy’s Original Destination filter\n  ","categories":"","description":"","excerpt":"一行代码使用 eBPF 代替 iptables 加速 Istio 介绍 以 Istio 为首的服务网格技术正在被越来越多的企业关注， …","ref":"/zh/blog/2022/03/01/merbridge-introduce/","tags":"","title":"一行代码，使用 eBPF 代替 iptables 加速服务网格"},{"body":"Merbridge - Accelerate your mesh with eBPF Replacing iptables rules with eBPF allows transporting data directly from inbound sockets to outbound sockets, shortening the datapath between sidecars and services.\nIntroduction The secret of Istio’s abilities in traffic management, security, observability and policy is all in the Envoy proxy. Istio uses Envoy as the “sidecar” to intercept service traffic, with the kernel’s netfilter packet filter functionality configured by iptables.\nThere are shortcomings in using iptables to perform this interception. Since netfilter is a highly versatile tool for filtering packets, several routing rules and data filtering processes are applied before reaching the destination socket. For example, from the network layer to the transport layer, netfilter will be used for processing for several times with the rules predefined, like pre_routing, post_routing and etc. When the packet becomes a TCP packet or UDP packet, and is forwarded to user space, some additional steps like packet validation, protocol policy processing and destination socket searching will be performed. When a sidecar is configured to intercept traffic, the original data path can become very long, since duplicated steps are performed several times.\nOver the past two years, eBPF has become a trending technology, and many projects based on eBPF have been released to the community. Tools like Cilium and Pixie show great use cases for eBPF in observability and network packet processing. With eBPF’s sockops and redir capabilities, data packets can be processed efficiently by directly being transported from an inbound socket to an outbound socket. In an Istio mesh, it is possible to use eBPF to replace iptables rules, and accelerate the data plane by shortening the data path.\nWe have created an open source project called Merbridge, and by applying the following command to your Istio-managed cluster, you can use eBPF to achieve such network acceleration.\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml  Attention: Merbridge uses eBPF functions which require a Linux kernel version ≥ 5.7.\n With Merbridge, the packet datapath can be shortened directly from one socket to another destination socket, and here’s how it works.\nUsing eBPF sockops for performance optimization Network connection is essentially socket communication. eBPF provides a function bpf_msg_redirect_hash, to directly forward the packets sent by the application in the inbound socket to the outbound socket. By entering the function mentioned before, developers can perform any logic to decide the packet destination. According to this characteristic, the datapath of packets can noticeably be optimized in the kernel.\nThe sock_map is the crucial piece in recording information for packet forwarding. When a packet arrives, an existing socket is selected from the sock_map to forward the packet. As a result, we need to save all the socket information for packets to make the transportation process function properly. When there are new socket operations — like a new socket being created — the sock_ops function is executed. The socket metadata is obtained and stored in the sock_map to be used when processing packets. The common key type in the sock_map is a “quadruple” of source and destination addresses and ports. With the key and the rules stored in the map, the destination socket will be found when a new packet arrives.\nThe Merbridge approach Let’s introduce the detailed design and implementation principles of Merbridge step by step, with a real scenario.\nIstio sidecar traffic interception based on iptables When external traffic hits your application’s ports, it will be intercepted by a PREROUTING rule in iptables, forwarded to port 15006 of the sidecar container, and handed over to Envoy for processing. This is shown as steps 1-4 in the red path in the above diagram.\nEnvoy processes the traffic using the policies issued by the Istio control plane. If allowed, the traffic will be sent to the actual container port of the application container.\nWhen the application tries to access other services, it will be intercepted by an OUTPUT rule in iptables, and then be forwarded to port 15001 of the sidecar container, where Envoy is listening. This is steps 9-12 in the red path, similar to inbound traffic processing.\nTraffic to the application port needs to be forwarded to the sidecar, then sent to the container port from the sidecar port, which is overhead. Moreover, iptables’ versatility determines that its performance is not always ideal because it inevitably adds delays to the whole datapath with different filtering rules applied. Although iptables is the common way to do packet filtering, in the Envoy proxy case, the longer datapath amplifies the bottleneck of packet filtering process in the kernel.\nIf we use sockops to directly connect the sidecar’s socket to the application’s socket, the traffic will not need to go through iptables rules, and thus performance can be improved.\nProcessing outbound traffic As mentioned above, we would like to use eBPF’s sockops to bypass iptables to accelerate network requests. At the same time, we also do not want to modify any parts of Istio, to make Merbridge fully adaptive to the community version. As a result, we need to simulate what iptables does in eBPF.\nTraffic redirection in iptables utilizes its DNAT function. When trying to simulate the capabilities of iptables using eBPF, there are two main things we need to do:\n Modify the destination address, when the connection is initiated, so that traffic can be sent to the new interface. Enable Envoy to identify the original destination address, to be able to identify the traffic.  For the first part, we can use eBPF’s connect program to process it, by modifying user_ip and user_port.\nFor the second part, we need to understand the concept of ORIGINAL_DST which belongs to the netfilter module in the kernel.\nWhen an application (including Envoy) receives a connection, it will call the get_sockopt function to obtain ORIGINAL_DST. If going through the iptables DNAT process, iptables will set this parameter, with the “original IP + port” value, to the current socket. Thus, the application can get the original destination address according to the connection.\nWe have to modify this call process through eBPF’s get_sockopts function. (bpf_setsockopt is not used here because this parameter does not currently support the optname of SO_ORIGINAL_DST).\nReferring to the figure below, when an application initiates a request, it will go through the following steps:\n When the application initiates a connection, the connect program will modify the destination address to 127.x.y.z:15001, and use cookie_original_dst to save the original destination address. In the sockops program, the current socket information and the quadruple are saved in sock_pair_map. At the same time, the same quadruple and its corresponding original destination address will be written to pair_original_dst. (Cookie is not used here because it cannot be obtained in the get_sockopt program). After Envoy receives the connection, it will call the get_sockopt function to read the destination address of the current connection. get_sockopt will extract and return the original destination address from pair_original_dst, according to the quadruple information. Thus, the connection is completely established. In the data transport step, the redir program will read the sock information from sock_pair_map according to the quadruple information, and then forward it directly through bpf_msg_redirect_hash to speed up the request.  Why do we set the destination address to 127.x.y.z instead of 127.0.0.1? When different pods exist, there might be conflicting quadruples, and this gracefully avoids conflict. (Pods’ IPs are different, and they will not be in the conflicting condition at any time.)\nInbound traffic processing The processing of inbound traffic is basically similar to outbound traffic, with the only difference: revising the port of the destination to 15006.\nIt should be noted that since eBPF cannot take effect in a specified namespace like iptables, the change will be global, which means that if we use a Pod that is not originally managed by Istio, or an external IP address, serious problems will be encountered — like the connection not being established at all.\nAs a result, we designed a tiny control plane (deployed as a DaemonSet), which watches all pods — similar to the kubelet watching pods on the node — to write the pod IP addresses that have been injected into the sidecar to the local_pod_ips map.\nWhen processing inbound traffic, if the destination address is not in the map, we will not do anything to the traffic.\nThe other steps are the same as for outbound traffic.\nSame-node acceleration Theoretically, acceleration between Envoy sidecars on the same node can be achieved directly through inbound traffic processing. However, Envoy will raise an error when accessing the application of the current pod in this scenario.\nIn Istio, Envoy accesses the application by using the current pod IP and port number. With the above scenario, we realized that the pod IP exists in the local_pod_ips map as well, and the traffic will be redirected to the pod IP on port 15006 again because it is the same address that the inbound traffic comes from. Redirecting to the same inbound address causes an infinite loop.\nHere comes the question: are there any ways to get the IP address in the current namespace with eBPF? The answer is yes!\nWe have designed a feedback mechanism: When Envoy tries to establish the connection, we redirect it to port 15006. However, in the sockops step, we will determine if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard this connection in the sockops process. In the meantime, the current ProcessID and IP information will be written into the process_ip map, to allow eBPF to support correspondence between processes and IPs.\nWhen the next request is sent, the same process need not be performed again. We will check directly from the process_ip map if the destination address is the same as the current IP address.\n Envoy will retry when the request fails, and this retry process will only occur once, meaning subsequent requests will be accelerated.\n Connection relationship Before applying eBPF using Merbridge, the data path between pods is like:\nAfter applying Merbridge, the outbound traffic will skip many filter steps to improve the performance:\nIf two pods are on the same machine, the connection can even be faster:\nPerformance results  The below tests are from our development, and not yet validated in production use cases.\n Let’s see the effect on overall latency using eBPF instead of iptables (lower is better):\nWe can also see overall QPS after using eBPF (higher is better):\n Test results are generated with wrk.\n Summary We have introduced the core ideas of Merbridge in this post. By replacing iptables with eBPF, the data transportation process can be accelerated in a mesh scenario. At the same time, Istio will not be changed at all. This means if you do not want to use eBPF any more, just delete the DaemonSet, and the datapath will be reverted to the traditional iptables-based routing without any problems.\nMerbridge is a completely independent open source project. It is still at an early stage, and we are looking forward to having more users and developers to get engaged. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!\nMerbridge Project: https://github.com/merbridge/merbridge\nSee also   https://ebpf.io/\n  https://cilium.io/\n  Merbridge on GitHub\n  Using eBPF instead of iptables to optimize the performance of service grid data plane by Liu Xu, Tencent\n  Sidecar injection and transparent traffic hijacking process in Istio explained in detail by Jimmy Song, Tetrate\n  Accelerate the Istio data plane with eBPF by Yizhou Xu, Intel\n  Envoy’s Original Destination filter\n  ","categories":"","description":"","excerpt":"Merbridge - Accelerate your mesh with eBPF Replacing iptables rules …","ref":"/blog/1/01/01/merbridge-introduce/","tags":"","title":"Merbridge - Accelerate your mesh with eBPF"},{"body":" Obtain a general view of Merbridge.\n What is Merbridge Merbridge is designed to make traffic interception and forwarding more efficient for service meshes by replacing iptables with eBPF.\nWhy Merbridge In the service mesh scenario, in order to use sidecars for silent traffic management, we need to forward the inbound and outbound traffic of a Pod to sidecars.\nThe most common solution to this is to use the REDIRECT feature of iptables (netfilter) to redirect the original traffic to a sidecar for traffic management and other purposes.\nFor example, the inbound traffic that originally flows directly to the application is now forwarded by iptables (netfilter) to the sidecar, which then forwards the traffic to its destination.\nHowever, netfilter is not performing this job well. eBPF’s ability for similar purposes makes Merbridge possible.\nWe hope to replace iptables (netfilter) with eBPF to make service meshes more efficient.\nWhen to use Merbridge Merbridge is recommended if you have following problems:\n iptables increase latency when high-performance connection is needed. Your system cannot use iptables for certain reasons. Ordinary Pods cannot have too many permissions for certain reasons (e.g., the init container may require NET_ADMIN and other permissions).  ","categories":"","description":"This page provides an overview of Merbridge.\n","excerpt":"This page provides an overview of Merbridge.\n","ref":"/docs/overview/","tags":"","title":"Overview"},{"body":"什么是 Merbridge Merbridge 是一个专门为服务网格设计，用于代替传统的 iptables 流量劫持技术，让服务网格的流量拦截和转发能力更加高效。\neBPF (extended Berkeley Packet Filter) 是一种可以在 Linux 内核中运行用户编写的程序且不需要修改内核代码或加载内核模块的技术，目前被广泛用于网络、安全、监控等领域。相比传统的 iptables 流量劫持技术，Merbridge 可以绕过很多内核模块，缩短边车和服务间的数据路径，从而起到网络加速的作用。同时，Merbridge 没有对现有的 Istio 作出任何修改，原有的逻辑依然畅通。这意味着，如果您不想继续使用 eBPF，可以直接删除 DaemonSet，改为传统的 iptables 方式也不会出现任何问题。\nMerbridge 有哪些特性 Merbridge 的核心特性包括：\n  入口流量处理\nMerbridge 使用 eBPF 的 connect 程序，通过修改 user_ip 和 user_port 修改连接发起时的目的地址，让流量能够发送到新的接口。为了让 Envoy 识别出原始的目的地址，应用程序（包括 Envoy）会在收到连接之后调用 get_sockopts 函数，获取 ORIGINAL_DST。\n  出口流量处理\n出口流量处理与入口流量处理大体类似。需要注意的是，eBPF 是全局性的，不能在指定的命名空间生效。因此，如果对本来不由 Istio 管理的 Pod 或者一个外部的 IP 地址执行此操作，就会导致请求无法建立连接。所以这里设计了一个小的控制平面（以 DaemonSet 方式部署），通过 Watch 所有的 Pod，类似于 kubelet 那样获取当前节点的 Pod 列表，将已经被注入 Sidecar 的 Pod IP 地址写入 local_pod_ips map。如果目的地址不在这个列表之中， Merbridge 就不做处理，而使用原来的逻辑。这样就可以灵活且简单地处理入口流量。\n  同节点加速\n在 Istio 中，Envoy 访问应用的方式是使用当前 PodIP 加服务端口。由于 PodIP 肯定也存在于 local_pod_ips 中，那么请求就会被转发到 PodIP + 15006 端口。这会造成无限递归，不能在 eBPF 获取当前命名空间的 IP 地址信息。因此，需要一套反馈机制。在 Envoy 尝试建立连接时，仍然重定向到 15006 端口，但在 sockops 阶段判断源 IP 和目的地址 IP是否一致。如果一致，则说明发送了错误的请求，需要在 sockops 丢弃这个连接，并将当前的 ProcessID 和 IP 地址信息写入 process_ip map，让 eBPF 支持进程和 IP 的对应关系。下次请求发送时直接从 process_ip 表检查目的地址是否与当前 IP 地址一致。Envoy 会在请求失败的时候重试，且这个错误只会发生一次，后续的连接会非常快。\n  为什么需要 Merbridge 在服务网格的场景中，为了能够在应用完全无感知的情况下利用边车进行流量治理，需要把 Pod 的出入口流量都转发到边车进行治理。在这种情况下，最通用的技术就是使用 iptables(netfilter) 的 Redirect 能力，将原本的流量转发到边车，从而实现流量治理和其它相应功能。这种方案的缺点是增加了网络延迟，因为 iptables 对出口流量和入口流量都进行拦截。以入口流量为例，原本直接流向应用的流量，需要先由 iptables(netfilter) 转发到边车，再由边车将流量转发到实际的应用。原本只需要在内核态处理两次的链路如今变成四次，损失了不少性能。\n幸运的是，eBPF 技术提供了一个 bpf_msg_redirect_hash 函数，可以将应用发出的数据包直接转发到对端的 socket 上面，从而极大地加速数据包在内核中的处理流程。我们希望用 eBPF 技术代替 iptables(netfilter)，提高服务网格的效率。于是 Merbridge 就这样诞生了。\nMerbridge 适用哪些场景 如果您的环境存在下列任一问题，建议考虑使用 Merbridge：\n 使用 iptables 实现透明劫持需要借助 conntrack 模块实现连接跟踪，在连接数较多的情况下，会造成较大的消耗，同时可能会造成 track 表满的情况。 iptables 全局生效，不能显式的禁止相关联的修改，可管控性比较差。 在大并发、需要高性能连接的场景下，使用 iptables 会增加延迟。 如果系统因为某些原因不能使用 iptables。 某些特殊情况下不能为普通的 Pod 授予太多权限（如 init 容器可能需要 NET_ADMIN 等权限）。　  使用 Merbridge 后连接关系有什么变化 使用 eBPF 在主机上对相应的连接进行处理，可以大幅度减少内核处理流量的流程，提升服务之间的通讯质量。\n 如果不用 Merbridge (eBPF)，Pod 到 Pod 间的访问连接关系如下图所示：  使用 Merbridge (eBPF) 优化之后，处理出入口流量时会跳过很多内核模块，从而加速网络。  如果两个 Pod 在同一台机器上，使用 Merbridge (eBPF) 能让 Pod 之间的通讯更加高效。   Merbridge 是一个完全独立的开源项目（Github 地址：https://github.com/merbridge/merbridge），但目前仍处于早期阶段。希望有更多的用户或开发者参与其中，不断完善 Merbridge，共同优化服务网格。　","categories":"","description":"希望您阅读本页后，能够对 Merbridge 有一个大体的了解。\n","excerpt":"希望您阅读本页后，能够对 Merbridge 有一个大体的了解。\n","ref":"/zh/docs/overview/","tags":"","title":"概览"},{"body":"Prerequisites  Use kernel 5.7 or a higher version. Check your version with name -r. Activate cgroup2 in your system. Check the status with mount | grep cgroup2.  Installation Merbridge can be installed on Istio and Linkerd2 only.\nInstall on Istio Apply the following command to install Merbridge:\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml Install on Linkerd2 Apply the following command to install Merbridge:\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-linkerd.yaml Verification Verify installation Before you start this verification, make sure each Pod of Merbridge is running well. You can check the Pod status in Istio with the following command:\nkubectl -n istio-system get pods If the status of all Pods related to Merbridge is Running, it means Merbridge is successfully installed.\nVerify connection Use the following methods to check the connectivity of Merbridge:\nInstall sleep and helloworld and wait for a complete start: kubectl label ns default istio-injection=enabled kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/sleep/sleep.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/helloworld/helloworld.yaml Conduct curl test: kubectl exec $(kubectl get po -l app=sleep -o=jsonpath='{..metadata.name}') -c sleep -- curl -s -v helloworld:5000/hello If you see words like * Connected to helloworld (127.128.0.1) port 5000 (#0) in the output, it means Merbridge has managed to replace iptables with eBPF for traffic forwarding.\n","categories":"","description":"This page helps you quickly get started with Merbridge.\n","excerpt":"This page helps you quickly get started with Merbridge.\n","ref":"/docs/getting-started/","tags":"","title":"Quick Start"},{"body":"先决条件  确保您系统的内核版本大于等于 5.7，可以使用 uname -r 查看。 确保您系统开启了 cgroup2，可以通过 mount | grep cgroup2 进行验证。  安装 Merbridge 目前支持在 Istio 和 Linkerd2 环境下安装。\n在 Istio 环境安装 目前，您只需要在您的环境中执行以下命令即可安装 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml 在 Linkerd2 环境安装 目前，您只需要在您的环境中执行以下命令即可安装 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-linkerd.yaml 验证 验证安装 要验证 Merbridge 是否能正常工作，首先需要确保 Merbridge 的 Pod 都运行正常，以 Istio 为例，您可以使用以下命令查看 Merbridge 的 Pod 状态：\nkubectl -n istio-system get pods 等待所有 Merbridge 相关 Pod 都为 Running 状态即表示 Merbridge 已经安装成功。\n连接测试 可以按照下面的方案验证 Merbridge 的连接是否正常：\n安装 sleep 和 helloworld 应用，并等待完全启动： kubectl label ns default istio-injection=enabled kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/sleep/sleep.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/helloworld/helloworld.yaml 执行 curl 测试： kubectl exec $(kubectl get po -l app=sleep -o=jsonpath='{..metadata.name}') -c sleep -- curl -s -v helloworld:5000/hello 如果在结果中看到类似 * Connected to helloworld (127.128.0.1) port 5000 (#0) 的字样，表示 Merbridge 已经成功使用 eBPF 代替 iptables 进行转发。\n","categories":"","description":"如何快速开始使用 Merbridge。\n","excerpt":"如何快速开始使用 Merbridge。\n","ref":"/zh/docs/getting-started/","tags":"","title":"快速开始"},{"body":"eBPF The full name of eBPF is Extended Berkeley Packet Filter. As the name implies, this is a module used to filter network packets. For example, sockops and redir capabilities of eBPF can efficiently filter and intercept packets.\neBPF is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in an operating system kernel. It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules\niptables iptables is a traffic filter built on netfilter. It implements traffic filtering and interception by registering hook functions on the mount point of netfilter. From the name of iptables, we can guess it may contain some tables. In practice, by mounting rule tables on different chains of netfilter, iptables can filter or modify the traffic packets entering and leaving the kernel protocol stack.\niptables has 4 tables by default:\n Filter NAT Raw Mangle  iptables has 5 chains by default:\n INPUT chain (ingress rules) OUTPUT chain (exgress rules) FORWARD chain (rules of forwarding) PREROUTING chain (rules before routing) POSTROUTING chain (rules after routing)  Service Mesh A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. A service mesh can guarantee fast, reliable, and secure communication between containerized application infrastructure services. Key capabilities provided by mesh include service discovery, load balancing, secure encryption and authentication, failover, observability, and more. A service mesh typically injects a sidecar proxy into each service instance. These sidcars handle inter-service communication, monitoring, and security. In this way, developers can focus on the development, support, and maintenance of the application code in the service, while the O\u0026M team is responsible for the maintenance of the service mesh and applications. Today, the most well-known service mesh is Istio.\nIstio Istio is a service mesh technology originally open sourced by IBM, Google, and Lyft. It can be layered transparently onto distributed applications and provides all the benefits of a service mesh, such as traffic governance, security, and observability.\nIstio can adapt to all services hosted with on-premises, cloud, Kubernetes containers, and virtual machines. It is typically used with microservices deployed on a Kubernetes platform.\nFundamentally, Istio works by deploying an extended version of Envoy as a sidecar proxy to each microservice. The proxy network it uses forms a data plane of Istio. The configuration and management of these proxies is done in a control plane, providing discovery, configuration, and certificate management for Envoy proxies in the data plane.\nLinkerd Linkerd is the first service mesh launched on the market, but Istio is more popular today.\nLinkerd is an open source, ultra-lightweight service mesh designed by Buoyant for Kubernetes. It is completely rewritten in Rust, which makes it as small, light and safe as possible. It provides runtime debugging, observability, reliability, and safety without code changes in distributed applications.\nLinkerd has three basic components: UI, data plane, and control plane. Linkerd works by installing a set of ultra-light, transparent proxies next to each service instance that automatically handle all traffic to and from the service.\n","categories":"","description":"Learn about some of the key concepts in the Merbridge project.\n","excerpt":"Learn about some of the key concepts in the Merbridge project.\n","ref":"/docs/concepts/","tags":"","title":"Concepts"},{"body":"eBPF eBPF 全称为 Extended Berkeley Packet Filter，顾名思义，这是一个用来过滤网络数据包的模块。例如 eBPF 的 sockops 和 redir 能力，就可以高效地过滤和拦截数据包。\neBPF 是一项起源于 Linux 内核的革命性技术，可以在操作系统的内核中运行沙盒程序，能够安全、有效地扩展 Linux 内核的功能，无需改变内核的源代码，也无需加载内核模块。\niptables iptables 是建立在 netfilter 之上的流量过滤器，通过向 netfilter 的挂载点上注册钩子函数来实现对流量过滤和拦截。从 iptables 这个名字上可以看出有表的概念，iptables 通过把这些规则表挂载在 netfilter 的不同链上，对进出内核协议栈的流量数据包进行过滤或者修改。\niptables 默认有 4 个表：\n Filter 表（数据过滤表） NAT 表（地址转换表） Raw 表（状态跟踪表） Mangle 表（包标记表）  iptables 默认有 5 个链：\n INPUT 链（入站规则） OUTPUT 链（出站规则） FORWARD 链（转发规则） PREROUTING 链（路由前规则） POSTROUTING 链（路由后规则）  Service Mesh 中文名为服务网格，这是一个可配置的低延迟基础设施层，通过 API 接口处理应用服务之间的网络进程间通信。服务网格能确保容器化应用基础结构服务之间的通信快速、可靠和安全。网格提供的关键功能包括服务发现、负载均衡、安全加密和身份验证、故障恢复、可观测性等。 服务网格通常会为每个服务实例注入一个 Sidcar 的代理实例。这些 Sidcar 会处理服务间的通信、监控和安全等问题。这样，开发人员就可以专注于服务中应用代码的开发、支持和维护，而运维团队负责服务网格以及应用的维护工作。 目前最著名的服务网格架构是 Istio。\nIstio Istio 是最初由 IBM、Google 和 Lyft 开源的服务网格技术。它可以透明地分层到分布式应用上，并提供服务网格的所有优点，例如流量治理、安全性和可观测性等。\nIstio 能够适配本地部署、云托管、Kubernetes 容器以及虚拟机上运行的服务程序。通常与 Kubernetes 平台上部署的微服务一起使用。\n从根本上讲，Istio 的工作原理是以 Sidcar 的形式将 Envoy 的扩展版本作为代理布署到每个微服务中。其使用的代理网络构成了 Istio 的数据平面。而这些代理的配置和管理在控制平面完成，为数据平面中的 Envoy 代理提供发现、配置和证书管理。\nLinkerd Linkerd 是市场上出现的第一个服务网格，但目前而言 Istio 的服务网格更受欢迎。\nLinkerd 是 Buoyant 为 Kubernetes 设计的开源、超轻量级的服务网格。用 Rust 语言完全重写，使其尽可能小、轻和安全，它提供了运行时调试、可观测性、可靠性和安全性，而无需在分布式应用中更改代码。\nLinkerd 有三个基本组件：UI、数据平面和控制平面。Linkerd 通过在每个服务实例旁安装一组超轻、透明的代理来工作，这些代理会自动处理进出服务的所有流量。\n","categories":"","description":"了解 Merbridge 项目中一些关键的概念。\n","excerpt":"了解 Merbridge 项目中一些关键的概念。\n","ref":"/zh/docs/concepts/","tags":"","title":"概念"},{"body":"Merbridge is currently hosted on GitHub for open source, and all code-related issues are managed on GitHub.\nIf you have any questions or suggestions about Merbridge, you can create a New Issue on GitHub. We will review and process it as soon as possible.\nIf you find a bug in Merbridge and are interested in helping us fix it, you are more than welcome to share your fix code by creating a Pull Request. We will review and process it as soon as possible.\n","categories":"","description":"This page helps you make contributions to Merbridge.\n","excerpt":"This page helps you make contributions to Merbridge.\n","ref":"/docs/contribution-guidelines/","tags":"","title":"Make Contributions"},{"body":"Merbridge 目前托管在 GitHub 上进行开源，所有与代码相关的事情都在 GitHub 上进行管理。\n如果您对 Merbridge 有疑问，需要我们帮忙解决问题，或者想要提供一些新的功能，可以在 GitHub 上创建一个新的 Issue，我们会及时查看和处理。\n如果您发现了 Merbridge 的 bug，且有兴趣帮助我们进行修复，那么非常欢迎您提交一个 Pull Request，附带上您的修复代码，我们会及时处理您的 PR。\n","categories":"","description":"如何参与到 Merbridge 的贡献中。\n","excerpt":"如何参与到 Merbridge 的贡献中。\n","ref":"/zh/docs/contribution-guidelines/","tags":"","title":"参与贡献"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/2022/","tags":"","title":"2022 年 Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2022/","tags":"","title":"Blogs of 2022"},{"body":" Get all information you need to know about Merbridge.\n Merbridge is an eBPF-based solution that can accelerate the data plane of service meshes, with a shorter packet datapath than iptables.\n","categories":"","description":"","excerpt":" Get all information you need to know about Merbridge.\n Merbridge is …","ref":"/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/releases/","tags":"","title":"Version Release"},{"body":" 帮助您了解如何使用 Merbridge。\n Merbridge 旨在使用 eBPF 代替 iptables 技术，加速服务网格的数据平面。\n","categories":"","description":"","excerpt":" 帮助您了解如何使用 Merbridge。\n Merbridge 旨在使用 eBPF 代替 iptables 技术，加速服务网格的数据平面。 …","ref":"/zh/docs/","tags":"","title":"文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/releases/","tags":"","title":"版本发布"},{"body":"Added  Support passive sockops. (#77) @kebe7jun . Use ingress path for message redirection and forwarding. (#82) @dddddai . Support helm-based deployment of Merbridge (#65) @Xunzhuo .  Fixed  Fix the key size of cookie_original_dst(#75) @dddddai.  ","categories":"","description":"What is new in Version 0.5.0?\n","excerpt":"What is new in Version 0.5.0?\n","ref":"/blog/2022/03/30/release-0.5.0/","tags":"","title":"Release 0.5.0"},{"body":"新增  增加对 passive sockops 的支持。 (#77) @kebe7jun . 使用 ingress path 进行消息重定向转发(#82) @dddddai . 支持使用 helm 部署 Merbridge (#65) @Xunzhuo .  修复  修复 cookie_original_dst 的 key 大小(#75) @dddddai .  ","categories":"","description":"0.5.0 版本更新内容。\n","excerpt":"0.5.0 版本更新内容。\n","ref":"/zh/blog/2022/01/27/release-0.5.0/","tags":"","title":"Release 0.5.0"},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  Merbridge Accelerate your mesh with eBPF        The service mesh technology characterized by Istio is attracting more and more attention from enterprises. It uses sidecars and iptables to intercept traffic with the compromise of performance. Over the past two years, eBPF has become a trending technology, creating many projects based on eBPF.\n    eBPF’s sockops and redir capabilities enable more efficient processing of data packets. Therefore, it is possible to accelerate meshes with eBPF instead of iptables.      We have created an open source project called Merbridge, which allows you to accelerate your meshes with eBPF by applying just one command in your Istio-managed cluster.     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/about/","tags":"","title":"Merbridge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Merbridge Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/","tags":"","title":"Merbridge Blog"},{"body":"","categories":"","description":"This page helps you identify and fix errors in installing Merbridge.\n","excerpt":"This page helps you identify and fix errors in installing Merbridge.\n","ref":"/docs/getting-started/trouble/","tags":"","title":"Installation Troubleshooting"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_top.jpg); } }  Merbridge: use eBPF to improve your mesh performance by reducing latency Learn More   Get Started   Explore Merbridge\n         Merbridge replaces iptables rules with eBPF to intercept traffic, and combines msg_redirect to reduce latency with a shortened datapath between sidecars and services.\nFor details, see: Blog.\n      Documentation  If you have any problems in using Merbridge, Documentation may offer desirable solutions.\n   Make a Contribution  You can create a PR on GitHub to improve Merbridge. Your contribution is a big help！\nRead more …\n   How it Works  If you are interested in details of Merbridge, Blog may tell you more.\nRead more …\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"Merbridge"},{"body":"  #td-cover-block-0 { background-image: url(/zh/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_top.jpg); } }  Merbridge: 使用 eBPF 加速您的服务网格，提高性能，降低延迟。 学习更多   快速开始   在 Merbridge 中探索吧。\n         Merbridge 技术使用 eBPF 技术，在服务网格中，代替 iptables 实现流量拦截，并通过 eBPF 和 msg_redirect 技术，提高 Sidecar 和应用之间的传输速度，降低延迟。\n要了解更多实现细节或者原理，可以参考：Blog。\n      查看文档  如果对 Merbridge 使用有任何问题可以查看我们的文档。\n   欢迎贡献！  可以在 GitHub 上创建一个 PR 进行贡献，我们一直欢迎您的加入！\n更多 …\n   实现原理!  了解 Merbridge 如何实现，可以查阅我们的博客\n更多 …\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/","tags":"","title":"Merbridge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Result"},{"body":"  #td-cover-block-0 { background-image: url(/zh/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  关于 Merbridge 使用 eBPF 技术代替 iptables 加速服务网格。        以 Istio 为首的服务网格技术正在被越来越多的企业所关注，其使用 Sidecar 借助 iptables 技术实现流量拦截，但是使用 iptables 的方式进行拦截会损失不少性能。近两年，由于 eBPF 技术的兴起，不少围绕 eBPF 的项目应声而出。\n    借助 eBPF 的 sockops 和 redir 能力，可以高效的处理数据包，再结合实际场景，我们可以使用 eBPF 去代替 iptables 为 Istio 进行加速。      现在，我们开源了 Merbridge 项目，只需要在您的 Istio 集群执行一条命令，即可直接使用 eBPF 代替 iptables 实现网络加速！     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/about/","tags":"","title":"关于 Merbridge"},{"body":"","categories":"","description":"帮助您如何进行错误排查和诊断。\n","excerpt":"帮助您如何进行错误排查和诊断。\n","ref":"/zh/docs/getting-started/trouble/","tags":"","title":"安装错误排查"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/search/","tags":"","title":"搜索结果"}]
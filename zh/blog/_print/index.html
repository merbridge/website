<!doctype html><html lang=zh class=no-js>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0"><link rel=canonical type=text/html href=/zh/blog/>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<link rel="shortcut icon" href=/favicons/favicon.ico>
<link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16>
<link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32>
<link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36>
<link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48>
<link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72>
<link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96>
<link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144>
<link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192>
<title>Merbridge Blog | Merbridge</title>
<meta name=description content="Merbridge 的官网"><meta property="og:title" content="Merbridge Blog">
<meta property="og:description" content="Merbridge 的官网">
<meta property="og:type" content="website">
<meta property="og:url" content="/zh/blog/"><meta property="og:site_name" content="Merbridge">
<meta itemprop=name content="Merbridge Blog">
<meta itemprop=description content="Merbridge 的官网"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Merbridge Blog">
<meta name=twitter:description content="Merbridge 的官网">
<link rel=preload href=/scss/main.min.957d988e396d495afa3156d4640b99742d961f39c79e92f8d152969969aa9ece.css as=style>
<link href=/scss/main.min.957d988e396d495afa3156d4640b99742d961f39c79e92f8d152969969aa9ece.css rel=stylesheet integrity>
<script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script>
</head>
<body class="td-section td-blog">
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
<a class=navbar-brand href=/zh/>
<span class=navbar-logo><svg id="Graph" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><defs><style>.cls-1{fill:#33de72}.cls-2{fill:#fff}.cls-3{fill:none}</style></defs><g id="Merbridge_-_Graph_-_Color_Light" data-name="Merbridge - Graph - Color Light"><path id="Primary" class="cls-1" d="M440 86.059V454l-48-48V201.941l-44 44V178.059zm-184 184c-34.6-34.6-151.526-151.526-184-184V454l48-48V201.941l136 136 44-44V226.059z"/><path id="Secondary" class="cls-2" d="M212 382l-48 48V245.941l48 48zm0-155.941V48L164 96v82.059zM3e2 48V382l48 48V96z"/><rect id="Frame" class="cls-3" width="512" height="512"/></g></svg></span><span class="text-uppercase font-weight-bold">Merbridge</span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/zh/about/><span>关于</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/zh/docs/><span>文档</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class="nav-link active" href=/zh/blog/><span class=active>Blog</span></a>
</li>
<li class="nav-item dropdown mr-4 d-none d-lg-block">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
中文 Chinese
</a>
<div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/blog/>English</a>
</div>
</li>
</ul>
</div>
<div class="navbar-nav d-none d-lg-block">
<input type=search class="form-control td-search-input" placeholder="&#xf002; 站内搜索…" aria-label=站内搜索… autocomplete=off data-offline-search-index-json-src=/offline-search-index.681fa72294eecb02c76bdc8a79e89f37.json data-offline-search-base-href=/ data-offline-search-max-results=10>
</div>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
</div>
<div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
</div>
<main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.
</p><p>
<a href=/zh/blog/>返回本页常规视图</a>.
</p>
</div>
<h1 class=title>Merbridge Blog</h1>
<ul>
<li><a href=#pg-ea4e888c31a16f912d612ee4ba1570cb>2022 年 Blog</a></li>
<ul>
<li><a href=#pg-084ecb4659c778fbfd1b89e55853320e>Merbridge 支持 Ambient Mesh，无惧 CNI 兼容性！</a></li>
<li><a href=#pg-32ef913748e8f122396febed28845701>Kuma 2.0 集成 Merbridge 降低 12% 的网络延迟</a></li>
<li><a href=#pg-1ca19c17b948e8cd4d6c551fe443f48e>深入 Ambient Mesh - 流量路径</a></li>
<li><a href=#pg-ddae09d9f86ef7a0b6eecd3628b18eb5>Merbridge CNI 模式</a></li>
<li><a href=#pg-f8bd1de928cfdde2faaf3cb4d5cf2dfd>Merbridge 和 Cilium</a></li>
<li><a href=#pg-9c52cc92a2a851c972fc1e7d42cec0af>与 Solo.io 一起举办的直播活动</a></li>
<li><a href=#pg-df869185d0e53dcbdf1283e406176894>一行代码，使用 eBPF 代替 iptables 加速服务网格</a></li>
</ul>
<li><a href=#pg-fbc771f2981eed29c0bc016b2d68a41a>版本发布</a></li>
<ul>
<li><a href=#pg-a6701c9eab58ae39e4b3132e47f0a682>发布 0.7.0</a></li>
<li><a href=#pg-05837f4050931df7b3acefdff81265e8>发布 0.6.0</a></li>
<li><a href=#pg-6fd979268616a6a4b66027d152f363e6>Release 0.5.0</a></li>
</ul>
</ul>
<div class=content>
</div>
</div>
<div class=td-content>
<h1 id=pg-ea4e888c31a16f912d612ee4ba1570cb>2022 年 Blog</h1>
<div class="td-byline mb-4">
</div>
</div>
<div class=td-content>
<h1 id=pg-084ecb4659c778fbfd1b89e55853320e>Merbridge 支持 Ambient Mesh，无惧 CNI 兼容性！</h1>
<div class=lead>此篇博客介绍 Merbridge 支持 Ambient Mesh 的新特性。</div>
<div class="td-byline mb-4">
By <b>Kebe Liu</b> |
<time datetime=2022-11-11 class=text-muted>2022.11.11</time>
</div>
<p>在 <a href=/zh/blog/2022/10/13/ambient-mesh-data-path/>深入 Ambient Mesh - 流量路径</a> 一文中，我们分析了 Ambient Mesh 如何实现在不引入 Sidecar 的情况下，将特定 Pod 的出入口流量转发到 ztunnel 进行处理。我们可以发现，其通过 iptables + TPROXY + 路由表的方式进行实现，路径比较长，理解起来比较复杂。而且，由于其使用 mark 做路由标记，这可能导致在某些同样依赖 CNI 的网络下无法正常工作，或者在一些 Bridge 模式的网络 CNI 下无法工作，会极大的限制 Ambient Mesh 使用场景。</p>
<p>Merbridge 主要场景就是使用 eBPF 代替 iptables 为服务网格应用加速。Ambient Mesh 作为 Istio 服务网格的全新模式，Merbridge 自然也要支持这种新模式。
iptables 技术强大，为很多软件实现了各种功能，但在实际应用中也存在一些缺陷。首先，iptables 使用线性匹配方式，当多个应用使用相同能力时可能会产生冲突，进而导致某些功能不可用。其次，虽然其足够灵活，但是仍旧无法实现像 eBPF 一样自由编程的能力。
所以，使用 eBPF 技术代替 iptables 的能力，帮助 Ambient Mesh 实现流量拦截，这应该是一项令人兴奋的技术。</p>
<h2 id=确定目标>确定目标</h2>
<p>通过 <a href=/zh/blog/2022/10/13/ambient-mesh-data-path/>深入 Ambient Mesh - 流量路径</a> 一文，我们可以知道的是，我们最终的目标有两个：</p>
<ul>
<li>将位于 Ambient Mesh 模式下的 Pod 向外发出的流量，拦截到 ztunnel 的 15001 端口。</li>
<li>当主机程序向 Ambient Mesh 模式下的 Pod 发送流量时，应该将流量重定向到 ztunnel 的 15006 端口。</li>
</ul>
<p>至于其中的 istioin 等网卡，完全是为了配合 Ambient Mesh 原有模式设计的，所以无需关注。</p>
<h2 id=分析难点>分析难点</h2>
<p>在运行模式上，Ambient Mesh 模式和 Sidecar 模式存在很大区别。根据 Istio 官方的定义，将一个 Pod 加入 Ambient Mesh 并不需要重启 Pod，在 Pod 中也不存在 Sidecar 相关进程，这就意味着两个问题：</p>
<ol>
<li>之前 Merbridge 使用 CNI 的方案，让 eBPF 程序能够感知到当前 Pod 的 IP 地址从而做出策略判断的方式将失效，因为 Pod 加入或移除 Ambient Mesh 模式并不会重启，也不会调用 CNI 插件，需要重新考量。</li>
<li>以前的流量拦截，只需要在 eBPF 的 connect 钩子中，将目标地址改为 <code>127.0.0.1:15001</code>，但是在 Ambient Mesh 模式下，需要将目标 IP 换成 ztunnel 的 IP 地址。</li>
</ol>
<p>同时，由于 Ambient Mesh 模式下的 Pod 中，并不存在 Sidecar 相关进程，所以之前 Merbridge 用查看当前 Pod 中是否监听了 15006 等端口的方式也不再适用，需要重新设计方案判断进程的运行环境。</p>
<p>所以，基于上述分析，基本上需要重新设计整个拦截方案，以便让 Merbridge 支持 Ambient Mesh 模式。</p>
<p>总结下来，我们需要做这些事情：</p>
<ul>
<li>重新设计判断 Pod 是否加入 Ambient Mesh 模式的方案</li>
<li>不依赖 CNI，实现 eBPF 感知当前 Pod IP 的方案</li>
<li>让 eBPF 程序知道当前节点的 ztunnel 的 IP 地址方案</li>
</ul>
<h2 id=解决方案>解决方案</h2>
<p>在 0.7.2 版本中，我们引入了使用 cgroup id 加速 connect 程序的性能。通常每一个 Pod 中的容器都有一个对应的 cgroup id，我们可以在 bpf 程序中，通过 <code>bpf_get_current_cgroup_id</code> 函数获取到。通过将 IP 信息写入专用的 <code>cgroup_info_map</code> ，可以优化 connect 程序的运行速度。</p>
<p>之前 CNI 在网络命名空间中监听一个特殊的端口，用于存储 Pod 的信息，而 Ambient Mesh 模式与此不同。在 Ambient Mesh 模式中可以借助 cgroup id，如果能将 cgroup id 与 Pod IP 产生关联，那就可以在 eBPF 中获取到当前 Pod IP 信息。</p>
<p>由于不能依赖 CNI，所以需要改变获取 Pod 状态变更信息的方案。为此，我们通过观测进程的创建和销毁信息来探测本地 Pod 的创建和销毁等操作，创建了一个新的工具，用来观测主机上进程的变更信息：<a href=https://github.com/merbridge/process-watcher>process-watcher 项目</a>。</p>
<p>通过从进程 ID 读取所在的 cgroup id 和 ip 信息并将其写入 <code>cgroup_info_map</code>。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#000>tcg</span> <span style=color:#ce5c00;font-weight:700>:=</span> <span style=color:#000>cgroupInfo</span><span style=color:#000;font-weight:700>{</span>
		<span style=color:#000>ID</span><span style=color:#000;font-weight:700>:</span>            <span style=color:#000>cgroupInode</span><span style=color:#000;font-weight:700>,</span>
		<span style=color:#000>IsInMesh</span><span style=color:#000;font-weight:700>:</span>      <span style=color:#000>in</span><span style=color:#000;font-weight:700>,</span>
		<span style=color:#000>CgroupIp</span><span style=color:#000;font-weight:700>:</span>      <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>4</span><span style=color:#000;font-weight:700>]</span><span style=color:#204a87;font-weight:700>uint32</span><span style=color:#000;font-weight:700>)(</span><span style=color:#000>_ip</span><span style=color:#000;font-weight:700>),</span>
		<span style=color:#000>Flags</span><span style=color:#000;font-weight:700>:</span>         <span style=color:#000>flag</span><span style=color:#000;font-weight:700>,</span>
		<span style=color:#000>DetectedFlags</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>cgrinfo</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>DetectedFlags</span> <span style=color:#000;font-weight:700>|</span> <span style=color:#000>AMBIENT_MESH_MODE_FLAG</span> <span style=color:#000;font-weight:700>|</span> <span style=color:#000>ZTUNNEL_FLAG</span><span style=color:#000;font-weight:700>,</span>
	<span style=color:#000;font-weight:700>}</span>
	<span style=color:#204a87;font-weight:700>return</span> <span style=color:#000>ebpfs</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>GetCgroupInfoMap</span><span style=color:#000;font-weight:700>().</span><span style=color:#000>Update</span><span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>cgroupInode</span><span style=color:#000;font-weight:700>,</span> <span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>tcg</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>ebpf</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>UpdateAny</span><span style=color:#000;font-weight:700>)</span>
</code></pre></div><p>然后在 eBPF 中获取到当前 cgroup 所关联的信息：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#000>__u64</span> <span style=color:#000>cgroup_id</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>bpf_get_current_cgroup_id</span><span style=color:#000;font-weight:700>();</span>
<span style=color:#204a87;font-weight:700>void</span> <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000>info</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>bpf_map_lookup_elem</span><span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>cgroup_info_map</span><span style=color:#000;font-weight:700>,</span> <span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>cgroup_id</span><span style=color:#000;font-weight:700>);</span>
</code></pre></div><p>通过这里，我们可以知道，当前的容器是否启用了 Ambient Mesh 模式、是否在网格中等信息。</p>
<p>其次，对于 ztunnel 的 IP 地址，Istio 通过增加网卡绑定固定 IP 的方式实现，这种方案可能存在冲突的风险，也可能在某些情况下（比如 SNAT）会造成原地址信息丢失的情况。所以 Merbridge 放弃了此方案，直接在控制面获取 ztunnel 的 IP 地址，写入 map，让 eBPF 程序读取（这样速度更快）。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#204a87;font-weight:700>static</span> <span style=color:#204a87;font-weight:700>inline</span> <span style=color:#000>__u32</span> <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000>get_ztunnel_ip</span><span style=color:#000;font-weight:700>()</span>
<span style=color:#000;font-weight:700>{</span>
    <span style=color:#000>__u32</span> <span style=color:#000>ztunnel_ip_key</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ZTUNNEL_KEY</span><span style=color:#000;font-weight:700>;</span>
    <span style=color:#204a87;font-weight:700>return</span> <span style=color:#000;font-weight:700>(</span><span style=color:#000>__u32</span> <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000;font-weight:700>)</span><span style=color:#000>bpf_map_lookup_elem</span><span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>settings</span><span style=color:#000;font-weight:700>,</span> <span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>ztunnel_ip_key</span><span style=color:#000;font-weight:700>);</span>
<span style=color:#000;font-weight:700>}</span>
</code></pre></div><p>然后，可以利用 connect 程序，重写目标地址：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=color:#000>ctx</span><span style=color:#ce5c00;font-weight:700>-&gt;</span><span style=color:#000>user_ip4</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ztunnel_ip</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>3</span><span style=color:#000;font-weight:700>];</span>
<span style=color:#000>ctx</span><span style=color:#ce5c00;font-weight:700>-&gt;</span><span style=color:#000>user_port</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>bpf_htons</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>OUT_REDIRECT_PORT</span><span style=color:#000;font-weight:700>);</span>
</code></pre></div><p>通过与 cgroup id 的关联，可以实现在 eBPF 中获取当前进程所在的 Pod IP 地址，从而进行策略操作，将 Ambient Mesh 模式下 Pod 发出的流量转发到 ztunnel 进行处理，从而实现 Merbridge 在 Ambient Mesh 模式下的兼容。</p>
<p>这将是对所有 CNI 都适配的一种能力，可以避免原有 Ambient Mesh 模式不能很好的在大多数 CNI 模式下无法工作的问题。</p>
<h2 id=使用与反馈>使用与反馈</h2>
<p>由于目前 Ambient 仍处于早期阶段，且 Merbridge 对于 Ambient 模式的支持也相对初级，还有一些问题没有得到很好的解决，所以 Ambient 模式还没有合并入主分支。如果想要体验 Merbridge 代替 iptables 为 Ambient Mesh 实现流量拦截的能力，可以按照如下的方式操作（首先需要安装好 Ambient Mesh 模式的网格）：</p>
<ol>
<li>禁用 Istio CNI（安装时设置 <code>--set components.cni.enabled=false</code> ，或删除 Istio CNI 的 DaemonSet <code>kubectl -n istio-system delete ds istio-cni</code> 的方式）。</li>
<li>删除 ztunnel 的 init 容器（因为它会初始化 iptables 规则、网卡等，而 Merbridge 不需要这个操作）。</li>
<li>使用 <code>kubectl apply -f https://github.com/merbridge/merbridge/raw/ambient/deploy/all-in-one.yaml</code> 安装支持 Merbridge 。</li>
</ol>
<p>等待 Merbridge 就绪之后，即可以使用 Ambient Mesh 的所有能力。</p>
<p>*<strong>注意：</strong></p>
<ol>
<li>当前不支持在 Kind 下使用 Ambient 模式（将在后续支持）；</li>
<li>主机内核版本需要大于等于 5.7；</li>
<li>需要开启 cgroup v2；</li>
<li>此模式也兼容 Sidecar 模式；</li>
<li>Ambient 模式下安装默认会开启 debug 模式，会对性能造成一定影响。</li>
</ol>
<p>更多实现细节可以查看<a href=https://github.com/merbridge/merbridge/tree/ambient>源码</a>。</p>
<p>如果遇到任何问题，可在 <a href=https://join.slack.com/t/merbridge/shared_invite/zt-11uc3z0w7-DMyv42eQ6s5YUxO5mZ5hwQ>Slack</a> 中与我们反馈，或加入我们的技术交流微信群。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-32ef913748e8f122396febed28845701>Kuma 2.0 集成 Merbridge 降低 12% 的网络延迟</h1>
<div class="td-byline mb-4">
By <b>Kebe Liu</b> |
<time datetime=2022-11-08 class=text-muted>2022.11.08</time>
</div>
<p>最近 Kuma 正式<a href=https://kuma.io/blog/2022/kuma-2-0-0/>发布了 2.0.0 版本</a>，宣布了几项重大功能，其中第一条就是 Kuma 使用 eBPF 来加速应用访问。</p>
<p><img src=./imgs/kuma-2.0-release-preview.png alt="Kuma 2.0 发布预览"></p>
<p>根据 Kuma 官方的描述，Kuma 正是使用了 Merbridge，来实现 eBPF 的能力。</p>
<p><img src=./imgs/kuma-2.0-ebpf-vs-iptables.png alt="Kuma 2.0 eBPF 与 iptables 性能对比"></p>
<blockquote>
<p>We are utilizing the Merbridge OSS project within our eBPF capabilities and are very excited that we have been able to contribute back to that library and become co-maintainers. We look forward to working more with the Merbridge team as we continue to explore different areas to include eBPF functionality in Kuma.</p>
</blockquote>
<p>我们非常高兴 Merbridge 作为一个开源项目，能够为 Kuma 提供如此令人兴奋的能力。这意味着，基本不需要任何开销，即可降低网格应用通讯延迟！</p>
<p>Kuma 从 6 月开始，就在着手于集成 Merbridge 项目，尝试使用社区现有的能力来为 Kuma 提供加速能力。</p>
<p>得益于 Merbridge 比较清晰的架构设计，Kuma 在很短的时间内就完成了对 Merbridge 的兼容。非常感谢 Kuma 社区能够为 Merbridge 贡献如此重要的兼容能力，这有助于双方社区共同成长！</p>
<p>截止目前，Merbridge 已经支持了 Istio、Linkerd2 和 Kuma 等主流的服务网格，也计划了很多新的特性，比如 IPv4/IPv6 双栈支持、Ambient Mesh 支持、更低的内核版本要求等。希望 Merbridge 能够被越来越广泛的应用，并且能够真实地帮助到大家。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1ca19c17b948e8cd4d6c551fe443f48e>深入 Ambient Mesh - 流量路径</h1>
<div class=lead>此篇博客介绍 Ambient Mesh 中数据面的流量路径。</div>
<div class="td-byline mb-4">
By <b>Kebe Liu</b> |
<time datetime=2022-10-13 class=text-muted>2022.10.13</time>
</div>
<p>Ambient Mesh 发布已经有一段时间，也有不少文章讲述了其用法和架构。本文将深入梳理数据面流量在 Ambient 模式下的路径，帮助大家全面地理解 Ambient 数据面的实现方案。</p>
<p>在阅读本文之前，请先阅读 <a href=https://istio.io/latest/blog/2022/introducing-ambient-mesh/>Ambient Mesh 介绍</a> 了解 Ambient Mesh 的基本架构。</p>
<blockquote>
<p>为了方便阅读和同步实践，本文使用的环境按照 <a href=https://istio.io/latest/blog/2022/get-started-ambient/>Ambient 使用</a> 的方式进行部署。</p>
</blockquote>
<h2 id=从发起请求的一刻开始>从发起请求的一刻开始</h2>
<p>为了探究流量路径，首先我们分析同在 Ambient 模式下的两个服务互相访问的情况（仅 L4 模式，不同节点）。</p>
<p>在 default 命名空间启用 Ambient 模式后，所有的服务都将具备网格治理的能力。</p>
<p>我们的分析从这条命令开始： <code>kubectl exec deploy/sleep -- curl -s http://productpage:9080/ | head -n1</code></p>
<p>在 Sidecar 模式下，Istio 通过 iptables 进行流量拦截，当在 sleep 的 Pod 中执行 curl 时，流量会被 iptables 转发到 Sidecar 的 15001 端口进行处理。
但是在 Ambient 模式下，在 Pod 中不存在 Sidecar，且开启 Ambient 模式也不需要重启 Pod，那它的请求如何确保被 ztunnel 处理呢？</p>
<h2 id=出口流量拦截>出口流量拦截</h2>
<p>要了解出口流量拦截的方案，我们首先可以看一下控制面组件：</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system get po
NAME                                   READY   STATUS    RESTARTS   AGE
istio-cni-node-5rh5z                   1/1     Running   0          20h
istio-cni-node-qsvsz                   1/1     Running   0          20h
istio-cni-node-wdffp                   1/1     Running   0          20h
istio-ingressgateway-5cfcb57bd-kx9hx   1/1     Running   0          20h
istiod-6b84499b75-ncmn7                1/1     Running   0          20h
ztunnel-nptf6                          1/1     Running   0          20h
ztunnel-vxv4b                          1/1     Running   0          20h
ztunnel-xkz4s                          1/1     Running   0          20h
</code></pre><p>在 Ambient 模式下 istio-cni 变成了默认组件。
而在 Sidecar 模式下，istio-cni 主要是为了避免使用 istio-init 容器处理 iptables 规则而造成权限泄露等情况推出的 CNI 插件。
但是在 Ambient 模式下，理论上不需要 Sidecar，为什么还需要 istio-cni 呢？</p>
<p>我们可以看一下日志：</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system logs istio-cni-node-qsvsz
...
2022-10-12T07:34:33.224957Z	info	ambient	Adding route for reviews-v1-6494d87c7b-zrpks/default: [table 100 10.244.1.4/32 via 192.168.126.2 dev istioin src 10.244.1.1]
2022-10-12T07:34:33.226054Z	info	ambient	Adding pod 'reviews-v2-79857b95b-m4q2g/default' (0ff78312-3a13-4a02-b39d-644bfb91e861) to ipset
2022-10-12T07:34:33.228305Z	info	ambient	Adding route for reviews-v2-79857b95b-m4q2g/default: [table 100 10.244.1.5/32 via 192.168.126.2 dev istioin src 10.244.1.1]
2022-10-12T07:34:33.229967Z	info	ambient	Adding pod 'reviews-v3-75f494fccb-92nq5/default' (e41edf7c-a347-45cb-a144-97492faa77bf) to ipset
2022-10-12T07:34:33.232236Z	info	ambient	Adding route for reviews-v3-75f494fccb-92nq5/default: [table 100 10.244.1.6/32 via 192.168.126.2 dev istioin src 10.244.1.1]
</code></pre><p>我们可以看到，对于在 Ambient 模式下的 Pod，istio-cni 做了两件事情：</p>
<ol>
<li>添加 Pod 到 ipset</li>
<li>添加了一个路由规则到 table 100（后面介绍用途）</li>
</ol>
<p>我们可以在其所在的节点上查看一下 ipset 里面的内容（注意，这里使用 kind 集群，需要用 <code>docker exec</code> 先进入所在主机）：</p>
<pre><code class=language-console data-lang=console>kebe@pc $ docker exec -it ambient-worker2 bash
root@ambient-worker2:/# ipset list
Name: ztunnel-pods-ips
Type: hash:ip
Revision: 0
Header: family inet hashsize 1024 maxelem 65536
Size in memory: 520
References: 1
Number of entries: 5
Members:
10.244.1.5
10.244.1.7
10.244.1.8
10.244.1.4
10.244.1.6
</code></pre><p>我们发现这个 Pod 所在的节点上有一个 ipset，其中保存了很多 IP。这些 IP 是 PodIP：</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl get po -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE              NOMINATED NODE   READINESS GATES
details-v1-76778d6644-wn4d2       1/1     Running   0          20h   10.244.1.9   ambient-worker2   &lt;none&gt;           &lt;none&gt;
notsleep-6d6c8669b5-pngxg         1/1     Running   0          20h   10.244.2.5   ambient-worker    &lt;none&gt;           &lt;none&gt;
productpage-v1-7c548b785b-w9zl6   1/1     Running   0          20h   10.244.1.7   ambient-worker2   &lt;none&gt;           &lt;none&gt;
ratings-v1-85c74b6cb4-57m52       1/1     Running   0          20h   10.244.1.8   ambient-worker2   &lt;none&gt;           &lt;none&gt;
reviews-v1-6494d87c7b-zrpks       1/1     Running   0          20h   10.244.1.4   ambient-worker2   &lt;none&gt;           &lt;none&gt;
reviews-v2-79857b95b-m4q2g        1/1     Running   0          20h   10.244.1.5   ambient-worker2   &lt;none&gt;           &lt;none&gt;
reviews-v3-75f494fccb-92nq5       1/1     Running   0          20h   10.244.1.6   ambient-worker2   &lt;none&gt;           &lt;none&gt;
sleep-7b85956664-z6qh7            1/1     Running   0          20h   10.244.2.4   ambient-worker    &lt;none&gt;           &lt;none&gt;
</code></pre><p>所以，这个 ipset 保存了当前节点上所有处于 Ambient 模式下的 PodIP 列表。</p>
<p>那这个 ipset 在哪可以用到呢？</p>
<p>我们看一下 iptables 规则，可以发现：</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# iptables-save
*mangle
...
-A POSTROUTING -j ztunnel-POSTROUTING
...
-A ztunnel-PREROUTING -p tcp -m set --match-set ztunnel-pods-ips src -j MARK --set-xmark 0x100/0x100
</code></pre><p>通过这个我们知道，当节点上处于 Ambient 模式的 Pod（<code>ztunnel-pods-ips</code> ipset 中）发起请求时，其连接会被打上 <code>0x100/0x100</code> 的标记。</p>
<p>一般在这种情况下，会与路由相关，我们看一下路由规则：</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip rule
0:	from all lookup local
100:	from all fwmark 0x200/0x200 goto 32766
101:	from all fwmark 0x100/0x100 lookup 101
102:	from all fwmark 0x40/0x40 lookup 102
103:	from all lookup 100
32766:	from all lookup main
32767:	from all lookup default
</code></pre><p>可以看到，被标记了 <code>0x100/0x100</code> 的流量会走 table 101 的路由表，我们可以查看路由表：</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip r show table 101
default via 192.168.127.2 dev istioout
10.244.1.2 dev veth5db63c11 scope link
</code></pre><p>可以明显看到，默认网关被换成了 <code>192.168.127.2</code>，且走了 istioout 网卡。</p>
<p>这里就有问题了，<code>192.168.127.2</code> 这个 IP 并不属于 NodeIP、PodIP、ClusterIP 中的任意一种，istioout 网卡默认应该也不存在，那这个 IP 是谁创建的呢？
因为流量最终需要发往 ztunnel，我们可以查看 ztunnel 的配置，看看能否找到答案。</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system get po ztunnel-vxv4b -o yaml
apiVersion: v1
kind: Pod
metadata:
  ...
  name: ztunnel-vxv4b
  namespace: istio-system
	...
spec:
  ...
  initContainers:
  - command:
			...
      OUTBOUND_TUN=istioout
			...
      OUTBOUND_TUN_IP=192.168.127.1
      ZTUNNEL_OUTBOUND_TUN_IP=192.168.127.2

      ip link add name p$INBOUND_TUN type geneve id 1000 remote $HOST_IP
      ip addr add $ZTUNNEL_INBOUND_TUN_IP/$TUN_PREFIX dev p$INBOUND_TUN

      ip link add name p$OUTBOUND_TUN type geneve id 1001 remote $HOST_IP
      ip addr add $ZTUNNEL_OUTBOUND_TUN_IP/$TUN_PREFIX dev p$OUTBOUND_TUN

      ip link set p$INBOUND_TUN up
      ip link set p$OUTBOUND_TUN up
      ...
</code></pre><p>如上，ztunnel 会负责创建 istioout 网卡，我们现在去节点上查看对应网卡。</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip a
11: istioout: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether 0a:ea:4e:e0:8d:26 brd ff:ff:ff:ff:ff:ff
    inet 192.168.127.1/30 brd 192.168.127.3 scope global istioout
       valid_lft forever preferred_lft forever
</code></pre><p>那 <code>192.168.127.2</code> 这个网关 IP 在哪呢？它被分配在了 ztunnel 里面。</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a
Defaulted container &quot;istio-proxy&quot; out of: istio-proxy, istio-init (init)
2: eth0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::448a:46ff:fe72:1d3b/64 scope link
       valid_lft forever preferred_lft forever
4: pistioout: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether c2:d0:18:20:3b:97 brd ff:ff:ff:ff:ff:ff
    inet 192.168.127.2/30 scope global pistioout
       valid_lft forever preferred_lft forever
    inet6 fe80::c0d0:18ff:fe20:3b97/64 scope link
       valid_lft forever preferred_lft forever
</code></pre><p>现在可以看到，流量会到 ztunnel 里面，但此时并没有对流量做任何其它操作，只是简单地路由到了 ztunnel。如何才能让 ztunnel 里面的 Envoy 对流量进行处理呢？</p>
<p>我们继续看一看 ztunnel 的配置，其中写了很多 iptables 规则。我们可以进入 ztunnel 看一下具体的规则：</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save
Defaulted container &quot;istio-proxy&quot; out of: istio-proxy, istio-init (init)
...
*mangle
-A PREROUTING -i pistioout -p tcp -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
...
COMMIT
</code></pre><p>现在可以看到，当流量进入 ztunnel 时，会使用 TPROXY 将流量转入 15001 端口进行处理，此处的 15001 即为 Envoy 实际监听用于处理 Pod 出口流量的端口。
关于 TPROXY，大家可以自行学习相关信息，本文不再赘述。</p>
<p>所以，总结下来，当 Pod 处于 Ambient 模式下，其出口流量路径大致为：</p>
<ol>
<li>从 Pod 里面的进程发起流量。</li>
<li>流量流经所在节点网络，经节点的 iptables 进行标记。</li>
<li>节点上的路由表会将流量转发到当前节点的 ztunnel Pod。</li>
<li>流量到达 ztunnel 时，会经过 iptables 进行 TPROXY 透明代理，将流量交给当前 Pod 中的 Envoy 的 15001 端口进行处理。</li>
</ol>
<p>到此我们可以看出，在 Ambient 模式下，对于 Pod 出口流量的处理相对复杂。路径也比较长，不像 Sidecar 模式，直接在 Pod 内部完成流量转发。</p>
<h2 id=入口流量拦截>入口流量拦截</h2>
<p>有了上面的经验，我们不难发现，Ambient 模式下，对于流量的拦截主要通过 MARK 路由 + TPROXY 的方式，入口流量应该也差不多。</p>
<p>我们采用最简单的方式分析一下。当节点上的进程，或者其他主机上的程序相应访问当前节点上的 Pod 时，流量会经过主机的路由表。
我们查看一下当响应访问 productpage-v1-7c548b785b-w9zl6(10.244.1.7) 时的路由信息：</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip r get 10.244.1.7
10.244.1.7 via 192.168.126.2 dev istioin table 100 src 10.244.1.1 uid 0
    cache
</code></pre><p>我们可以看到，当访问 <code>10.244.1.7</code> 时，流量会被路由到 <code>192.168.126.2</code>，而这条规则正是由上面 istio-cni 添加的。</p>
<p>同样地 <code>192.168.126.2</code> 这个 IP 属于 ztunnel：</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a
Defaulted container &quot;istio-proxy&quot; out of: istio-proxy, istio-init (init)
2: eth0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::448a:46ff:fe72:1d3b/64 scope link
       valid_lft forever preferred_lft forever
3: pistioin: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 7e:b2:e6:f9:a4:92 brd ff:ff:ff:ff:ff:ff
    inet 192.168.126.2/30 scope global pistioin
       valid_lft forever preferred_lft forever
    inet6 fe80::7cb2:e6ff:fef9:a492/64 scope link
       valid_lft forever preferred_lft forever
</code></pre><p>按照相同的分析方法，我们看一下 iptables 规则：</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save
...
-A PREROUTING -i pistioin -p tcp -m tcp --dport 15008 -j TPROXY --on-port 15008 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
-A PREROUTING -i pistioin -p tcp -j TPROXY --on-port 15006 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
...
</code></pre><p>如果直接在节点上访问 PodIP + Pod 端口，流量会被转发到 ztunnel 的 15006 端口，而这就是 Istio 处理入口流量的端口。</p>
<p>至于目标端口为 15008 端口的流量，这是 ztunnel 用来做四层流量隧道的端口。本文暂不细述。</p>
<h2 id=对于-envoy-自身的流量处理>对于 Envoy 自身的流量处理</h2>
<p>我们知道，在 Sidecar 模式下，Envoy 和业务容器运行在相同的网络命名空间中。对于业务容器的流量，我们需要全部拦截，以保证对流量的完全掌控，但是在 Ambient 模式下是否需要呢？</p>
<p>答案是否定的，因为 Envoy 已经被独立到其它 Pod 中，Envoy 发出的流量是不需要特殊处理的。换言之，对于 ztunnel，我们只需要处理入口流量即可，所以 ztunnel 中的规则看起来相对简单。</p>
<h2 id=未完待续>未完待续…</h2>
<p>上面我们主要分析了在 Ambient 模式下对于 Pod 流量拦截的方案，还没有涉及到七层流量的处理以及 ztunnel 实现的具体原理，后续将分析流量在 ztunnel 和 waypoint proxy 中详细的处理路径。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ddae09d9f86ef7a0b6eecd3628b18eb5>Merbridge CNI 模式</h1>
<div class=lead>此篇博客将向您介绍 Merbridge CNI 的工作原理。</div>
<div class="td-byline mb-4">
<time datetime=2022-05-18 class=text-muted>2022.05.18</time>
</div>
<p>Merbridge CNI 模式的出现，旨在能够更好地适配服务网格的功能。之前没有 CNI 模式时，Merbridge 能够做得事情比较有限。其中最大的问题是不能适配注入 Istio 的 Sidecar Annotation，这就导致 Merbridge 无法排除某些端口或 IP 段的流量等。
同时，由于之前 Merbridge 只处理 Pod 内部的连接请求，这就导致，如果是外部发送到 Pod 的流量，Merbridge 将无法处理。</p>
<p>为此，我们精心设计了 Merbridge CNI，旨在解决这些问题。</p>
<h2 id=为什么需要-cni-模式>为什么需要 CNI 模式？</h2>
<p>其一，之前的 Merbridge 只有一个很小的控制面，其监听 Pod 资源，将当前节点的 IP 信息写入 <code>local_pod_ips</code> 的 map，以供 connect 使用。
但是，connect 程序由于工作在主机内核层，其无法知道当前正在处理的是哪个 Pod 的流量，就没法处理如 <code>excludeOutboundPorts</code> 等配置。
为了能够适配注入 <code>excludeOutboundPorts</code> 的 Sidecar Annotation，我们需要让 eBPF 程序能够得知当前正在处理哪个 Pod 的请求。</p>
<p>为此，我们设计了一套方法，与 CNI 配合，能够获取当前 Pod 的 IP，以适配针对 Pod 的特殊配置。</p>
<p>其二，在之前的 Merbridge 版本中，只有 connect 会处理主机发起的请求，这在同一台主机上的 Pod 互相通讯时，是没有问题的。但是在不同主机之间通讯时就会出现问题，因为按照之前的逻辑，在跨节点通讯时流量不会被修改，这会导致在接收端还是离不开 iptables。</p>
<p>这次，我们依靠 XDP 程序，解决入口流量处理的问题。因为 XDP 程序需要挂载网卡，所以也需要借助 CNI。</p>
<h2 id=cni-如何解决问题>CNI 如何解决问题？</h2>
<p>这里我们将探讨 CNI 的工作原理，以及如何使用 CNI 来解决问题。</p>
<h3 id=如何通过-cni-让-ebpf-程序获取当前正在处理的-pod-ip>如何通过 CNI 让 eBPF 程序获取当前正在处理的 Pod IP？</h3>
<p>我们通过 CNI，在 Pod 创建的时候，将 Pod 的 IP 信息写入一个 Map（<code>mark_pod_ips_map</code>），其 Key 为一个随机的值，Value 为 Pod 的 IP。然后，在当前 Pod 的 NetNS 里面监听一个特殊的端口 39807，将 Key 使用 <code>setsockopt</code> 写入这个端口 socket 的 mark。</p>
<p>在 eBPF 中，我们通过 <code>bpf_sk_lookup_tcp</code> 取得端口 39807 的 Mark 信息，然后从 <code>mark_pod_ips_map</code> 中即可取得当前 NetNS（也是当期 Pod）的 IP。</p>
<p>有了当前 Pod IP 之后，我们可以根据这个 Pod 的配置，确认流量处理路径（比如 <code>excludeOutboundPorts</code>）。</p>
<p>同时，我们还使用 Pod 优化了之前解决四元组冲突的方案，改为使用 <code>bpf_bind</code> 绑定源 IP，目的 IP 直接使用 <code>127.0.0.1</code>，为了后续支持 IPv6 做准备。</p>
<h3 id=如何处理入口流量>如何处理入口流量？</h3>
<p>为了能够处理入口流量，我们引入了 XDP 程序，XDP 程序作用在网卡上，能够对原始数据包做修改。
我们借助 XDP 程序，在流量到达 Pod 的时候，修改目的端口为 15006 以完成流量转发。</p>
<p>同时考虑到可能存在主机直接访问 Pod 的情况，也为了减小影响范围，我们选择将 XDP 程序附加到 Pod 的网卡上。这就需要借助 CNI 的能力，在创建 Pod 时进行附加操作。</p>
<h2 id=如何体验-cni-模式>如何体验 CNI 模式？</h2>
<p>CNI 模式默认被关闭，需要手动开启。</p>
<p>可以使用以下命令一键开启：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml <span style=color:#000;font-weight:700>|</span> sed <span style=color:#4e9a06>&#39;s/--cni-mode=false/--cni-mode=true/g&#39;</span> <span style=color:#000;font-weight:700>|</span> kubectl apply -f -
</code></pre></div><h2 id=注意事项>注意事项</h2>
<h3 id=cni-模式处于测试阶段>CNI 模式处于测试阶段</h3>
<p>CNI 模式刚被设计和开发出来，可能存在不少问题，我们欢迎大家在测试阶段进行反馈，或者提出更好的建议，以帮助我们改进 Merbridge！</p>
<p>如果需要使用注入 Istio perf benchmark 等工具进行测试性能，请开启 CNI 模式，否则会导致性能测试结果不准确。</p>
<h3 id=需要注意主机是否可开启-hardware-checksum-能力>需要注意主机是否可开启 hardware-checksum 能力</h3>
<p>为了保证 CNI 模式的正常运行，我们默认关闭了 hardware-checksum 能力，这可能会影响到网络性能。建议大家在开启 CNI 模式前，先确认主机是否可开启 hardware-checksum 能力。如果可以开启，建议设置 <code>--hardware-checksum=true</code> 以获得最佳的性能表现。</p>
<p>测试方法：<code>ethtool -k &lt;网卡> | grep tx-checksum-ipv4</code> 为 on 表示开启。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-f8bd1de928cfdde2faaf3cb4d5cf2dfd>Merbridge 和 Cilium</h1>
<div class="td-byline mb-4">
<time datetime=2022-04-23 class=text-muted>2022.04.23</time>
</div>
<h1 id=merbridge-与-cilium>Merbridge 与 Cilium</h1>
<p><a href=https://cilium.io/>Cilium</a> 是一款基于 eBPF 为云原生应用提供诸多网络能力的优秀开源软件，有很多很棒的设计。例如，Cilium 设计了一套基于 sockmap 的 redir 能力，帮助加速网络通讯，这给了我们很大的启发，也是 Merbridge 提供网络加速的基础，这真是一个非常棒的设计。</p>
<p>Merbridge 借助于 Cilium 打下的良好基础，加上我们在服务网格领域做地一些针对性的适配，让大家可以更加方便地将 eBPF 技术应用于服务网格。</p>
<p>我们的开发团队从 Cilium 提供的资料中学习了很多关于 eBPF 的理论知识、实践方法和测试方法等，也与 Cilium 技术团队多有交流，也就是因为这些经历，才能有 Merbridge 项目的诞生。</p>
<p>再次衷心感谢 Cilium 项目和社区，以及 Cilium 的这些优秀设计。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-9c52cc92a2a851c972fc1e7d42cec0af>与 Solo.io 一起举办的直播活动</h1>
<div class="td-byline mb-4">
<time datetime=2022-03-29 class=text-muted>2022.03.29</time>
</div>
<p>2022 年 3 月 29 日，Solo.io 和 Merbridge 共同举办了一场直播活动。</p>
<p>在这次直播中，我们一起探讨了很多与 Merbridge 相关的问题，其中包含了一个线上 Demo，可以帮你快速了解 Merbridge 的功能和使用方法。</p>
<p>同时，PPT 可以在<a href=./merbridge.pdf>这里</a>下载。</p>
<p>如果您有兴趣，可以查看：</p>
<iframe width=1280 height=720 src=https://www.youtube.com/embed/r2wgInmsqsU title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-df869185d0e53dcbdf1283e406176894>一行代码，使用 eBPF 代替 iptables 加速服务网格</h1>
<div class="td-byline mb-4">
<time datetime=2022-03-01 class=text-muted>2022.03.01</time>
</div>
<h1 id=一行代码使用-ebpf-代替-iptables-加速-istio>一行代码使用 eBPF 代替 iptables 加速 Istio</h1>
<h2 id=介绍>介绍</h2>
<p>以 Istio 为首的服务网格技术正在被越来越多的企业关注，其使用 Sidecar 借助 iptables 技术实现流量拦截，可以处理所有应用的出入口流量，以实现诸如治理、观测、加密等能力。</p>
<p>但是使用 iptables 的方式进行拦截，由于需要对出入口都拦截，会让原本只需要在内核态处理两次的链路变成四次，会损失不少性能，这在一些要求高性能的场景下显然是有影响的。</p>
<p>近两年，由于 eBPF 技术的兴起，不少围绕 eBPF 的项目也应声而出，eBPF 在可观测性和网络包的处理方面也有不少优秀的案例。如 Cilium、px.dev 等项目。</p>
<p>借助 eBPF 的 sockops 和 redir 能力，可以高效的处理数据包，再结合实际场景，那么我们就可以使用 eBPF 去代替 iptables 为 Istio 进行加速。</p>
<p>现在，我们开源了 Merbridge 项目，只需要在您的 Istio 集群执行以下命令，即可直接使用 eBPF 代替 iptables 实现网络加速！</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml
</code></pre></div><blockquote>
<p>注意：当前仅支持在 5.7 版本及以上的内核下运行，请事先升级您的内核版本。</p>
</blockquote>
<h3 id=ebpf-的-sockops-加速>eBPF 的 sockops 加速</h3>
<p>网络连接本质上是 socket 的通讯，eBPF 提供了一个 <a href=https://man7.org/linux/man-pages/man7/bpf-helpers.7.html>bpf_msg_redirect_hash</a> 函数，用来将应用发出的包，直接转发到对端的 socket 上面，可以极大的加速包在内核中的处理流程。</p>
<p>这里需要一个 sock_map，需要根据当前的数据包信息，从 sock_map 中挑选一个存在的 socket 连接，转发请求，所以，需要在 sockops 的 hook 处或者其它地方将 socket 信息保存到 sock_map，并提供根据 key 查到 socket 的规则（一般为四元组）。</p>
<h2 id=原理>原理</h2>
<p>下面，将按照实际的场景，逐步的介绍 Merbridge 详细的设计和实现原理，这将让你对 Merbridge 或者 eBPF 有一个初步的了解。</p>
<h3 id=istio-基于-iptables-的原理>Istio 基于 iptables 的原理</h3>
<p><img src=./imgs/1.png alt="Istio 基于 iptables 的流量拦截原理"></p>
<p>如上图所示，当外部流量相应访问应用的端口时，会在 iptables 中被 PREROUTING 拦截，最后转发到 Sidecar 容器的 15006 端口，然后交给 Envoy 来进行处理。（图中红色 1 2 3 4 的路径）</p>
<p>Envoy 根据从控制平面下发的规则进行处理，处理完成后，会发送请求给实际的容器端口。</p>
<p>当应用想要访问其它服务时，会在 iptables 中 OUTPUT 拦截，然后转发给 Sidecar 容器的 15001 端口（Envoy 监听）。（图中红色 9 10 11 12 的路径）然后和入口流量处理差不多。</p>
<p>由此可以看到，原本流量可以直接到应用端口，但是中间需要通过 iptables 转发到 Sidecar，然后又让 Sidecar 发送给应用，这无疑增加了开销。并且，iptables 的通用性决定了它的性能没有很理想。会在整条链路上增加不少延迟。</p>
<p>如果我们能使用 sockops 去直接连接 Sidecar 到应用的 Socket，这样可以使流量不经过 iptables，可以提高性能。</p>
<h3 id=出口流量处理>出口流量处理</h3>
<p>如上所述，我们希望使用 eBPF 的 sockops 来绕过 iptables 以加速网络请求。同时，我们希望创造的是一个能够完全适配社区版 Istio，不做任何改造。所以，我们需要模拟 iptables 所做的操作。</p>
<p>这个时候我们在看回 iptables 本身，其使用 DNAT 功能做流量转发。</p>
<p>想要用 eBPF 模拟 iptables 的能力，那么就需要使用 eBPF 实现类似 iptables DNAT 的能力。</p>
<p>这里主要有两个点：</p>
<ol>
<li>修改连接发起时的目的地址，让流量能够发送到新的接口；</li>
<li>让 Envoy 能识别原始的目的地址，以能够识别流量；</li>
</ol>
<p>对于其中第一点，我们可以使用 eBPF 的 connect 程序来做，通过修改 <code>user_ip</code> 和 <code>user_port</code> 实现。</p>
<p>对于其中第二点，需要用到 ORIGINAL_DST 的概念。这个在内核中其实是在 netfilter 模块专属的。</p>
<p>其原理就是，应用程序（包括 Envoy）会在收到连接之后，调用 get_sockopts 函数，获取 ORIGINAL_DST，如果经过了 iptables 的 DNAT，那么 iptables 就会给当前的 socket 设置这个值，并把原有的 IP + 端口写入这个值，应用程序就可以根据连接拿到原有的目的地址。</p>
<p>那么我们就需要通过 eBPF 的 <code>get_sockopt</code> 程序来修改这个调用。（不用 **<code>bpf</code>_setsockopt<code>** 的原因是因为目前这个参数并不支持 </code>SO_ORIGINAL_DST` 的 optname）</p>
<p>参见下图，在应用向外发起请求时，会经过如下阶段：</p>
<ol>
<li>在应用向外发起连接时，connect 程序会将目标地址修改为 <code>127.x.y.z:15001</code>，并用 <code>cookie_original_dst</code> 保存原始目的地址。</li>
<li>在 sockops 程序中，将当前 sock 和四元组保存在 <code>sock_pair_map</code> 中。同时，将四元组信息和对应的原始目的地址写入 <code>pair_original_dst</code> 中（之所以不用 cookie，是因为在 get_sockopt 程序中无法获取当前 cookie）。</li>
<li>Envoy 收到连接之后会调用 getsockopt 获取当前连接的目的地址，get_sockopt 程序会根据四元组信息从 <code>pair_original_dst</code> 取出原始目的地址并返回，由此连接完全建立。</li>
<li>在发送数据阶段，redir 程序会根据四元组信息，从 <code>sock_pair_map</code> 中读取 sock，然后通过 <code>bpf_msg_redirect_hash</code> 进行直`接转发，加速请求。</li>
</ol>
<p><img src=./imgs/2.png alt=出口流量处理></p>
<p>其中，之所以在 connect 的时候，修改目的地址为 127.x.y.z 而不是 127.0.0.1，是因为在不同的 Pod 中，可能产生冲突的四元组，使用此方式即可巧妙的避开。（每个 Pod 间的目的 IP 就已经不同了，不存在冲突的情况）</p>
<h3 id=入口流量处理>入口流量处理</h3>
<p>入口流量处理基本和出口流量类似，唯一差别：只需要将目的地址的端口改成 15006 即可。</p>
<p>但是，需要注意的是，由于 eBPF 不像 iptables 能在指定命名空间生效，它是全局的，这就造成如果我们将一个本来不是 Istio 所管理的 Pod，或者就是一个外部的 IP 地址，也做了这个操作的话，那就会引起严重问题，会请求直接无法建立连接。</p>
<p>所以这里我们设计了一个小的控制平面（以 DaemonSet 方式部署），其通过 Watch 所有的 Pod，类似于像 kubelet 那样获取当前节点的 Pod 列表，将已经被注入了 Sidecar 的 Pod IP 地址写入 <code>local_pod_ips</code> 这个 map。</p>
<p>当我们在做入口流量处理的时候，如果目的地址不在这个列表之中，我们就不做处理，让它走原来的逻辑，这样就可以比较灵活且简单的处理入口流量。</p>
<p>其他的流程和出口流量流程一样。</p>
<p><img src=./imgs/3.png alt=入口流量处理></p>
<h3 id=同节点加速>同节点加速</h3>
<p>通过入口流量处理，理论上，我们已经可以直接加速同节点的 Envoy 到 Envoy 的加速。但是存在一个问题。就是在这种场景下，Envoy 访问当前 Pod 的应用的时候会出错。</p>
<p>在 Istio 中，Envoy 访问应用的方式是使用当前 PodIP 加服务端口。经过上面入口流量处理章节，其实我们会发现，由于 PodIP 肯定也存在于 <code>local_pod_ips</code> 中，那么这个请求会被转发到 PodIP + 15006 端口，这显然是不行的，会造成无限递归。</p>
<p>那么我们也没办法在 eBPF 中获取当前 ns 的 IP 地址信息，怎么办？</p>
<p>为此，我们设计了一套反馈机制：</p>
<p>即，在 Envoy 尝试建立连接的时候，我们还是会走重定向到 15006 端口，但是，在 sockops 阶段，我们会判断源 IP 和目的地址 IP是否一致，如果一致，代表发送了错误的请求，那么我们会在 sockops 丢弃这个连接，并将当前的 ProcessID 和 IP 地址信息写入 <code>process_ip</code> 这个 map，让 eBPF 支持进程和 IP 的对应关系。</p>
<p>当下次请求发送时，我们直接从 <code>process_ip</code> 表检查目的地址是否和当前 IP 地址</p>
<blockquote>
<p>Envoy 会在请求失败的时候重试，且这个错误只会发生一次，后续的连接会非常快。</p>
</blockquote>
<p><img src=./imgs/4.png alt=同节点加速></p>
<h3 id=连接关系>连接关系</h3>
<p>在没有使用 Merbridge（eBPF） 优化之前，Pod 到 Pod 间的访问入下图所示：</p>
<p><img src=./imgs/5.png alt="iptable 路径"></p>
<blockquote>
<p>图片参考：<a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></p>
</blockquote>
<p>在使用 Merbridge（eBPF）优化之后，出入口流量会使用直接跳过很多内核模块，提高性能：</p>
<p><img src=./imgs/6.png alt="eBPF 路径"></p>
<blockquote>
<p>图片参考：<a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></p>
</blockquote>
<p>同时，如果两个 Pod 在同一台机器上，那么他们之间的通讯将更加高效：</p>
<p><img src=./imgs/7.png alt="同节点 eBPF 路径"></p>
<blockquote>
<p>图片参考：<a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></p>
</blockquote>
<p>以上，通过使用 eBPF 在主机上对相应的连接进行处理，可以大幅度的减少内核处理流量的流程，提升服务之间的通讯质量。</p>
<h2 id=加速效果>加速效果</h2>
<blockquote>
<p>下面的测试只是一个基本的测试，不是非常严谨。</p>
</blockquote>
<p>下图展示了使用 eBPF 代替 iptables 之后，整体延迟的情况（越低越好）：</p>
<p><img src=./imgs/8.png alt=延迟与连接数></p>
<p>下图展示了使用 eBPF 代替 iptables 之后，整体 QPS 的情况（越高越好）：</p>
<p><img src=./imgs/9.png alt="延迟与 QPS"></p>
<blockquote>
<p>以上数据使用 wrk 测试得出。</p>
</blockquote>
<h2 id=merbridge-项目>Merbridge 项目</h2>
<p>以上介绍的都是 Merbridge 项目的核心能力，其通过使用 eBPF 代替 iptables，可以在服务网格场景下，完全无感知的对流量通路进行加速。同时，我们不会对现有的 Istio 做任何修改，原有的逻辑依然畅通，这意味着，如果不再希望使用 eBPF，那么可以直接删除掉 DaemonSet，改为传统的 iptables 方式也不会出任何问题。</p>
<p>Merbridge 是一个完全独立的开源项目，此时还处于早期阶段，我们希望可以有更多的用户或者开发者参与其中，使用先进的技术能力，优化我们的服务网格。</p>
<p>项目地址：<a href=https://github.com/merbridge/merbridge>https://github.com/merbridge/merbridge</a></p>
<p>参考文档：</p>
<ul>
<li><a href=https://ebpf.io/>eBPF</a></li>
<li><a href=https://cilium.io/>Cilium</a></li>
<li><a href=https://github.com/merbridge/merbridge>Merbridge on GitHub</a></li>
<li><a href=https://developpaper.com/kubecon-2021-%EF%BD%9C-using-ebpf-instead-of-iptables-to-optimize-the-performance-of-service-grid-data-plane/>Using eBPF instead of iptables to optimize the performance of service grid data plane</a> by Liu Xu, Tencent</li>
<li><a href=https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/>Sidecar injection and transparent traffic hijacking process in Istio explained in detail</a> by Jimmy Song, Tetrate</li>
<li><a href=https://01.org/blogs/xuyizhou/2021/accelerate-istio-dataplane-ebpf-part-1>Accelerate the Istio data plane with eBPF</a> by Yizhou Xu, Intel</li>
<li><a href=https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/original_dst_filter>Envoy&rsquo;s Original Destination filter</a></li>
<li><a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-fbc771f2981eed29c0bc016b2d68a41a>版本发布</h1>
<div class="td-byline mb-4">
</div>
</div>
<div class=td-content>
<h1 id=pg-a6701c9eab58ae39e4b3132e47f0a682>发布 0.7.0</h1>
<div class=lead>0.7.0 版本更新内容。</div>
<div class="td-byline mb-4">
<time datetime=2022-07-20 class=text-muted>2022.07.20</time>
</div>
<p><strong>我们很高兴地发布 Merbridge 0.7.0！</strong></p>
<p>本次更新主要体现在两个方面：</p>
<ol>
<li>使用 <code>tc</code>（Traffic Control）代替 XDP 来做容器的出入口流量处理，规避 XDP Generic 模式存在的问题，并且可用于生产环境。</li>
<li>增加对 Kuma 的支持，可以在 Kuma 中使用和 Istio 相同的能力。感谢 Kuma 工程师 <a href=https://github.com/bartsmykla>@bartsmykla</a> 的贡献。</li>
</ol>
<p>有关其他更新内容，请参考：<a href=https://github.com/merbridge/merbridge/releases/tag/0.7.0>Merbridge 0.7.0</a></p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-05837f4050931df7b3acefdff81265e8>发布 0.6.0</h1>
<div class=lead>0.6.0 版本更新内容。</div>
<div class="td-byline mb-4">
<time datetime=2022-05-23 class=text-muted>2022.05.23</time>
</div>
<p><strong>我们很高兴地发布 Merbridge 0.6.0！</strong></p>
<p>在这个版本中，我们首次引入了 CNI 模式，用于支持全量的 Istio 流量转发相关的能力。有关 CNI 模式的支持功能，请参考 <a href=/zh/blog/2022/05/18/cni-mode/>Merbridge CNI 模式</a></p>
<p>有关版本发布说明，请参考：<a href=https://github.com/merbridge/merbridge/releases/tag/0.6.0>Merbridge 0.6.0</a></p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-6fd979268616a6a4b66027d152f363e6>Release 0.5.0</h1>
<div class=lead>0.5.0 版本更新内容。</div>
<div class="td-byline mb-4">
<time datetime=2022-01-27 class=text-muted>2022.01.27</time>
</div>
<h2 id=新增>新增</h2>
<ol>
<li>增加对 passive sockops 的支持。 (<a href=https://github.com/merbridge/merbridge/pull/77>#77</a>) <a href=https://github.com/kebe7jun>@kebe7jun</a> .</li>
<li>使用 ingress path 进行消息重定向转发(<a href=https://github.com/merbridge/merbridge/pull/82>#82</a>) <a href=https://github.com/dddddai>@dddddai</a> .</li>
<li>支持使用 helm 部署 Merbridge (<a href=https://github.com/merbridge/merbridge/pull/65>#65</a>) <a href=https://github.com/Xunzhuo>@Xunzhuo</a> .</li>
</ol>
<h2 id=修复>修复</h2>
<ol>
<li>修复 <code>cookie_original_dst</code> 的 key 大小(<a href=https://github.com/merbridge/merbridge/pull/75>#75</a>) <a href=https://github.com/dddddai>@dddddai</a> .</li>
</ol>
</div>
</main>
</div>
</div>
<footer class="bg-dark py-5 row d-print-none">
<div class="container-fluid mx-sm-5">
<div class=row>
<div class="col-6 col-sm-4 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank rel=noopener href=https://groups.google.com/g/merbridge aria-label="User mailing list">
<i class="fa fa-envelope"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank rel=noopener href=https://github.com/merbridge/merbridge aria-label=GitHub>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank rel=noopener href=https://join.slack.com/t/merbridge/shared_invite/zt-11uc3z0w7-DMyv42eQ6s5YUxO5mZ5hwQ aria-label=Slack>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Developer mailing list" aria-label="Developer mailing list">
<a class=text-white target=_blank rel=noopener href=https://groups.google.com/g/merbridge aria-label="Developer mailing list">
<i class="fa fa-envelope"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
<small class=text-white>&copy; 2022 The Merbridge Authors 版权所有</small>
<p class=mt-2><a href=/zh/about/>关于 Merbridge</a></p>
</div>
</div>
</div>
</footer>
</div>
<script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script>
<script src=/js/main.min.3b172c13b62c2bea8b1c9d2599cddc8cf89718a92d792c680871c81ba43d8c85.js integrity="sha256-OxcsE7YsK+qLHJ0lmc3cjPiXGKkteSxoCHHIG6Q9jIU=" crossorigin=anonymous></script>
</body>
</html>
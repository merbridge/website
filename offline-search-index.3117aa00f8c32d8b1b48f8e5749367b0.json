

































[{"body":"The CNI mode is designed to better adapt to the service mesh functions. Before having this mode, Merbridge was limited to certain scenarios. The biggest problem was that it could not adapt to the sidecar annotations from the container injected by Istio, which led to Merbridge cannot exclude traffic from certain ports and IP ranges. Furthermore, Merbridge was only able to handle requests inside the pod, which means the external traffic sent to the pod was not handled.\nTherefore, we have implemented the Merbridge CNI to address these issues.\nWhy CNI mode is needed First, Merbridge had a small control plane before, which listened to pods resources, and wrote the current node IP into the map of local_pod_ips for use by connect. However, since the connect program only works at the host kernel layer, it won’t know which pod’s traffic is being processed. Thus, configurations like excludeOutboundPorts cannot be handled. In order to be able to adapt to the injected sidecar annotation excludeOutboundPorts, we need to let the eBPF program know which Pod’s request is currently being processed.\nTo this end, we have designed a method to cooperate with the CNI, through which you can get the current Pod IP to validate special configurations for the Pod.\nSecond, for early versions of Merbridge, only connect would process requests from the host, which had no problem for intra-node pod communication. However, it becomes problematic when traffic flows between different nodes. According to the previous logic, the traffic will not be modified during the cross-node communication, which will lead to the use of iptables at the end.\nHere, we turned to the XDP program for processing the inbound traffic. The XDP program needs to mount a network card, which also needs to use CNI.\nHow does CNI work This section will explore how CNI works and how to use CNI to solve the issues mentioned above.\nHow to use CNI to let eBPF have the current Pod IP When a pod is created, we write Pod IP into the map mark_pod_ips_map through CNI, where the key is a random value, and the value is the Pod IP. Then, we listen to a special port 39807 in the NetNS of the current Pod, and write the key to the mark of this port socket using setsockopt.\nIn eBPF, we get the recorded mark information of port 39807 through bpf_sk_lookup_tcp, and use it to get the current Pod IP (also the current NetNS) from mark_pod_ips_map.\nWith the current Pod IP, we can determine the path to route traffic (such as excludeOutboundPorts) according to the configuration of this Pod.\nIn addition, we also optimized the quadruple conflicts by using bpf_bind to bind the source IP and using 127.0.0.1 as the destination IP, which also prepares for future support of IPv6.\nHow to handle ingress traffic In order to handle inbound traffic, we introduced the XDP program, which works on the network card and can modify the original data packets.\nWe use the XDP program to modify the destination port as 15006 when the traffic reaches the Pod, so as to complete traffic forwarding.\nAt the same time, considering the possibility that the host directly accesses the Pod, and in order to reduce the scope of influence, we choose to attach the XDP program to the Pod’s network card. This requires the ability of CNI to perform additional operations when creating Pods\nHow to use CNI mode? CNI mode is disabled by default. You need to enable it manually with the following command.\n curl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml | sed 's/--cni-mode=false/--cni-mode=true/g' | kubectl apply -f - Notes CNI mode is in beta The CNI mode is a new feature that may not be perfect. We welcome your feedback and suggestions to help improve Merbridge.\nIf you are trying to do benchmark test using tools like Istio perf benchmark, it is suggested to enable the CNI mode. Otherwise the test results will be inaccurate.\nCheck whether the host can enable the hardware-checksum capability In order to ensure the CNI mode works properly, the hardware-checksum capability is disabled by default, which may affect network performance. It is recommended to check whether you can enable this capability on the host before enabling the CNI mode. If yes, we suggest to set --hardware-checksum=true for best performance.\n| grep tx-checksum-ipv4` 为 on 表示开启。-- Test method: if ethtool -k \u003cnetwork card\u003e | grep tx-checksum-ipv4 is on, it means enabled.\n","categories":"","description":"This blog explains how CNI works in Merbridge.","excerpt":"This blog explains how CNI works in Merbridge.","ref":"/blog/2022/05/18/cni-mode/","tags":"","title":"Merbridge CNI Mode"},{"body":"Merbridge CNI 模式的出现，旨在能够更好地适配服务网格的功能。之前没有 CNI 模式时，Merbridge 能够做得事情比较有限。其中最大的问题是不能适配注入 Istio 的 Sidecar Annotation，这就导致 Merbridge 无法排除某些端口或 IP 段的流量等。 同时，由于之前 Merbridge 只处理 Pod 内部的连接请求，这就导致，如果是外部发送到 Pod 的流量，Merbridge 将无法处理。\n为此，我们精心设计了 Merbridge CNI，旨在解决这些问题。\n为什么需要 CNI 模式？ 其一，之前的 Merbridge 只有一个很小的控制面，其监听 Pod 资源，将当前节点的 IP 信息写入 local_pod_ips 的 map，以供 connect 使用。 但是，connect 程序由于工作在主机内核层，其无法知道当前正在处理的是哪个 Pod 的流量，就没法处理如 excludeOutboundPorts 等配置。 为了能够适配注入 excludeOutboundPorts 的 Sidecar Annotation，我们需要让 eBPF 程序能够得知当前正在处理哪个 Pod 的请求。\n为此，我们设计了一套方法，与 CNI 配合，能够获取当前 Pod 的 IP，以适配针对 Pod 的特殊配置。\n其二，在之前的 Merbridge 版本中，只有 connect 会处理主机发起的请求，这在同一台主机上的 Pod 互相通讯时，是没有问题的。但是在不同主机之间通讯时就会出现问题，因为按照之前的逻辑，在跨节点通讯时流量不会被修改，这会导致在接收端还是离不开 iptables。\n这次，我们依靠 XDP 程序，解决入口流量处理的问题。因为 XDP 程序需要挂载网卡，所以也需要借助 CNI。\nCNI 如何解决问题？ 这里我们将探讨 CNI 的工作原理，以及如何使用 CNI 来解决问题。\n如何通过 CNI 让 eBPF 程序获取当前正在处理的 Pod IP？ 我们通过 CNI，在 Pod 创建的时候，将 Pod 的 IP 信息写入一个 Map（mark_pod_ips_map），其 Key 为一个随机的值，Value 为 Pod 的 IP。然后，在当前 Pod 的 NetNS 里面监听一个特殊的端口 39807，将 Key 使用 setsockopt 写入这个端口 socket 的 mark。\n在 eBPF 中，我们通过 bpf_sk_lookup_tcp 取得端口 39807 的 Mark 信息，然后从 mark_pod_ips_map 中即可取得当前 NetNS（也是当期 Pod）的 IP。\n有了当前 Pod IP 之后，我们可以根据这个 Pod 的配置，确认流量处理路径（比如 excludeOutboundPorts）。\n同时，我们还使用 Pod 优化了之前解决四元组冲突的方案，改为使用 bpf_bind 绑定源 IP，目的 IP 直接使用 127.0.0.1，为了后续支持 IPv6 做准备。\n如何处理入口流量？ 为了能够处理入口流量，我们引入了 XDP 程序，XDP 程序作用在网卡上，能够对原始数据包做修改。 我们借助 XDP 程序，在流量到达 Pod 的时候，修改目的端口为 15006 以完成流量转发。\n同时考虑到可能存在主机直接访问 Pod 的情况，也为了减小影响范围，我们选择将 XDP 程序附加到 Pod 的网卡上。这就需要借助 CNI 的能力，在创建 Pod 时进行附加操作。\n如何体验 CNI 模式？ CNI 模式默认被关闭，需要手动开启。\n可以使用以下命令一键开启：\ncurl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml | sed 's/--cni-mode=false/--cni-mode=true/g' | kubectl apply -f - 注意事项 CNI 模式处于测试阶段 CNI 模式刚被设计和开发出来，可能存在不少问题，我们欢迎大家在测试阶段进行反馈，或者提出更好的建议，以帮助我们改进 Merbridge！\n如果需要使用注入 Istio perf benchmark 等工具进行测试性能，请开启 CNI 模式，否则会导致性能测试结果不准确。\n需要注意主机是否可开启 hardware-checksum 能力 为了保证 CNI 模式的正常运行，我们默认关闭了 hardware-checksum 能力，这可能会影响到网络性能。建议大家在开启 CNI 模式前，先确认主机是否可开启 hardware-checksum 能力。如果可以开启，建议设置 --hardware-checksum=true 以获得最佳的性能表现。\n测试方法：ethtool -k \u003c网卡\u003e | grep tx-checksum-ipv4 为 on 表示开启。\n","categories":"","description":"此篇博客将向您介绍 Merbridge CNI 的工作原理。","excerpt":"此篇博客将向您介绍 Merbridge CNI 的工作原理。","ref":"/zh/blog/2022/05/18/cni-mode/","tags":"","title":"Merbridge CNI 模式"},{"body":"Merbridge and Cilium Cilium is a great open source software that provides a lot of networking capabilities for cloud native applications based on eBPF, with a lot of great designs. Among others, Cilium designed a set of sockmap-based redir capabilities to help accelerate network communications, which inspired us and is the basis for Merbridge to provide network acceleration. It is a really great design.\nMerbridge leverages the great foundation that Cilium has provided, along with some targeted adaptations we’ve made in the Service Mesh, to make it easier to apply eBPF technology to Service Mesh.\nOur development team have learned a lot eBPF theoretical knowledge, practical methods, and testing methods, from Cilium’s detailed documentation and our frequent exchanges with the Cilium technical team. All these together helps make Merbridge possible.\nThanks again to the Cilium project and community, and to Cilium for these great designs.\n","categories":"","description":"","excerpt":"Merbridge and Cilium Cilium is a great open source software that …","ref":"/blog/2022/04/23/merbridge-and-cilium/","tags":"","title":"Merbridge and Cilium"},{"body":"Merbridge 与 Cilium Cilium 是一款基于 eBPF 为云原生应用提供诸多网络能力的优秀开源软件，有很多很棒的设计。例如，Cilium 设计了一套基于 sockmap 的 redir 能力，帮助加速网络通讯，这给了我们很大的启发，也是 Merbridge 提供网络加速的基础，这真是一个非常棒的设计。\nMerbridge 借助于 Cilium 打下的良好基础，加上我们在服务网格领域做地一些针对性的适配，让大家可以更加方便地将 eBPF 技术应用于服务网格。\n我们的开发团队从 Cilium 提供的资料中学习了很多关于 eBPF 的理论知识、实践方法和测试方法等，也与 Cilium 技术团队多有交流，也就是因为这些经历，才能有 Merbridge 项目的诞生。\n再次衷心感谢 Cilium 项目和社区，以及 Cilium 的这些优秀设计。\n","categories":"","description":"","excerpt":"Merbridge 与 Cilium Cilium 是一款基于 eBPF 为云原生应用提供诸多网络能力的优秀开源软件，有很多很棒的设计。例 …","ref":"/zh/blog/2022/04/23/merbridge-and-cilium/","tags":"","title":"Merbridge 和 Cilium"},{"body":"On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.\nIn this livestream, we discussed a lot of Merbridge-related issues, including a live demo that will help you get a quick overview of Merbridge’s features and usage.\nAlso, the PPT is available here for download.\nIf you are interested, see:\n ","categories":"","description":"","excerpt":"On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.\nIn …","ref":"/blog/2022/03/29/solo-io-livestream/","tags":"","title":"Livestream with Solo.io"},{"body":"2022 年 3 月 29 日，Solo.io 和 Merbridge 共同举办了一场直播活动。\n在这次直播中，我们一起探讨了很多与 Merbridge 相关的问题，其中包含了一个线上 Demo，可以帮你快速了解 Merbridge 的功能和使用方法。\n同时，PPT 可以在这里下载。\n如果您有兴趣，可以查看：\n ","categories":"","description":"","excerpt":"2022 年 3 月 29 日，Solo.io 和 Merbridge 共同举办了一场直播活动。\n在这次直播中， …","ref":"/zh/blog/2022/03/29/solo-io-livestream/","tags":"","title":"与 Solo.io 一起举办的直播活动"},{"body":"Merbridge - Accelerate your mesh with eBPF Replacing iptables rules with eBPF allows transporting data directly from inbound sockets to outbound sockets, shortening the datapath between sidecars and services.\nIntroduction The secret of Istio’s abilities in traffic management, security, observability and policy is all in the Envoy proxy. Istio uses Envoy as the “sidecar” to intercept service traffic, with the kernel’s netfilter packet filter functionality configured by iptables.\nThere are shortcomings in using iptables to perform this interception. Since netfilter is a highly versatile tool for filtering packets, several routing rules and data filtering processes are applied before reaching the destination socket. For example, from the network layer to the transport layer, netfilter will be used for processing for several times with the rules predefined, like pre_routing, post_routing and etc. When the packet becomes a TCP packet or UDP packet, and is forwarded to user space, some additional steps like packet validation, protocol policy processing and destination socket searching will be performed. When a sidecar is configured to intercept traffic, the original data path can become very long, since duplicated steps are performed several times.\nOver the past two years, eBPF has become a trending technology, and many projects based on eBPF have been released to the community. Tools like Cilium and Pixie show great use cases for eBPF in observability and network packet processing. With eBPF’s sockops and redir capabilities, data packets can be processed efficiently by directly being transported from an inbound socket to an outbound socket. In an Istio mesh, it is possible to use eBPF to replace iptables rules, and accelerate the data plane by shortening the data path.\nWe have created an open source project called Merbridge, and by applying the following command to your Istio-managed cluster, you can use eBPF to achieve such network acceleration.\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml  Attention: Merbridge uses eBPF functions which require a Linux kernel version ≥ 5.7.\n With Merbridge, the packet datapath can be shortened directly from one socket to another destination socket, and here’s how it works.\nUsing eBPF sockops for performance optimization Network connection is essentially socket communication. eBPF provides a function bpf_msg_redirect_hash, to directly forward the packets sent by the application in the inbound socket to the outbound socket. By entering the function mentioned before, developers can perform any logic to decide the packet destination. According to this characteristic, the datapath of packets can noticeably be optimized in the kernel.\nThe sock_map is the crucial piece in recording information for packet forwarding. When a packet arrives, an existing socket is selected from the sock_map to forward the packet. As a result, we need to save all the socket information for packets to make the transportation process function properly. When there are new socket operations — like a new socket being created — the sock_ops function is executed. The socket metadata is obtained and stored in the sock_map to be used when processing packets. The common key type in the sock_map is a “quadruple” of source and destination addresses and ports. With the key and the rules stored in the map, the destination socket will be found when a new packet arrives.\nThe Merbridge approach Let’s introduce the detailed design and implementation principles of Merbridge step by step, with a real scenario.\nIstio sidecar traffic interception based on iptables When external traffic hits your application’s ports, it will be intercepted by a PREROUTING rule in iptables, forwarded to port 15006 of the sidecar container, and handed over to Envoy for processing. This is shown as steps 1-4 in the red path in the above diagram.\nEnvoy processes the traffic using the policies issued by the Istio control plane. If allowed, the traffic will be sent to the actual container port of the application container.\nWhen the application tries to access other services, it will be intercepted by an OUTPUT rule in iptables, and then be forwarded to port 15001 of the sidecar container, where Envoy is listening. This is steps 9-12 in the red path, similar to inbound traffic processing.\nTraffic to the application port needs to be forwarded to the sidecar, then sent to the container port from the sidecar port, which is overhead. Moreover, iptables’ versatility determines that its performance is not always ideal because it inevitably adds delays to the whole datapath with different filtering rules applied. Although iptables is the common way to do packet filtering, in the Envoy proxy case, the longer datapath amplifies the bottleneck of packet filtering process in the kernel.\nIf we use sockops to directly connect the sidecar’s socket to the application’s socket, the traffic will not need to go through iptables rules, and thus performance can be improved.\nProcessing outbound traffic As mentioned above, we would like to use eBPF’s sockops to bypass iptables to accelerate network requests. At the same time, we also do not want to modify any parts of Istio, to make Merbridge fully adaptive to the community version. As a result, we need to simulate what iptables does in eBPF.\nTraffic redirection in iptables utilizes its DNAT function. When trying to simulate the capabilities of iptables using eBPF, there are two main things we need to do:\n Modify the destination address, when the connection is initiated, so that traffic can be sent to the new interface. Enable Envoy to identify the original destination address, to be able to identify the traffic.  For the first part, we can use eBPF’s connect program to process it, by modifying user_ip and user_port.\nFor the second part, we need to understand the concept of ORIGINAL_DST which belongs to the netfilter module in the kernel.\nWhen an application (including Envoy) receives a connection, it will call the get_sockopt function to obtain ORIGINAL_DST. If going through the iptables DNAT process, iptables will set this parameter, with the “original IP + port” value, to the current socket. Thus, the application can get the original destination address according to the connection.\nWe have to modify this call process through eBPF’s get_sockopts function. (bpf_setsockopt is not used here because this parameter does not currently support the optname of SO_ORIGINAL_DST).\nReferring to the figure below, when an application initiates a request, it will go through the following steps:\n When the application initiates a connection, the connect program will modify the destination address to 127.x.y.z:15001, and use cookie_original_dst to save the original destination address. In the sockops program, the current socket information and the quadruple are saved in sock_pair_map. At the same time, the same quadruple and its corresponding original destination address will be written to pair_original_dst. (Cookie is not used here because it cannot be obtained in the get_sockopt program). After Envoy receives the connection, it will call the get_sockopt function to read the destination address of the current connection. get_sockopt will extract and return the original destination address from pair_original_dst, according to the quadruple information. Thus, the connection is completely established. In the data transport step, the redir program will read the sock information from sock_pair_map according to the quadruple information, and then forward it directly through bpf_msg_redirect_hash to speed up the request.  Why do we set the destination address to 127.x.y.z instead of 127.0.0.1? When different pods exist, there might be conflicting quadruples, and this gracefully avoids conflict. (Pods’ IPs are different, and they will not be in the conflicting condition at any time.)\nInbound traffic processing The processing of inbound traffic is basically similar to outbound traffic, with the only difference: revising the port of the destination to 15006.\nIt should be noted that since eBPF cannot take effect in a specified namespace like iptables, the change will be global, which means that if we use a Pod that is not originally managed by Istio, or an external IP address, serious problems will be encountered — like the connection not being established at all.\nAs a result, we designed a tiny control plane (deployed as a DaemonSet), which watches all pods — similar to the kubelet watching pods on the node — to write the pod IP addresses that have been injected into the sidecar to the local_pod_ips map.\nWhen processing inbound traffic, if the destination address is not in the map, we will not do anything to the traffic.\nThe other steps are the same as for outbound traffic.\nSame-node acceleration Theoretically, acceleration between Envoy sidecars on the same node can be achieved directly through inbound traffic processing. However, Envoy will raise an error when accessing the application of the current pod in this scenario.\nIn Istio, Envoy accesses the application by using the current pod IP and port number. With the above scenario, we realized that the pod IP exists in the local_pod_ips map as well, and the traffic will be redirected to the pod IP on port 15006 again because it is the same address that the inbound traffic comes from. Redirecting to the same inbound address causes an infinite loop.\nHere comes the question: are there any ways to get the IP address in the current namespace with eBPF? The answer is yes!\nWe have designed a feedback mechanism: When Envoy tries to establish the connection, we redirect it to port 15006. However, in the sockops step, we will determine if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard this connection in the sockops process. In the meantime, the current ProcessID and IP information will be written into the process_ip map, to allow eBPF to support correspondence between processes and IPs.\nWhen the next request is sent, the same process need not be performed again. We will check directly from the process_ip map if the destination address is the same as the current IP address.\n Envoy will retry when the request fails, and this retry process will only occur once, meaning subsequent requests will be accelerated.\n Connection relationship Before applying eBPF using Merbridge, the data path between pods is like:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n After applying Merbridge, the outbound traffic will skip many filter steps to improve the performance:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n If two pods are on the same machine, the connection can even be faster:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n Performance results  The below tests are from our development, and not yet validated in production use cases.\n Let’s see the effect on overall latency using eBPF instead of iptables (lower is better):\nWe can also see overall QPS after using eBPF (higher is better):\n Test results are generated with wrk.\n Summary We have introduced the core ideas of Merbridge in this post. By replacing iptables with eBPF, the data transportation process can be accelerated in a mesh scenario. At the same time, Istio will not be changed at all. This means if you do not want to use eBPF any more, just delete the DaemonSet, and the datapath will be reverted to the traditional iptables-based routing without any problems.\nMerbridge is a completely independent open source project. It is still at an early stage, and we are looking forward to having more users and developers to get engaged. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!\nMerbridge Project: https://github.com/merbridge/merbridge\nSee also   https://ebpf.io/\n  https://cilium.io/\n  Merbridge on GitHub\n  Using eBPF instead of iptables to optimize the performance of service grid data plane by Liu Xu, Tencent\n  Sidecar injection and transparent traffic hijacking process in Istio explained in detail by Jimmy Song, Tetrate\n  Accelerate the Istio data plane with eBPF by Yizhou Xu, Intel\n  Envoy’s Original Destination filter\n  Accelerating Envoy and Istio with Cilium and the Linux Kernel\n  ","categories":"","description":"","excerpt":"Merbridge - Accelerate your mesh with eBPF Replacing iptables rules …","ref":"/blog/2022/03/01/merbridge-introduce/","tags":"","title":"Merbridge - Accelerate your mesh with eBPF"},{"body":"一行代码使用 eBPF 代替 iptables 加速 Istio 介绍 以 Istio 为首的服务网格技术正在被越来越多的企业关注，其使用 Sidecar 借助 iptables 技术实现流量拦截，可以处理所有应用的出入口流量，以实现诸如治理、观测、加密等能力。\n但是使用 iptables 的方式进行拦截，由于需要对出入口都拦截，会让原本只需要在内核态处理两次的链路变成四次，会损失不少性能，这在一些要求高性能的场景下显然是有影响的。\n近两年，由于 eBPF 技术的兴起，不少围绕 eBPF 的项目也应声而出，eBPF 在可观测性和网络包的处理方面也有不少优秀的案例。如 Cilium、px.dev 等项目。\n借助 eBPF 的 sockops 和 redir 能力，可以高效的处理数据包，再结合实际场景，那么我们就可以使用 eBPF 去代替 iptables 为 Istio 进行加速。\n现在，我们开源了 Merbridge 项目，只需要在您的 Istio 集群执行以下命令，即可直接使用 eBPF 代替 iptables 实现网络加速！\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml  注意：当前仅支持在 5.7 版本及以上的内核下运行，请事先升级您的内核版本。\n eBPF 的 sockops 加速 网络连接本质上是 socket 的通讯，eBPF 提供了一个 bpf_msg_redirect_hash 函数，用来将应用发出的包，直接转发到对端的 socket 上面，可以极大的加速包在内核中的处理流程。\n这里需要一个 sock_map，需要根据当前的数据包信息，从 sock_map 中挑选一个存在的 socket 连接，转发请求，所以，需要在 sockops 的 hook 处或者其它地方将 socket 信息保存到 sock_map，并提供根据 key 查到 socket 的规则（一般为四元组）。\n原理 下面，将按照实际的场景，逐步的介绍 Merbridge 详细的设计和实现原理，这将让你对 Merbridge 或者 eBPF 有一个初步的了解。\nIstio 基于 iptables 的原理 如上图所示，当外部流量相应访问应用的端口时，会在 iptables 中被 PREROUTING 拦截，最后转发到 Sidecar 容器的 15006 端口，然后交给 Envoy 来进行处理。（图中红色 1 2 3 4 的路径）\nEnvoy 根据从控制平面下发的规则进行处理，处理完成后，会发送请求给实际的容器端口。\n当应用想要访问其它服务时，会在 iptables 中 OUTPUT 拦截，然后转发给 Sidecar 容器的 15001 端口（Envoy 监听）。（图中红色 9 10 11 12 的路径）然后和入口流量处理差不多。\n由此可以看到，原本流量可以直接到应用端口，但是中间需要通过 iptables 转发到 Sidecar，然后又让 Sidecar 发送给应用，这无疑增加了开销。并且，iptables 的通用性决定了它的性能没有很理想。会在整条链路上增加不少延迟。\n如果我们能使用 sockops 去直接连接 Sidecar 到应用的 Socket，这样可以使流量不经过 iptables，可以提高性能。\n出口流量处理 如上所述，我们希望使用 eBPF 的 sockops 来绕过 iptables 以加速网络请求。同时，我们希望创造的是一个能够完全适配社区版 Istio，不做任何改造。所以，我们需要模拟 iptables 所做的操作。\n这个时候我们在看回 iptables 本身，其使用 DNAT 功能做流量转发。\n想要用 eBPF 模拟 iptables 的能力，那么就需要使用 eBPF 实现类似 iptables DNAT 的能力。\n这里主要有两个点：\n 修改连接发起时的目的地址，让流量能够发送到新的接口； 让 Envoy 能识别原始的目的地址，以能够识别流量；  对于其中第一点，我们可以使用 eBPF 的 connect 程序来做，通过修改 user_ip 和 user_port 实现。\n对于其中第二点，需要用到 ORIGINAL_DST 的概念。这个在内核中其实是在 netfilter 模块专属的。\n其原理就是，应用程序（包括 Envoy）会在收到连接之后，调用 get_sockopts 函数，获取 ORIGINAL_DST，如果经过了 iptables 的 DNAT，那么 iptables 就会给当前的 socket 设置这个值，并把原有的 IP + 端口写入这个值，应用程序就可以根据连接拿到原有的目的地址。\n那么我们就需要通过 eBPF 的 get_sockopt 程序来修改这个调用。（不用 **bpf_setsockopt** 的原因是因为目前这个参数并不支持 SO_ORIGINAL_DST` 的 optname）\n参见下图，在应用向外发起请求时，会经过如下阶段：\n 在应用向外发起连接时，connect 程序会将目标地址修改为 127.x.y.z:15001，并用 cookie_original_dst 保存原始目的地址。 在 sockops 程序中，将当前 sock 和四元组保存在 sock_pair_map 中。同时，将四元组信息和对应的原始目的地址写入 pair_original_dst 中（之所以不用 cookie，是因为在 get_sockopt 程序中无法获取当前 cookie）。 Envoy 收到连接之后会调用 getsockopt 获取当前连接的目的地址，get_sockopt 程序会根据四元组信息从 pair_original_dst 取出原始目的地址并返回，由此连接完全建立。 在发送数据阶段，redir 程序会根据四元组信息，从 sock_pair_map 中读取 sock，然后通过 bpf_msg_redirect_hash 进行直`接转发，加速请求。  其中，之所以在 connect 的时候，修改目的地址为 127.x.y.z 而不是 127.0.0.1，是因为在不同的 Pod 中，可能产生冲突的四元组，使用此方式即可巧妙的避开。（每个 Pod 间的目的 IP 就已经不同了，不存在冲突的情况）\n入口流量处理 入口流量处理基本和出口流量类似，唯一差别：只需要将目的地址的端口改成 15006 即可。\n但是，需要注意的是，由于 eBPF 不像 iptables 能在指定命名空间生效，它是全局的，这就造成如果我们将一个本来不是 Istio 所管理的 Pod，或者就是一个外部的 IP 地址，也做了这个操作的话，那就会引起严重问题，会请求直接无法建立连接。\n所以这里我们设计了一个小的控制平面（以 DaemonSet 方式部署），其通过 Watch 所有的 Pod，类似于像 kubelet 那样获取当前节点的 Pod 列表，将已经被注入了 Sidecar 的 Pod IP 地址写入 local_pod_ips 这个 map。\n当我们在做入口流量处理的时候，如果目的地址不在这个列表之中，我们就不做处理，让它走原来的逻辑，这样就可以比较灵活且简单的处理入口流量。\n其他的流程和出口流量流程一样。\n同节点加速 通过入口流量处理，理论上，我们已经可以直接加速同节点的 Envoy 到 Envoy 的加速。但是存在一个问题。就是在这种场景下，Envoy 访问当前 Pod 的应用的时候会出错。\n在 Istio 中，Envoy 访问应用的方式是使用当前 PodIP 加服务端口。经过上面入口流量处理章节，其实我们会发现，由于 PodIP 肯定也存在于 local_pod_ips 中，那么这个请求会被转发到 PodIP + 15006 端口，这显然是不行的，会造成无限递归。\n那么我们也没办法在 eBPF 中获取当前 ns 的 IP 地址信息，怎么办？\n为此，我们设计了一套反馈机制：\n即，在 Envoy 尝试建立连接的时候，我们还是会走重定向到 15006 端口，但是，在 sockops 阶段，我们会判断源 IP 和目的地址 IP是否一致，如果一致，代表发送了错误的请求，那么我们会在 sockops 丢弃这个连接，并将当前的 ProcessID 和 IP 地址信息写入 process_ip 这个 map，让 eBPF 支持进程和 IP 的对应关系。\n当下次请求发送时，我们直接从 process_ip 表检查目的地址是否和当前 IP 地址\n Envoy 会在请求失败的时候重试，且这个错误只会发生一次，后续的连接会非常快。\n 连接关系 在没有使用 Merbridge（eBPF） 优化之前，Pod 到 Pod 间的访问入下图所示：\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n 在使用 Merbridge（eBPF）优化之后，出入口流量会使用直接跳过很多内核模块，提高性能：\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n 同时，如果两个 Pod 在同一台机器上，那么他们之间的通讯将更加高效：\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n 以上，通过使用 eBPF 在主机上对相应的连接进行处理，可以大幅度的减少内核处理流量的流程，提升服务之间的通讯质量。\n加速效果  下面的测试只是一个基本的测试，不是非常严谨。\n 下图展示了使用 eBPF 代替 iptables 之后，整体延迟的情况（越低越好）：\n下图展示了使用 eBPF 代替 iptables 之后，整体 QPS 的情况（越高越好）：\n 以上数据使用 wrk 测试得出。\n Merbridge 项目 以上介绍的都是 Merbridge 项目的核心能力，其通过使用 eBPF 代替 iptables，可以在服务网格场景下，完全无感知的对流量通路进行加速。同时，我们不会对现有的 Istio 做任何修改，原有的逻辑依然畅通，这意味着，如果不再希望使用 eBPF，那么可以直接删除掉 DaemonSet，改为传统的 iptables 方式也不会出任何问题。\nMerbridge 是一个完全独立的开源项目，此时还处于早期阶段，我们希望可以有更多的用户或者开发者参与其中，使用先进的技术能力，优化我们的服务网格。\n项目地址：https://github.com/merbridge/merbridge\n参考文档：\n eBPF Cilium Merbridge on GitHub Using eBPF instead of iptables to optimize the performance of service grid data plane by Liu Xu, Tencent Sidecar injection and transparent traffic hijacking process in Istio explained in detail by Jimmy Song, Tetrate Accelerate the Istio data plane with eBPF by Yizhou Xu, Intel Envoy’s Original Destination filter Accelerating Envoy and Istio with Cilium and the Linux Kernel  ","categories":"","description":"","excerpt":"一行代码使用 eBPF 代替 iptables 加速 Istio 介绍 以 Istio 为首的服务网格技术正在被越来越多的企业关注， …","ref":"/zh/blog/2022/03/01/merbridge-introduce/","tags":"","title":"一行代码，使用 eBPF 代替 iptables 加速服务网格"},{"body":"What is Merbridge Merbridge is designed to make traffic interception and forwarding more efficient for service mesh. It replaced iptables with eBPF.\neBPF (extended Berkeley Packet Filter) can run user’s programs in the Linux kernel without modifying the kernel code or loading kernel modules. It is widely used in networking, security, monitoring and other relevant fields. Compared with iptables, Merbridge can shorten the data path between sidecars and services and therefore accelerate networking. Meanwhile, using Merbridge will not change the original architecture of Istio. The original logic is still valid. This means that if you don’t want Merbridge anymore, just delete the DaemonSet. The original iptables will function again without any troubles.\nWhat Merbridge can do Merbridge has following core features:\n  Processing outbound traffic\nMerbridge uses eBPF’s connect program to modify user_ip and user_port, so as to change the destination address of a connection and ensure traffic can be sent to the new interface. In order to help Envoy identify the original destination, the application (incl. Envoy) will call the get_sockopt function to get ORIGINAL_DST when receiving a connection.\n  Processing inbound traffic\nInbound traffic is processed similarly to outbound traffic. Note that eBPF cannot take effect in a specified namespace like iptables, so changes will be global. It means that if we apply eBPF to Pods that are not originally managed by Istio, or an external IP, serious problems will occur, e.g., cannot establish a connection.\nTo address this issue, we designed a tiny control plane, deployed as a DaemonSet. It can help watch and get a list of all pods on the node, similar to kubelet. Then, Pod IPs injected into the sidecar will be written into the local_pod_ips map. For traffic with a destination address not in the map, Merbridge will not intercept it.\n  Accelerating networking\nIn Istio, Envoy visits the application by the current podIP and port number. Because the podIP exists in the local_pod_ips map, traffic will be redirected to the podIP on port 15006, producing an infinite loop. Are there any ways for eBPF to get the IP address in the current namespace? Yes! We have designed a feedback mechanism: When Envoy tries to establish a connection, we redirect it to port 15006. When it moves to sockops, we will check if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard it in the sockops process. Meanwhile, the current ProcessID and IP will be written into the process_ip map, allowing eBPF to support corresponding relationship between processes and IPs. When the next request is sent, we will check directly from the process_ip map if the destination is the same as the current IP. Envoy will retry when the request fails. This retry process will only occur once, and subsequent connections will go very fast.\n  Why Merbridge is better In the service mesh scenario, in order to use sidecars for traffic management without the application being aware of it, ingress and egress traffic of Pods should be forwarded to the sidecar. The most common solution is using the redirect capability of iptables (netfilter) to forward the original traffic. However, this approach will increase network latency, because iptables intercept both egress and ingress traffic. For example, the traffic that originally flows directly to the application now is forwarded to the sidecar by iptables (netfilter), and the sidecar will then forward it to the final application. The data path becomes very long, since duplicated steps are performed several times.\nLuckily, eBPF provides a function bpf_msg_redirect_hash to directly forward packets from applications in the inbound socket to the outbound socket. By doing so, packet processing can be greatly accelerated in the kernel. Therefore, we hope to replace iptables with eBPF. That’s how Merbridge came into being.\nWhen to use Merbridge Merbridge is recommended if you have any of following problems:\n In scenarios that require high-performance connections, using iptables will increase latency.  The performance of iptables control plane and data plane degrades dramatically as the number of containers in the cluster increases. It needs to traverse and modify all the rules every time a new rule is added. Systems that use IP addresses for security filtering will come under increasing pressure as Pod lifecycle is getting shorter, sometimes just a few seconds, because it requires more frequent updates of iptables rules. Using iptables to achieve transparent interception needs a conntrack module for connection trace. It will cause a lot of consumption when there are many connections.   The system cannot use iptables for some reasons.  Sometimes it needs to process numerous active connections simultaneously, but using iptables is easily to have a full conntrack table. Sometimes numerous connections should be processed in one second, which will exceed limit of the conntrack table. For example, if you try to process 1100 connections per second with timeout set as 120 seconds and a table capacity of 128k, it would exceed the conntrack table’s limit (128k/120 seconds = 1092 connections/second).   Due to security concerns, some ordinary Pods cannot have too many permissions, but using Istio (without CNI) must allow these Pods to gain more permissions.  Running the init container may require permissions such as NET_ADMIN. Running an iptables command may need CAP_NET_ADMIN permission. Mounting a file system may need CAP_SYS_ADMIN permission.    What Merbridge will change Using eBPF can greatly simplify the kernel’s processing of traffic and make inter-service communication more efficient.\n Before applying eBPF with Merbridge, the data path between pods is like:   Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n  After applying Merbridge, the outbound traffic can skip many filter steps to improve performance:   Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n  If two pods are on the same node, the connection will be even faster:   Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n Merbridge is a completely independent open source project. It is still at an early stage, and we wish to have more users and developers engaged in. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!　","categories":"","description":"This page outlines Merbridge and its features, applicable scenarios, and competitiveness.\n","excerpt":"This page outlines Merbridge and its features, applicable scenarios, …","ref":"/docs/overview/","tags":"","title":"Overview"},{"body":"Merbridge 是什么 Merbridge 专为服务网格设计，使用 eBPF 代替传统的 iptables 劫持流量，能够让服务网格的流量拦截和转发能力更加高效。\neBPF (extended Berkeley Packet Filter) 技术可以在 Linux 内核中运行用户编写的程序，而且不需要修改内核代码或加载内核模块，目前广泛应用于网络、安全、监控等领域。相比传统的 iptables 流量劫持技术，基于 eBPF 的 Merbridge 可以绕过很多内核模块，缩短边车和服务间的数据路径，从而加速网络。Merbridge 没有对现有的 Istio 作出任何修改，原有的逻辑依然畅通。这意味着，如果您不想继续使用 eBPF，直接删除相关的 DaemonSet 就能恢复为传统的 iptables 方式，不会出现任何问题。\nMerbridge 有哪些特性 Merbridge 的核心特性包括：\n  出口流量处理\nMerbridge 使用 eBPF 的 connect 程序，修改 user_ip 和 user_port 以改变连接发起时的目的地址，从而让流量能够发送到新的接口。为了让 Envoy 识别出原始目的地址，应用程序（包括 Envoy）会在收到连接之后调用 get_sockopts 函数，获取 ORIGINAL_DST。\n  入口流量处理\n入口流量处理与出口流量处理基本类似。需要注意的是，eBPF 是全局性的，不能在指定的命名空间生效。因此，如果对原本不是由 Istio 管理的 Pod 或者对外部的 IP 地址执行此操作，就会导致请求无法建立连接。为了解决此问题，Merbridge 设计了一个小的控制平面（以 DaemonSet 方式部署），通过 Watch 所有的 Pod，用类似于 kubelet 的方式获取当前节点的 Pod 列表，然后将已经注入 Sidecar 的 Pod IP 地址写入 local_pod_ips map。如果流量的目的地址不在该列表中，Merbridge 就不做处理，转而使用原来的逻辑。这样就可以灵活且便捷地处理入口流量。\n  同节点加速\n在 Istio 中，Envoy 使用当前 PodIP 加服务端口来访问应用程序。由于 PodIP 肯定也存在于 local_pod_ips ，所以请求就会被转发到 PodIP + 15006 端口。这样会造成无限递归，不能在 eBPF 获取当前命名空间的 IP 地址信息。因此，需要一套反馈机制：在 Envoy 尝试建立连接时仍然重定向到 15006 端口，在 sockops 阶段判断源 IP 和目的 IP 是否一致。如果一致，说明发送了错误的请求，需要在 sockops 丢弃该连接，并将当前的 ProcessID 和 IP 地址信息写入 process_ip map，让 eBPF 支持进程和 IP 的对应关系。下次发送请求时直接从 process_ip 表检查目的地址是否与当前 IP 地址一致。Envoy 会在请求失败时重试，且这个错误只会发生一次，后续的连接会非常快。\n  为什么需要 Merbridge 在服务网格场景中，为了能在应用程序完全无感知的情况下利用边车进行流量治理，需要把 Pod 的出入口流量都转发到边车。在这种情况下，最常见的解决方案就是使用 iptables (netfilter) 的重定向能力。这种方案的缺点是增加了网络延迟，因为 iptables 对出口流量和入口流量都进行拦截。以入口流量为例，原本直接流向应用的流量，需要先由 iptables 转发到边车，再由边车将流量转发到实际的应用。原本只需要在内核态处理两次的链路如今变成四次，损失了不少性能。\n幸运的是，eBPF 技术提供了一个 bpf_msg_redirect_hash 函数，可以将应用发出的数据包直接转发到对端的 socket，从而极大地加速数据包在内核中的处理流程。我们希望用 eBPF 技术代替 iptables 提高服务网格的效率，于是 诞生了 Merbridge。\nMerbridge 适用哪些场景 如果您遇到下列任一问题，建议使用 Merbridge：\n 在需要高性能连接的场景下，使用 iptables 会增加延迟。  由于集群中容器数量增加，iptables 控制面和数据面的性能会急剧下降。在 iptables 控制面的接口设计中，每添加一条规则都需要遍历并修改所有的规则。 由于 Pod 生命周期越来越短，有时甚至只有几秒钟，这就需要快速更新 iptables 规则，使用 IP 地址进行安全过滤的系统将承受越来越大的压力。 由于使用 iptables 实现透明劫持需要借助 conntrack 模块跟踪连接，连接较多时会造成大量消耗。   系统因某些原因不能使用 iptables。  有时需同时处理大量活动连接，但使用 iptables 容易出现 conntrack 表满的情况。 有时又需每秒处理极大数量的连接，但超出了 conntrack 表的限制。例如，在超时设置为 120 秒且表容量是 128k 的情况下，如果尝试每秒处理 1100 个连接，就会超出 conntrack 表的限制（128k/120秒 = 1092 连接/秒）。   出于安全考虑不能为普通的 Pod 授予太多权限，但使用 Istio（若无 CNI）必须允许 Pod 获得更多权限。  运行 init 容器，可能需要 NET_ADMIN 等权限。 运行 iptables 命令，对应的进程可能需要 CAP_NET_ADMIN 权限。 挂载文件系统，对应的进程可能需要 CAP_SYS_ADMIN 权限。    Merbridge 如何改变连接关系 使用 eBPF 在主机上处理连接，可以显著简化内核处理流量的流程，提升服务之间的通讯质量。\n 如果不用 Merbridge (eBPF)，Pod 到 Pod 间的访问连接关系如下图。   图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n  使用 Merbridge (eBPF) 优化之后，处理出入口流量时会跳过很多内核模块，从而加速网络。   图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n  如果两个 Pod 在同一节点上，使用 Merbridge (eBPF) 能让 Pod 之间的通讯更加高效。   图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n Merbridge 是完全独立的开源项目，目前仍处于早期阶段。希望有更多的用户或开发者参与其中，不断完善 Merbridge，共同优化服务网格。如果您发现了 Merbridge 的漏洞而且有兴趣帮助修复，非常欢迎您提交 Pull Request，附上您的修复代码，我们会及时处理您的 PR。\n","categories":"","description":"本文概述了 Merbridge 的含义、特性、适用场景等内容。\n","excerpt":"本文概述了 Merbridge 的含义、特性、适用场景等内容。\n","ref":"/zh/docs/overview/","tags":"","title":"概览"},{"body":"Prerequisites  Use kernel 5.7 or a higher version. Check your version with name -r. Activate cgroup2 in your system. Check the status with mount | grep cgroup2.  Installation Merbridge can be installed on Istio and Linkerd2 only.\nInstall on Istio Apply the following command to install Merbridge:\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml Install on Linkerd2 Apply the following command to install Merbridge:\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-linkerd.yaml Verification Verify installation Before you start this verification, make sure all Pods relevant to Merbridge are running well. You can check Pod status in Istio with the following command:\nkubectl -n istio-system get pods If all these Pods are Running, it means Merbridge is successfully installed.\nVerify connection Use the following methods to check the connectivity of Merbridge:\nInstall sleep and helloworld and wait for a full start: kubectl label ns default istio-injection=enabled kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/sleep/sleep.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/helloworld/helloworld.yaml Conduct curl test: kubectl exec $(kubectl get po -l app=sleep -o=jsonpath='{..metadata.name}') -c sleep -- curl -s -v helloworld:5000/hello If you see words like * Connected to helloworld (127.128.0.1) port 5000 (#0) in the output, it means Merbridge has managed to replace iptables with eBPF for traffic forwarding.\n","categories":"","description":"This page helps you quickly get started with Merbridge.\n","excerpt":"This page helps you quickly get started with Merbridge.\n","ref":"/docs/getting-started/","tags":"","title":"Quick Start"},{"body":"先决条件  系统的内核版本应大于等于 5.7，可以使用 uname -r 查看。 系统应开启 cgroup2，可以通过 mount | grep cgroup2 进行验证。  安装 目前支持在 Istio 和 Linkerd2 环境下安装 Merbridge。\nIstio 环境 只需要在环境中执行以下命令即可安装 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml Linkerd2 环境 只需要在环境中执行以下命令即可安装 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-linkerd.yaml 验证 验证安装 在验证 Merbridge 是否能正常工作之前，需要先确保 Merbridge 的 Pod 都运行正常。以 Istio 为例，可以使用以下命令查看 Merbridge 的 Pod 状态：\nkubectl -n istio-system get pods 当 Merbridge 相关的所有 Pod 都处于 Running 状态时，表明 Merbridge 已经安装成功。\n连接测试 可以按照如下方案验证 Merbridge 的连接是否正常：\n安装 sleep 和 helloworld 应用，并等待完全启动： kubectl label ns default istio-injection=enabled kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/sleep/sleep.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/helloworld/helloworld.yaml 执行 curl 测试： kubectl exec $(kubectl get po -l app=sleep -o=jsonpath='{..metadata.name}') -c sleep -- curl -s -v helloworld:5000/hello 如果在结果中看到类似 * Connected to helloworld (127.128.0.1) port 5000 (#0) 的字样，表明 Merbridge 已经成功使用 eBPF 代替 iptables 进行流量转发。\n","categories":"","description":"本文将帮助您快速开始使用 Merbridge\n","excerpt":"本文将帮助您快速开始使用 Merbridge\n","ref":"/zh/docs/getting-started/","tags":"","title":"快速开始"},{"body":"eBPF The full name of eBPF is Extended Berkeley Packet Filter. As the name implies, this is a module used to filter network packets. For example, sockops and redir capabilities of eBPF can efficiently filter and intercept packets.\neBPF is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in an operating system kernel. It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules.\niptables iptables is a traffic filter built on netfilter. It implements traffic filtering and interception by registering hook functions on the mount point of netfilter. From the name of iptables, we can guess it may contain some tables. In practice, by mounting rule tables on different chains of netfilter, iptables can filter or modify the traffic packets entering and leaving the kernel protocol stack.\niptables has 4 tables by default:\n Filter NAT Raw Mangle  iptables has 5 chains by default:\n INPUT chain (ingress rules) OUTPUT chain (egress rules) FORWARD chain (rules of forwarding) PREROUTING chain (rules before routing) POSTROUTING chain (rules after routing)  Service Mesh A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. A service mesh can guarantee fast, reliable, and secure communication between containerized application infrastructure services. Key capabilities provided by mesh include service discovery, load balancing, secure encryption and authentication, failover, observability, and more. A service mesh typically injects a sidecar proxy into each service instance. These sidecars handle inter-service communication, monitoring, and security. In this way, developers can focus on the development, support, and maintenance of the application code in the service, while the O\u0026M team is responsible for the maintenance of the service mesh and applications. Today, the most well-known service mesh is Istio.\nIstio Istio is a service mesh technology originally open sourced by IBM, Google, and Lyft. It can be layered transparently onto distributed applications and provides all the benefits of a service mesh, such as traffic governance, security, and observability.\nIstio can adapt to all services hosted with on-premises, cloud, Kubernetes containers, and virtual machines. It is typically used with microservices deployed on a Kubernetes platform.\nFundamentally, Istio works by deploying an extended version of Envoy as a sidecar proxy to each microservice. The proxy network it uses forms a data plane of Istio. The configuration and management of these proxies is done in a control plane, providing discovery, configuration, and certificate management for Envoy proxies in the data plane.\nLinkerd Linkerd is the first service mesh launched on the market, but Istio is more popular today.\nLinkerd is an open source, ultra-lightweight service mesh designed by Buoyant for Kubernetes. It is completely rewritten in Rust, which makes it as small, light and safe as possible. It provides runtime debugging, observability, reliability, and safety without code changes in distributed applications.\nLinkerd has three basic components: UI, data plane, and control plane. Linkerd works by installing a set of ultra-light, transparent proxies next to each service instance that automatically handle all traffic to and from the service.\n","categories":"","description":"This page describes some key concepts about Merbridge.\n","excerpt":"This page describes some key concepts about Merbridge.\n","ref":"/docs/concepts/","tags":"","title":"Concepts"},{"body":"eBPF eBPF 全称为 Extended Berkeley Packet Filter，顾名思义，这是一个用来过滤网络数据包的模块。例如 eBPF 的 sockops 和 redir 能力，就可以高效地过滤和拦截数据包。\neBPF 是一项起源于 Linux 内核的革命性技术，可以在操作系统的内核中运行沙盒程序，能够安全、有效地扩展 Linux 内核的功能，无需改变内核的源代码，也无需加载内核模块。\niptables iptables 是建立在 netfilter 之上的流量过滤器，通过向 netfilter 的挂载点上注册钩子函数来实现对流量过滤和拦截。从 iptables 这个名字上可以看出有表的概念，iptables 通过把这些规则表挂载在 netfilter 的不同链上，对进出内核协议栈的流量数据包进行过滤或者修改。\niptables 默认有 4 个表：\n Filter 表（数据过滤表） NAT 表（地址转换表） Raw 表（状态跟踪表） Mangle 表（包标记表）  iptables 默认有 5 个链：\n INPUT 链（入站规则） OUTPUT 链（出站规则） FORWARD 链（转发规则） PREROUTING 链（路由前规则） POSTROUTING 链（路由后规则）  Service Mesh 中文名为服务网格，这是一个可配置的低延迟基础设施层，通过 API 接口处理应用服务之间的网络进程间通信。服务网格能确保容器化应用基础结构服务之间的通信快速、可靠和安全。网格提供的关键功能包括服务发现、负载均衡、安全加密和身份验证、故障恢复、可观测性等。 服务网格通常会为每个服务实例注入一个 Sidcar 的代理实例。这些 Sidcar 会处理服务间的通信、监控和安全等问题。这样，开发人员就可以专注于服务中应用代码的开发、支持和维护，而运维团队负责服务网格以及应用的维护工作。\n目前最著名的服务网格架构是 Istio。\nIstio Istio 是最初由 IBM、Google 和 Lyft 开源的服务网格技术。它可以透明地分层到分布式应用上，并提供服务网格的所有优点，例如流量治理、安全性和可观测性等。\nIstio 能够适配本地部署、云托管、Kubernetes 容器以及虚拟机上运行的服务程序。通常与 Kubernetes 平台上部署的微服务一起使用。\n从根本上讲，Istio 的工作原理是以 Sidcar 的形式将 Envoy 的扩展版本作为代理布署到每个微服务中。其使用的代理网络构成了 Istio 的数据平面。而这些代理的配置和管理在控制平面完成，为数据平面中的 Envoy 代理提供发现、配置和证书管理。\nLinkerd Linkerd 是市场上出现的第一个服务网格。\nLinkerd 是 Buoyant 为 Kubernetes 设计的开源、超轻量级的服务网格。用 Rust 语言完全重写，使其尽可能小、轻和安全，它提供了运行时调试、可观测性、可靠性和安全性，而无需在分布式应用中更改代码。\nLinkerd 有三个基本组件：UI、数据平面和控制平面。Linkerd 通过在每个服务实例旁安装一组超轻、透明的代理来工作，这些代理会自动处理进出服务的所有流量。\n","categories":"","description":"本文介绍 Merbridge 项目中的一些关键概念\n","excerpt":"本文介绍 Merbridge 项目中的一些关键概念\n","ref":"/zh/docs/concepts/","tags":"","title":"概念"},{"body":"Merbridge is currently hosted on GitHub as an open source project. All code-related issues are managed on GitHub.\nIf you have any questions or suggestions about Merbridge, please create a New Issue on GitHub. We will review and process it as soon as possible.\nIf you find a bug in Merbridge and are interested in helping us fix it, you are more than welcome to share your fix code by creating a Pull Request. We will review and process it as soon as possible.\n","categories":"","description":"This page helps you make contributions to Merbridge.\n","excerpt":"This page helps you make contributions to Merbridge.\n","ref":"/docs/contribution-guidelines/","tags":"","title":"Make Contributions"},{"body":"Merbridge 目前托管在 GitHub 上进行开源，所有与代码相关的事情都在 GitHub 上进行管理。\n如果您对 Merbridge 有疑问，需要我们帮忙解决问题，或者想要提供一些新的功能，可以在 GitHub 上创建新的 Issue，我们会及时查看并处理。\n如果您发现了 Merbridge 的 bug，并且有兴趣帮助我们修复，那么非常欢迎您提交 Pull Request，附带上您的修复代码，我们会及时处理您的 PR。\n","categories":"","description":"本文介绍参与 Merbridge 项目的方式\n","excerpt":"本文介绍参与 Merbridge 项目的方式\n","ref":"/zh/docs/contribution-guidelines/","tags":"","title":"参与贡献"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/2022/","tags":"","title":"2022 年 Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2022/","tags":"","title":"Blogs of 2022"},{"body":" Get all information you need to know about Merbridge.\n Merbridge is eBPF-based and can accelerate the data plane of service meshes with a shorter packet datapath than iptables.\n","categories":"","description":"","excerpt":" Get all information you need to know about Merbridge.\n Merbridge is …","ref":"/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/releases/","tags":"","title":"Version Release"},{"body":" 帮助您了解如何使用 Merbridge\n Merbridge 旨在使用 eBPF 代替 iptables 技术，加速服务网格的数据平面。\n","categories":"","description":"","excerpt":" 帮助您了解如何使用 Merbridge\n Merbridge 旨在使用 eBPF 代替 iptables 技术，加速服务网格的数据平面。\n","ref":"/zh/docs/","tags":"","title":"文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/releases/","tags":"","title":"版本发布"},{"body":"Added  Support passive sockops. (#77) @kebe7jun . Use ingress path for message redirection and forwarding. (#82) @dddddai . Support helm-based deployment of Merbridge (#65) @Xunzhuo .  Fixed  Fix the key size of cookie_original_dst(#75) @dddddai.  ","categories":"","description":"What is new in Version 0.5.0?\n","excerpt":"What is new in Version 0.5.0?\n","ref":"/blog/2022/03/30/release-0.5.0/","tags":"","title":"Release 0.5.0"},{"body":"新增  增加对 passive sockops 的支持。 (#77) @kebe7jun . 使用 ingress path 进行消息重定向转发(#82) @dddddai . 支持使用 helm 部署 Merbridge (#65) @Xunzhuo .  修复  修复 cookie_original_dst 的 key 大小(#75) @dddddai .  ","categories":"","description":"0.5.0 版本更新内容。\n","excerpt":"0.5.0 版本更新内容。\n","ref":"/zh/blog/2022/01/27/release-0.5.0/","tags":"","title":"Release 0.5.0"},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  Merbridge Accelerate your mesh with eBPF        The service mesh technology represented by Istio is attracting more and more attention from enterprises. It uses sidecars and iptables to intercept traffic at the price of performance. Over the past two years, eBPF has become a trending technology, generating many projects based on eBPF.\n    eBPF’s sockops and redir capabilities allow more efficient processing of packets. Therefore, it is possible to accelerate the mesh with eBPF instead of iptables.      Merbridge is an eBPF-based open source project. It can help accelerate your meshes with eBPF by applying just one command in your Istio-managed cluster.     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/about/","tags":"","title":"Merbridge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Merbridge Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/","tags":"","title":"Merbridge Blog"},{"body":"","categories":"","description":"This page helps you identify and fix errors in installing Merbridge.\n","excerpt":"This page helps you identify and fix errors in installing Merbridge.\n","ref":"/docs/getting-started/trouble/","tags":"","title":"Installation Troubleshooting"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_top.jpg); } }  Merbridge: use eBPF to accelerate your mesh Learn More   Get Started   Let's explore Merbridge\n         Merbridge replaces iptables rules with eBPF to intercept traffic. It also combines msg_redirect to reduce latency with a shortened datapath between sidecars and services.\nFor details, see: Blog.\n      Documentation  If you have any problems in using Merbridge, Documentation may offer a quick solution.\n   Contribution  You can create a PR on GitHub to improve Merbridge. Your contribution is a big help！\nRead more …\n   Implementation  If you are interested in details of Merbridge, Blog may tell you more.\nRead more …\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"Merbridge"},{"body":"  #td-cover-block-0 { background-image: url(/zh/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_top.jpg); } }  Merbridge: 使用 eBPF 加速服务网格，提高服务性能。 了解更多   快速开始   开始探索 Merbridge 吧。\n         Merbridge 在服务网格中使用 eBPF 技术代替 iptables，实现流量拦截。借助 eBPF 和 msg_redirect 技术，Merbridge 可以提高 Sidecar 和应用之间的传输速度，降低延迟。\n如需了解更多实现细节或者原理，请参考：Blog。\n      查看文档  如您在使用 Merbridge 时遇到任何问题，请查看文档。\n   欢迎贡献！  欢迎您在 GitHub 上创建 PR 贡献自己的聪明才智。我们一直期待您的加入！\n更多 …\n   实现原理!  有关 Merbridge 的实现细节，请参阅博客\n更多 …\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/","tags":"","title":"Merbridge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Result"},{"body":"  #td-cover-block-0 { background-image: url(/zh/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  Merbridge 使用 eBPF 技术代替 iptables 加速服务网格。        以 Istio 为首的服务网格技术正在被越来越多的企业所关注。服务网格使用 Sidecar 借助 iptables 技术实现流量拦截，但是这样会损失不少性能。近两年，由于 eBPF 技术的兴起，不少围绕 eBPF 的项目应声而出。\n    借助 eBPF 的 sockops 和 redir 能力，可以高效地处理数据包。因此，可以结合实际场景，使用 eBPF 代替 iptables 为 Istio 实现加速。      现在，我们开源了 Merbridge 项目。您只需要在 Istio 集群执行一条命令，即可直接使用 eBPF 代替 iptables 实现网络加速！     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/about/","tags":"","title":"关于 Merbridge"},{"body":"","categories":"","description":"本文将帮助您进行 Merbridge 的错误排查和诊断\n","excerpt":"本文将帮助您进行 Merbridge 的错误排查和诊断\n","ref":"/zh/docs/getting-started/trouble/","tags":"","title":"安装错误排查"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/search/","tags":"","title":"搜索结果"}]
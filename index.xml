<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Merbridge – Merbridge</title><link>/</link><description>Recent content on Merbridge</description><generator>Hugo -- gohugo.io</generator><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Merbridge now supports Ambient Mesh, no worry about CNI compatibility!</title><link>/blog/2022/11/11/ambient-mesh-support/</link><pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate><guid>/blog/2022/11/11/ambient-mesh-support/</guid><description>
&lt;p>In the blog &lt;a href="../ambient-mesh-data-path/index.md">Deep Dive into Ambient Mesh - Traffic Path&lt;/a>,
we analyzed how Ambient Mesh forwards the ingress and egress traffic of Pod to ztunnel.
It is implemented by iptables + TPROXY + routing table. The traffic datapath is relatively
long compared to sidecar mode, and the principle is complicated. Moreover, it uses routing marks, which may cause
unexpected behaviors in some cases when it relies on CNI or it is running in a CNI with the bridge mode.
These severely limit the applicable scope of ambient mesh.&lt;/p>
&lt;p>The main purpose of Merbridge is to replace iptables with eBPF to accelerate applications running in
a service mesh. Ambient Mesh is a new mode of Istio. It is necessary for Merbridge to support this new mode.
iptables is a powerful tool to block unwanted traffic, allow desired traffic, and redirect packets to specific
addresses and ports, but it also has some weaknesses. First, iptables uses a linear matching method.
When several applications simultaneously call a same program, conflicts may arise and make some features become unavailable.
Second, although it is flexible enough, it still cannot be programmed as freely as eBPF.
Therefore, replacing iptables with eBPF can help Ambient Mesh achieve traffic interception.&lt;/p>
&lt;h2 id="objectives">Objectives&lt;/h2>
&lt;p>As mentioned in &lt;a href="../ambient-mesh-data-path/index.md">Deep Dive into Ambient Mesh - Traffic Path&lt;/a>,
we set two objectives:&lt;/p>
&lt;ul>
&lt;li>Outgoing traffic from pods in Ambient Mesh should be intercepted and redirected to port 15001 of ztunnel.&lt;/li>
&lt;li>Traffic sent from host applications to pods in Ambient Mesh should be redirected to port 15006 of ztunnel.&lt;/li>
&lt;/ul>
&lt;p>Since istioin and other network interface cards (NICs) are completely designed to adapt to the native Ambient Mesh, we don&amp;rsquo;t need to make any changes.&lt;/p>
&lt;h2 id="pain-points-analysis">Pain points analysis&lt;/h2>
&lt;p>Ambient Mesh has a different operation mechanism from sidecars. According to the official definition of Istio,
adding a Pod to Ambient Mesh does not require restarting the Pod and no any sidecar-related process is running in the Pod. It means:&lt;/p>
&lt;ol>
&lt;li>Merbridge used the CNI mode to enable the eBPF program to get the current Pod IP to make policy decisions,
which is incompatible with ambient mesh. The reason is a pod will not be restarted after joining or
leaving the ambient mesh, nor will it call the CNI plug-in.&lt;/li>
&lt;li>In the sidecar mode, the only thing you need to change is the destination address to &lt;code>127.0.0.1:15001&lt;/code> in
the connect hook of eBPF, but in the ambient mesh you need to replace the desitination IP with that of ztunnel.&lt;/li>
&lt;/ol>
&lt;p>In addtion, no sidecar-related process exists in a Pod running in an ambient mesh,
so the legacy method of checking whether a port such as 15006 is listening in the current Pod is no longer applicable.
It is necessary to redesign the scheme to check the environment where processes are running.&lt;/p>
&lt;p>Therefore, based on the above analysis, it is required to redesign the entire interception scheme so that Merbridge can support the ambient mesh.&lt;/p>
&lt;p>In summary, we need to implement the following features:&lt;/p>
&lt;ul>
&lt;li>Redesign a scheme for judging whether a Pod is running in the ambient mesh&lt;/li>
&lt;li>Use eBPF to perceive current Pod IP regardless of CNIs&lt;/li>
&lt;li>Enable eBPF programs to know the ztunnel IP on the current node&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>In version 0.7.2, cgroup id is used to improve the performance of the connect program.
Usually, each container in a Pod has a proper cgroup id, which can be obtained through the &lt;code>bpf_get_current_cgroup_id&lt;/code>
function in the BPF program. The speed of the connect program can be optimized by writing IP information to a specific &lt;code>cgroup_info_map&lt;/code>.&lt;/p>
&lt;p>An ambient mesh is different from the legacy CNI listening on a special port in the network namespace for storing Pod-related information.
In the ambient mesh, cgroup id is useful. If cgroup id can be associated with the Pod IP, you can get the current Pod IP in eBPF.&lt;/p>
&lt;p>Since CNI cannot be relied on anymore, we need change the scheme for obtaining the information of Pod status.
For this reason, we detect the creation and revocation actions of local Pods by watching the process creation and revocation.
We created a new tool to watch the process changes on a host: &lt;a href="https://github.com/merbridge/process-watcher">process-watcher project&lt;/a>.&lt;/p>
&lt;p>Read the cgroup id and ip information from the process ID and writing it to the &lt;code>cgroup_info_map&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#000">tcg&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">:=&lt;/span> &lt;span style="color:#000">cgroupInfo&lt;/span>&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#000">ID&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000">cgroupInode&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#000">IsInMesh&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000">in&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#000">CgroupIp&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>&lt;span style="color:#204a87;font-weight:bold">uint32&lt;/span>&lt;span style="color:#000;font-weight:bold">)(&lt;/span>&lt;span style="color:#000">_ip&lt;/span>&lt;span style="color:#000;font-weight:bold">),&lt;/span>
&lt;span style="color:#000">Flags&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000">flag&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#000">DetectedFlags&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000">cgrinfo&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">DetectedFlags&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> &lt;span style="color:#000">AMBIENT_MESH_MODE_FLAG&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> &lt;span style="color:#000">ZTUNNEL_FLAG&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">return&lt;/span> &lt;span style="color:#000">ebpfs&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">GetCgroupInfoMap&lt;/span>&lt;span style="color:#000;font-weight:bold">().&lt;/span>&lt;span style="color:#000">Update&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">cgroupInode&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">tcg&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">ebpf&lt;/span>&lt;span style="color:#000;font-weight:bold">.&lt;/span>&lt;span style="color:#000">UpdateAny&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then get the current cgroup-related information in eBPF:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-c" data-lang="c">&lt;span style="color:#000">__u64&lt;/span> &lt;span style="color:#000">cgroup_id&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">bpf_get_current_cgroup_id&lt;/span>&lt;span style="color:#000;font-weight:bold">();&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">void&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000">info&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">bpf_map_lookup_elem&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">cgroup_info_map&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">cgroup_id&lt;/span>&lt;span style="color:#000;font-weight:bold">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, we can learn whether the current container has the ambient mesh enabled and it is located in a mesh or not.&lt;/p>
&lt;p>Second, for the ztunnel IP, Istio implements it by adding NIC and binding fixed IPs.
This scheme may have the risk of conflict, and the original addresses may be lost in some cases (such as SNAT).
So Merbridge gives up the scheme and directly obtains the ztunnel IPs on the control plane,
writes it into the map, and enables the eBPF program read it (this is faster).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-c" data-lang="c">&lt;span style="color:#204a87;font-weight:bold">static&lt;/span> &lt;span style="color:#204a87;font-weight:bold">inline&lt;/span> &lt;span style="color:#000">__u32&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000">get_ztunnel_ip&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#000">__u32&lt;/span> &lt;span style="color:#000">ztunnel_ip_key&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">ZTUNNEL_KEY&lt;/span>&lt;span style="color:#000;font-weight:bold">;&lt;/span>
&lt;span style="color:#204a87;font-weight:bold">return&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">__u32&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>&lt;span style="color:#000">bpf_map_lookup_elem&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">settings&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;amp;&lt;/span>&lt;span style="color:#000">ztunnel_ip_key&lt;/span>&lt;span style="color:#000;font-weight:bold">);&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then use the connect program to rewrite the destination address:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-c" data-lang="c">&lt;span style="color:#000">ctx&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">-&amp;gt;&lt;/span>&lt;span style="color:#000">user_ip4&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">ztunnel_ip&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">3&lt;/span>&lt;span style="color:#000;font-weight:bold">];&lt;/span>
&lt;span style="color:#000">ctx&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">-&amp;gt;&lt;/span>&lt;span style="color:#000">user_port&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">bpf_htons&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">OUT_REDIRECT_PORT&lt;/span>&lt;span style="color:#000;font-weight:bold">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the association with the cgroup id, the Pod IP of current processes can be obtained in eBPF, so as to enforce policies.
Forward the traffic from the Pod in the ambient mesh to the ztunnel, so that Merbridge can be compatible with the ambient mesh.&lt;/p>
&lt;p>This will be a capability that is adaptable to all CNIs and can avoid the problem that the native ambient mesh cannot work well in most CNI modes.&lt;/p>
&lt;h2 id="usage-and-feedback">Usage and feedback&lt;/h2>
&lt;p>Since the ambient mesh is still in its early stage and the support for ambient mode is relatively preliminary,
some problems have not been well resolved, so the code of supporting for the ambient mode has not been merged into the main branch.
If you want to experience the capability of Merbridge to implement traffic interception for ambient mesh instead of iptables,
you can perform the following steps (it is required to install the ambient mesh in advance):&lt;/p>
&lt;ol>
&lt;li>Disable Istio CNI (set &lt;code>--set components.cni.enabled=false&lt;/code> during installation, or delete Istio CNI&amp;rsquo;s DaemonSet &lt;code>kubectl -n istio-system delete ds istio-cni&lt;/code>).&lt;/li>
&lt;li>Remove the init container of ztunnel (because it initializes iptables rules and NICs, which is not required for Merbridge).&lt;/li>
&lt;li>Install Merbridge by running &lt;code>kubectl apply -f https://github.com/merbridge/merbridge/raw/ambient/deploy/all-in-one.yaml&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>After the Merbridge is ready, you can use all capabilities of ambient mesh.&lt;/p>
&lt;p>&lt;strong>*Attentions:&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>The Ambient mode under Kind is not supported currently (we have a plan to support it in the future)&lt;/li>
&lt;li>The host kernel version needs to be not less than 5.7&lt;/li>
&lt;li>cgroup v2 is required to be enabled&lt;/li>
&lt;li>This mode is also compatible with sidecars&lt;/li>
&lt;li>The debug mode will be enabled by default in an ambient mesh, which will have certain impact on performance&lt;/li>
&lt;/ol>
&lt;p>For more details see &lt;a href="https://github.com/merbridge/merbridge/tree/ambient">source code&lt;/a>.&lt;/p>
&lt;p>If you have any question, please reach out to us with &lt;a href="https://join.slack.com/t/merbridge/shared_invite/zt-11uc3z0w7-DMyv42eQ6s5YUxO5mZ5hwQ">Slack&lt;/a> or add the wechat group to chat.&lt;/p></description></item><item><title>Blog: Merbridge helps Kuma reduce network latency by 12%</title><link>/blog/2022/11/08/kuma-2.0-with-merbridge/</link><pubDate>Tue, 08 Nov 2022 00:00:00 +0000</pubDate><guid>/blog/2022/11/08/kuma-2.0-with-merbridge/</guid><description>
&lt;p>Recently, Kuma announced a major release of &lt;a href="https://kuma.io/blog/2022/kuma-2-0-0/">v2.0&lt;/a> with several new major features.
A notable feature is that Kuma is using eBPF to improve the traffic flow.&lt;/p>
&lt;p>&lt;img src="./imgs/kuma-2.0-release-preview.png" alt="Kuma 2.0 release preview">&lt;/p>
&lt;p>Based on the official release notes and blogs, Kuma implements the eBPF capabilities by integrating with Merbridge.&lt;/p>
&lt;p>&lt;img src="./imgs/kuma-2.0-ebpf-vs-iptables.png" alt="Performance comparison between eBPF and iptables for Kuma 2.0">&lt;/p>
&lt;p>A quote from &lt;a href="https://kuma.io/blog/2022/kuma-2-0-0/">Kuma 2.0 release blog&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>We are utilizing the Merbridge OSS project within our eBPF capabilities and are very excited
that we have been able to contribute back to that library and become co-maintainers. We look forward to working more
with the Merbridge team as we continue to explore different areas to include eBPF functionality in Kuma.&lt;/p>
&lt;/blockquote>
&lt;p>As an open source project, we are very excited to see that Merbridge brings such capabitilities to Kuma.
This case proves that traffic latency can be reduced without any extra overhead if you use Merbridge in a service mesh.&lt;/p>
&lt;p>Since June this year, Kuma developers have been working on integrating with Merbridge, trying to get the eBPF-based acceleration capabilities.&lt;/p>
&lt;p>Thanks to the clear architecture of Merbridge, Kuma is smoothly adapted with Merbridge in days.
A big thanks to the Kuma community for contributing such an important compatibility capability to Merbridge, which helps both communities grow together!&lt;/p>
&lt;p>So far, Merbridge has the capabilities to support popular service mesh products like Istio, Linkerd2, and Kuma,
and also has a clear plan to develop new features to support IPv4/IPv6 dual-stack, ambient mesh, and earlier versions of kernel.
It is exciting to see that Merbridge gets used more widely.
We really hope the project can help you land your project with eBPF technologies.
We are looking forward to receiving more comments, and having more developers get involved.&lt;/p></description></item><item><title>Blog: Deep Dive into Ambient Mesh - Traffic Path</title><link>/blog/2022/10/13/ambient-mesh-data-path/</link><pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate><guid>/blog/2022/10/13/ambient-mesh-data-path/</guid><description>
&lt;p>Ambient Mesh has been released for a while, and some online articles have talked much about its usage and architecture.
This blog will dive into the traffic path on data plane in the ambient mesh to help you fully understand
the implementations of the ambient data plane.&lt;/p>
&lt;p>Before start, you shall carefully read through &lt;a href="https://istio.io/latest/blog/2022/introducing-ambient-mesh/">introducing ambient mesh&lt;/a>
to learn the basic knowledge of the ambient mesh.&lt;/p>
&lt;blockquote>
&lt;p>For your convenience, the test environment can be deployed by following
&lt;a href="https://istio.io/latest/blog/2022/get-started-ambient/">Get Started with Istio Ambient Mesh&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="start-from-the-moment-you-make-a-request">Start from the moment you make a request&lt;/h2>
&lt;p>In order to explore the traffic path, we first analyze the scenario where two services access each other in
the ambient mesh (only for L4 mode on different nodes).&lt;/p>
&lt;p>After enabling the ambient mesh in the &lt;code>default&lt;/code> namespace, all services will have capabilities of mesh governance.&lt;/p>
&lt;p>Our analysis starts from this command: &lt;code>kubectl exec deploy/sleep -- curl -s http://productpage:9080/ | head -n1&lt;/code>&lt;/p>
&lt;p>In the sidecar mode, Istio intercepts traffic through iptables.
When you run the curl command in a sleep pod, the traffic will be forwarded by iptables to port 15001 of sidecar.
However, in the ambient mesh, no sidecar exists in a pod, and it does not need restart to enable the ambient mesh.
How to make sure the request is processed by ztunnel?&lt;/p>
&lt;h2 id="egress-traffic-interception">Egress traffic interception&lt;/h2>
&lt;p>To learn details about intercepting the egress traffic, let&amp;rsquo;s check the control plane components:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl -n istio-system get po
NAME READY STATUS RESTARTS AGE
istio-cni-node-5rh5z 1/1 Running 0 20h
istio-cni-node-qsvsz 1/1 Running 0 20h
istio-cni-node-wdffp 1/1 Running 0 20h
istio-ingressgateway-5cfcb57bd-kx9hx 1/1 Running 0 20h
istiod-6b84499b75-ncmn7 1/1 Running 0 20h
ztunnel-nptf6 1/1 Running 0 20h
ztunnel-vxv4b 1/1 Running 0 20h
ztunnel-xkz4s 1/1 Running 0 20h
&lt;/code>&lt;/pre>&lt;p>In the sidecar mode, istio-cni is mainly a CNI to avoid permission
leakage caused by using the istio-init container to process iptables rules.
However, in the ambient mesh, istio-cni becomes a required component.
Sidecars are theoretically not needed. Why is the istio-cni component being required?&lt;/p>
&lt;p>Let&amp;rsquo;s check the logs:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl -n istio-system logs istio-cni-node-qsvsz
...
2022-10-12T07:34:33.224957Z info ambient Adding route for reviews-v1-6494d87c7b-zrpks/default: [table 100 10.244.1.4/32 via 192.168.126.2 dev istioin src 10.244.1.1]
2022-10-12T07:34:33.226054Z info ambient Adding pod 'reviews-v2-79857b95b-m4q2g/default' (0ff78312-3a13-4a02-b39d-644bfb91e861) to ipset
2022-10-12T07:34:33.228305Z info ambient Adding route for reviews-v2-79857b95b-m4q2g/default: [table 100 10.244.1.5/32 via 192.168.126.2 dev istioin src 10.244.1.1]
2022-10-12T07:34:33.229967Z info ambient Adding pod 'reviews-v3-75f494fccb-92nq5/default' (e41edf7c-a347-45cb-a144-97492faa77bf) to ipset
2022-10-12T07:34:33.232236Z info ambient Adding route for reviews-v3-75f494fccb-92nq5/default: [table 100 10.244.1.6/32 via 192.168.126.2 dev istioin src 10.244.1.1]
&lt;/code>&lt;/pre>&lt;p>As shown in the above output, for a pod in the ambient mesh, istio-cni performs the following actions:&lt;/p>
&lt;ol>
&lt;li>Add the pod to ipset&lt;/li>
&lt;li>Add a routing rule to table 100 (for its usage see below)&lt;/li>
&lt;/ol>
&lt;p>You can view the ipset contents on the node (note that the kind cluster is used here,
you need to use &lt;code>docker exec&lt;/code> to enter the host first):&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ docker exec -it ambient-worker2 bash
root@ambient-worker2:/# ipset list
Name: ztunnel-pods-ips
Type: hash:ip
Revision: 0
Header: family inet hashsize 1024 maxelem 65536
Size in memory: 520
References: 1
Number of entries: 5
Members:
10.244.1.5
10.244.1.7
10.244.1.8
10.244.1.4
10.244.1.6
&lt;/code>&lt;/pre>&lt;p>It is found that an ipset exists on the node where this pod is running. ipset holds many IPs for pods.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl get po -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
details-v1-76778d6644-wn4d2 1/1 Running 0 20h 10.244.1.9 ambient-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
notsleep-6d6c8669b5-pngxg 1/1 Running 0 20h 10.244.2.5 ambient-worker &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
productpage-v1-7c548b785b-w9zl6 1/1 Running 0 20h 10.244.1.7 ambient-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
ratings-v1-85c74b6cb4-57m52 1/1 Running 0 20h 10.244.1.8 ambient-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
reviews-v1-6494d87c7b-zrpks 1/1 Running 0 20h 10.244.1.4 ambient-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
reviews-v2-79857b95b-m4q2g 1/1 Running 0 20h 10.244.1.5 ambient-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
reviews-v3-75f494fccb-92nq5 1/1 Running 0 20h 10.244.1.6 ambient-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
sleep-7b85956664-z6qh7 1/1 Running 0 20h 10.244.2.4 ambient-worker &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;p>Therefore, this ipset holds a list of all PodIPs in the ambient mesh on the current node.&lt;/p>
&lt;p>Where can this ipset be used?&lt;/p>
&lt;p>Let&amp;rsquo;s take a look at the iptables rules and you can find:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">root@ambient-worker2:/# iptables-save
*mangle
...
-A POSTROUTING -j ztunnel-POSTROUTING
...
-A ztunnel-PREROUTING -p tcp -m set --match-set ztunnel-pods-ips src -j MARK --set-xmark 0x100/0x100
&lt;/code>&lt;/pre>&lt;p>You now learn that when a pod in the ambient mesh on a node (in the &lt;code>ztunnel-pods-ips&lt;/code> ipset) initiates a request,
its connection will be marked with &lt;code>0x100/0x100&lt;/code>.&lt;/p>
&lt;p>Generally, it will be related to routing. Let&amp;rsquo;s check the routing rules:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">root@ambient-worker2:/# ip rule
0: from all lookup local
100: from all fwmark 0x200/0x200 goto 32766
101: from all fwmark 0x100/0x100 lookup 101
102: from all fwmark 0x40/0x40 lookup 102
103: from all lookup 100
32766: from all lookup main
32767: from all lookup default
&lt;/code>&lt;/pre>&lt;p>The traffic marked with &lt;code>0x100/0x100&lt;/code> goes via the routing table 101. Let&amp;rsquo;s check the routing table:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">root@ambient-worker2:/# ip r show table 101
default via 192.168.127.2 dev istioout
10.244.1.2 dev veth5db63c11 scope link
&lt;/code>&lt;/pre>&lt;p>It can be clearly seen that the default gateway has been replaced with &lt;code>192.168.127.2&lt;/code> via the istioout NIC (network interface card).&lt;/p>
&lt;p>&lt;code>192.168.127.2&lt;/code> does not belong to any of NodeIP, PodIP, and ClusterIP.
The istioout NIC should not exist by default, then where does this IP come from?
Since the traffic ultimately needs to go to ztunnel,
you can check the ztunnel configuration to see if you can find the answer.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl -n istio-system get po ztunnel-vxv4b -o yaml
apiVersion: v1
kind: Pod
metadata:
...
name: ztunnel-vxv4b
namespace: istio-system
...
spec:
...
initContainers:
- command:
...
OUTBOUND_TUN=istioout
...
OUTBOUND_TUN_IP=192.168.127.1
ZTUNNEL_OUTBOUND_TUN_IP=192.168.127.2
ip link add name p$INBOUND_TUN type geneve id 1000 remote $HOST_IP
ip addr add $ZTUNNEL_INBOUND_TUN_IP/$TUN_PREFIX dev p$INBOUND_TUN
ip link add name p$OUTBOUND_TUN type geneve id 1001 remote $HOST_IP
ip addr add $ZTUNNEL_OUTBOUND_TUN_IP/$TUN_PREFIX dev p$OUTBOUND_TUN
ip link set p$INBOUND_TUN up
ip link set p$OUTBOUND_TUN up
...
&lt;/code>&lt;/pre>&lt;p>As above, ztunnel will be responsible for creating the istioout NIC, you now go to the node to check the NIC.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">root@ambient-worker2:/# ip a
11: istioout: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN group default
link/ether 0a:ea:4e:e0:8d:26 brd ff:ff:ff:ff:ff:ff
inet 192.168.127.1/30 brd 192.168.127.3 scope global istioout
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>Where is the gateway IP of &lt;code>192.168.127.2&lt;/code>? It is allocated in ztunnel.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a
Defaulted container &amp;quot;istio-proxy&amp;quot; out of: istio-proxy, istio-init (init)
2: eth0@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0
valid_lft forever preferred_lft forever
inet6 fe80::448a:46ff:fe72:1d3b/64 scope link
valid_lft forever preferred_lft forever
4: pistioout: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000
link/ether c2:d0:18:20:3b:97 brd ff:ff:ff:ff:ff:ff
inet 192.168.127.2/30 scope global pistioout
valid_lft forever preferred_lft forever
inet6 fe80::c0d0:18ff:fe20:3b97/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>You can now see that the traffic is going to ztunnel, but nothing else is done to the traffic at this time,
it is simply routed to ztunnel. How to does Envoy in ztunnel process the traffic?&lt;/p>
&lt;p>Let&amp;rsquo;s continue to check the ztunnel configuration with many iptables rules. Let&amp;rsquo;s check the specific rules in ztunnel.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save
Defaulted container &amp;quot;istio-proxy&amp;quot; out of: istio-proxy, istio-init (init)
...
*mangle
-A PREROUTING -i pistioout -p tcp -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
...
COMMIT
&lt;/code>&lt;/pre>&lt;p>When traffic enters ztunnel, it will use TPROXY to transfer the traffic to port 15001 for processing,
where 15001 is the port that Envoy actually listens to and process the pod egress traffic.
As for TPROXY, you can learn relevant reference, and this blog will not repeat it further.&lt;/p>
&lt;p>So when a pod is running in the ambient mesh, its egress traffic path is as follows:&lt;/p>
&lt;ol>
&lt;li>Initiate traffic from a process in pod&lt;/li>
&lt;li>The traffic flows via the node network and get marks by iptables on the node&lt;/li>
&lt;li>The traffic is forwarded to the ztunnel pod on current node by the routing table&lt;/li>
&lt;li>When the traffic reaches ztunnel, it will go through iptables for TPROXY (transparent proxy),
and send the traffic to port 15001 of Envoy in the current pod.&lt;/li>
&lt;/ol>
&lt;p>So far in the ambient mesh, it is clear that the processing of pod egress traffic is relatively complex.
The path is also relatively long, unlike the sidecar mode in which a traffic forwarding is directly completed in the pod.&lt;/p>
&lt;h2 id="ingress-traffic-interception">Ingress traffic interception&lt;/h2>
&lt;p>With the above experience, it is easy to learn that in the ambient mesh, the traffic interception is mainly
through the method of MARK routing + TPROXY, and the ingress traffic should be similar.&lt;/p>
&lt;p>Let&amp;rsquo;s analyze it in the simplest way. When a process on a node, or a program on another host accesses
a pod on the current node, the traffic goes through the host&amp;rsquo;s routing table.
Let&amp;rsquo;s check the routing info when the response arrives at &lt;code>productpage-v1-7c548b785b-w9zl6(10.244.1.7)&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">root@ambient-worker2:/# ip r get 10.244.1.7
10.244.1.7 via 192.168.126.2 dev istioin table 100 src 10.244.1.1 uid 0
cache
&lt;/code>&lt;/pre>&lt;p>When accessing &lt;code>10.244.1.7&lt;/code>, the traffic will be routed to &lt;code>192.168.126.2&lt;/code>, and this rule is added by istio-cni.&lt;/p>
&lt;p>Similarly &lt;code>192.168.126.2&lt;/code> belongs to ztunnel:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a
Defaulted container &amp;quot;istio-proxy&amp;quot; out of: istio-proxy, istio-init (init)
2: eth0@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default
link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0
valid_lft forever preferred_lft forever
inet6 fe80::448a:46ff:fe72:1d3b/64 scope link
valid_lft forever preferred_lft forever
3: pistioin: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000
link/ether 7e:b2:e6:f9:a4:92 brd ff:ff:ff:ff:ff:ff
inet 192.168.126.2/30 scope global pistioin
valid_lft forever preferred_lft forever
inet6 fe80::7cb2:e6ff:fef9:a492/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>By using the same analysis method, let&amp;rsquo;s check the iptables rules:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save
...
-A PREROUTING -i pistioin -p tcp -m tcp --dport 15008 -j TPROXY --on-port 15008 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
-A PREROUTING -i pistioin -p tcp -j TPROXY --on-port 15006 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
...
&lt;/code>&lt;/pre>&lt;p>If you directly access the node via PodIP + pod port, the traffic will be forwarded to port 15006 of ztunnel,
which is the port to handle the ingress traffic in Istio.&lt;/p>
&lt;p>As for the traffic whose destination port is port 15008, this is the port used by ztunnel for L4 traffic tunneling.
This blog will not explain this further.&lt;/p>
&lt;h2 id="handle-traffic-for-envoy-itself">Handle traffic for Envoy itself&lt;/h2>
&lt;p>In the sidecar mode, Envoy and user containers run in the same network namespace.
For the traffic from user containers, you need to intercept all the traffic to
guarantee complete control of the traffic. However, is it also required in the ambient mesh?&lt;/p>
&lt;p>The answer is no, because Envoy has been isolated from other pods.
The traffic sent by Envoy does not require special notice.
In other words, you only need to handle ingress traffic for ztunnel,
so the rules in ztunnel seem relatively simple.&lt;/p>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>As above explained, this blog mainly analyzed the scheme for Pod traffic interception in the ambient mesh,
but this blog has not involved with how to handle L7 traffic and the specific principles of ztunnel implementation.
The next plan is to analyze the detailed traffic paths in ztunnel and waypoint proxy.&lt;/p></description></item><item><title>Blog: Merbridge CNI Mode</title><link>/blog/2022/05/18/cni-mode/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>/blog/2022/05/18/cni-mode/</guid><description>
&lt;!--Merbridge CNI 模式的出现，旨在能够更好地适配服务网格的功能。之前没有 CNI 模式时，Merbridge 能够做得事情比较有限。其中最大的问题是不能适配注入 Istio 的 Sidecar Annotation，这就导致 Merbridge 无法排除某些端口或 IP 段的流量等。同时，由于之前 Merbridge 只处理 Pod 内部的连接请求，这就导致，如果是外部发送到 Pod 的流量，Merbridge 将无法处理。-->
&lt;p>The CNI mode is designed to better adapt to the service mesh functions. Before having this mode, Merbridge was limited to certain scenarios. The biggest problem was that it could not adapt to the sidecar annotations from the container injected by Istio, which led to Merbridge cannot exclude traffic from certain ports and IP ranges. Furthermore, Merbridge was only able to handle requests inside the pod, which means the external traffic sent to the pod was not handled.&lt;/p>
&lt;!--为此，我们精心设计了 Merbridge CNI，旨在解决这些问题。-->
&lt;p>Therefore, we have implemented the Merbridge CNI to address these issues.&lt;/p>
&lt;!--## 为什么需要 CNI 模式？-->
&lt;h2 id="why-cni-mode-is-needed">Why CNI mode is needed&lt;/h2>
&lt;!--其一，之前的 Merbridge 只有一个很小的控制面，其监听 Pod 资源，将当前节点的 IP 信息写入 `local_pod_ips` 的 map，以供 connect 使用。但是，connect 程序由于工作在主机内核层，其无法知道当前正在处理的是哪个 Pod 的流量，就没法处理如 `excludeOutboundPorts` 等配置。为了能够适配注入 `excludeOutboundPorts` 的 Sidecar Annotation，我们需要让 eBPF 程序能够得知当前正在处理哪个 Pod 的请求。-->
&lt;p>First, Merbridge had a small control plane before, which listened to pods resources, and wrote the current node IP into the map of &lt;code>local_pod_ips&lt;/code> for use by &lt;code>connect&lt;/code>. However, since the &lt;code>connect&lt;/code> program only works at the host kernel layer, it won&amp;rsquo;t know which pod&amp;rsquo;s traffic is being processed. Thus, configurations like &lt;code>excludeOutboundPorts&lt;/code> cannot be handled. In order to be able to adapt to the injected sidecar annotation &lt;code>excludeOutboundPorts&lt;/code>, we need to let the eBPF program know which Pod&amp;rsquo;s request is currently being processed.&lt;/p>
&lt;!--为此，我们设计了一套方法，与 CNI 配合，能够获取当前 Pod 的 IP，以适配针对 Pod 的特殊配置。-->
&lt;p>To this end, we have designed a method to cooperate with the CNI, through which you can get the current Pod IP to validate special configurations for the Pod.&lt;/p>
&lt;!--其二，在之前的 Merbridge 版本中，只有 connect 会处理主机发起的请求，这在同一台主机上的 Pod 互相通讯时，是没有问题的。但是在不同主机之间通讯时就会出现问题，因为按照之前的逻辑，在跨节点通讯时流量不会被修改，这会导致在接收端还是离不开 iptables。-->
&lt;p>Second, for early versions of Merbridge, only &lt;code>connect&lt;/code> would process requests from the host, which had no problem for intra-node pod communication. However, it becomes problematic when traffic flows between different nodes. According to the previous logic, the traffic will not be modified during the cross-node communication, which will lead to the use of &lt;code>iptables&lt;/code> at the end.&lt;/p>
&lt;!--这次，我们依靠 XDP 程序，解决入口流量处理的问题。因为 XDP 程序需要挂载网卡，所以也需要借助 CNI。-->
&lt;p>Here, we turned to the XDP program for processing the inbound traffic. The XDP program needs to mount a network card, which also needs to use CNI.&lt;/p>
&lt;!--## CNI 如何解决问题？-->
&lt;h2 id="how-does-cni-work">How does CNI work&lt;/h2>
&lt;!--这里我们将探讨 CNI 的工作原理，以及如何使用 CNI 来解决问题。-->
&lt;p>This section will explore how CNI works and how to use CNI to solve the issues mentioned above.&lt;/p>
&lt;!--### 如何通过 CNI 让 eBPF 程序获取当前正在处理的 Pod IP？-->
&lt;h3 id="how-to-use-cni-to-let-ebpf-have-the-current-pod-ip">How to use CNI to let eBPF have the current Pod IP&lt;/h3>
&lt;!--我们通过 CNI，在 Pod 创建的时候，将 Pod 的 IP 信息写入一个 Map（`mark_pod_ips_map`），其 Key 为一个随机的值，Value 为 Pod 的 IP。然后，在当前 Pod 的 NetNS 里面监听一个特殊的端口 39807，将 Key 使用 `setsockopt` 写入这个端口 socket 的 mark。-->
&lt;p>When a pod is created, we write Pod IP into the map &lt;code>mark_pod_ips_map&lt;/code> through CNI, where the key is a random value, and the value is the Pod IP. Then, we listen to a special port &lt;code>39807&lt;/code> in the NetNS of the current Pod, and write the key to the mark of this port socket using &lt;code>setsockopt&lt;/code>.&lt;/p>
&lt;!--在 eBPF 中，我们通过 `bpf_sk_lookup_tcp` 取得端口 39807 的 Mark 信息，然后从 `mark_pod_ips_map` 中即可取得当前 NetNS（也是当期 Pod）的 IP。-->
&lt;p>In eBPF, we get the recorded mark information of port &lt;code>39807&lt;/code> through &lt;code>bpf_sk_lookup_tcp&lt;/code>, and use it to get the current Pod IP (also the current NetNS) from &lt;code>mark_pod_ips_map&lt;/code>.&lt;/p>
&lt;!--有了当前 Pod IP 之后，我们可以根据这个 Pod 的配置，确认流量处理路径（比如 `excludeOutboundPorts`）。-->
&lt;p>With the current Pod IP, we can determine the path to route traffic (such as &lt;code>excludeOutboundPorts&lt;/code>) according to the configuration of this Pod.&lt;/p>
&lt;!--同时，我们还使用 Pod 优化了之前解决四元组冲突的方案，改为使用 `bpf_bind` 绑定源 IP，目的 IP 直接使用 `127.0.0.1`，为了后续支持 IPv6 做准备。-->
&lt;p>In addition, we also optimized the quadruple conflicts by using &lt;code>bpf_bind&lt;/code> to bind the source IP and using &lt;code>127.0.0.1&lt;/code> as the destination IP, which also prepares for future support of IPv6.&lt;/p>
&lt;!--### 如何处理入口流量？-->
&lt;h3 id="how-to-handle-ingress-traffic">How to handle ingress traffic&lt;/h3>
&lt;!--为了能够处理入口流量，我们引入了 XDP 程序，XDP 程序作用在网卡上，能够对原始数据包做修改。-->
&lt;p>In order to handle inbound traffic, we introduced the XDP program, which works on the network card and can modify the original data packets.&lt;/p>
&lt;!--我们借助 XDP 程序，在流量到达 Pod 的时候，修改目的端口为 15006 以完成流量转发。-->
&lt;p>We use the XDP program to modify the destination port as &lt;code>15006&lt;/code> when the traffic reaches the Pod, so as to complete traffic forwarding.&lt;/p>
&lt;!--同时考虑到可能存在主机直接访问 Pod 的情况，也为了减小影响范围，我们选择将 XDP 程序附加到 Pod 的网卡上。这就需要借助 CNI 的能力，在创建 Pod 时进行附加操作。-->
&lt;p>At the same time, considering the possibility that the host directly accesses the Pod, and in order to reduce the scope of influence, we choose to attach the XDP program to the Pod&amp;rsquo;s network card. This requires the ability of CNI to perform additional operations when creating Pods&lt;/p>
&lt;!--## 如何体验 CNI 模式？-->
&lt;h2 id="how-to-use-cni-mode">How to use CNI mode?&lt;/h2>
&lt;!--CNI 模式默认被关闭，需要手动开启。-->
&lt;p>CNI mode is disabled by default. You need to enable it manually with the following command.&lt;/p>
&lt;!--可以使用以下命令一键开启：-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml &lt;span style="color:#000;font-weight:bold">|&lt;/span> sed &lt;span style="color:#4e9a06">&amp;#39;s/--cni-mode=false/--cni-mode=true/g&amp;#39;&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> kubectl apply -f -
&lt;/code>&lt;/pre>&lt;/div>&lt;!--## 注意事项-->
&lt;h2 id="notes">Notes&lt;/h2>
&lt;!--### CNI 模式处于测试阶段-->
&lt;h3 id="cni-mode-is-in-beta">CNI mode is in beta&lt;/h3>
&lt;!--CNI 模式刚被设计和开发出来，可能存在不少问题，我们欢迎大家在测试阶段进行反馈，或者提出更好的建议，以帮助我们改进 Merbridge！-->
&lt;p>The CNI mode is a new feature that may not be perfect. We welcome your feedback and suggestions to help improve Merbridge.&lt;/p>
&lt;!--如果需要使用注入 Istio perf benchmark 等工具进行测试性能，请开启 CNI 模式，否则会导致性能测试结果不准确。-->
&lt;p>If you are trying to do benchmark test using tools like &lt;code>Istio perf benchmark&lt;/code>, it is suggested to enable the CNI mode. Otherwise the test results will be inaccurate.&lt;/p>
&lt;!--### 需要注意主机是否可开启 hardware-checksum 能力-->
&lt;h3 id="check-whether-the-host-can-enable-the-hardware-checksum-capability">Check whether the host can enable the hardware-checksum capability&lt;/h3>
&lt;!--为了保证 CNI 模式的正常运行，我们默认关闭了 hardware-checksum 能力，这可能会影响到网络性能。建议大家在开启 CNI 模式前，先确认主机是否可开启 hardware-checksum 能力。如果可以开启，建议设置 `--hardware-checksum=true` 以获得最佳的性能表现。-->
&lt;p>In order to ensure the CNI mode works properly, the &lt;code>hardware-checksum&lt;/code> capability is disabled by default, which may affect network performance. It is recommended to check whether you can enable this capability on the host before enabling the CNI mode. If yes, we suggest to set &lt;code>--hardware-checksum=true&lt;/code> for best performance.&lt;/p>
&lt;!--测试方法：`ethtool -k &lt;网卡> | grep tx-checksum-ipv4` 为 on 表示开启。-->
&lt;p>Test method: if &lt;code>ethtool -k &amp;lt;network card&amp;gt; | grep tx-checksum-ipv4&lt;/code> is on, it means enabled.&lt;/p></description></item><item><title>Blog: Merbridge and Cilium</title><link>/blog/2022/04/23/merbridge-and-cilium/</link><pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate><guid>/blog/2022/04/23/merbridge-and-cilium/</guid><description>
&lt;h1 id="merbridge-and-cilium">Merbridge and Cilium&lt;/h1>
&lt;p>&lt;a href="https://cilium.io/">Cilium&lt;/a> is a great open source software that provides a lot of networking capabilities for cloud native applications based on eBPF, with a lot of great designs. Among others, Cilium designed a set of sockmap-based redir capabilities to help accelerate network communications, which inspired us and is the basis for Merbridge to provide network acceleration. It is a really great design.&lt;/p>
&lt;p>Merbridge leverages the great foundation that Cilium has provided, along with some targeted adaptations we&amp;rsquo;ve made in the Service Mesh, to make it easier to apply eBPF technology to Service Mesh.&lt;/p>
&lt;p>Our development team have learned a lot eBPF theoretical knowledge, practical methods, and testing methods, from Cilium&amp;rsquo;s detailed documentation and our frequent exchanges with the Cilium technical team. All these together helps make Merbridge possible.&lt;/p>
&lt;p>Thanks again to the Cilium project and community, and to Cilium for these great designs.&lt;/p></description></item><item><title>Blog: Livestream with Solo.io</title><link>/blog/2022/03/29/solo-io-livestream/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/29/solo-io-livestream/</guid><description>
&lt;p>On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.&lt;/p>
&lt;p>In this livestream, we discussed a lot of Merbridge-related issues, including a live demo that will help you get a quick overview of Merbridge&amp;rsquo;s features and usage.&lt;/p>
&lt;p>Also, the PPT is available &lt;a href="./merbridge.pdf">here&lt;/a> for download.&lt;/p>
&lt;p>If you are interested, see:&lt;/p>
&lt;iframe width="1280" height="720" src="https://www.youtube.com/embed/r2wgInmsqsU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Blog: Merbridge - Accelerate your mesh with eBPF</title><link>/blog/2022/03/01/merbridge-introduce/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/01/merbridge-introduce/</guid><description>
&lt;h1 id="merbridge---accelerate-your-mesh-with-ebpf">Merbridge - Accelerate your mesh with eBPF&lt;/h1>
&lt;p>&lt;em>Replacing iptables rules with eBPF allows transporting data directly from inbound sockets to outbound sockets, shortening the datapath between sidecars and services.&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The secret of Istio’s abilities in traffic management, security, observability and policy is all in the Envoy proxy. Istio uses Envoy as the “sidecar” to intercept service traffic, with the kernel’s &lt;code>netfilter&lt;/code> packet filter functionality configured by iptables.&lt;/p>
&lt;p>There are shortcomings in using iptables to perform this interception. Since netfilter is a highly versatile tool for filtering packets, several routing rules and data filtering processes are applied before reaching the destination socket. For example, from the network layer to the transport layer, netfilter will be used for processing for several times with the rules predefined, like &lt;code>pre_routing&lt;/code>, &lt;code>post_routing&lt;/code> and etc. When the packet becomes a TCP packet or UDP packet, and is forwarded to user space, some additional steps like packet validation, protocol policy processing and destination socket searching will be performed. When a sidecar is configured to intercept traffic, the original data path can become very long, since duplicated steps are performed several times.&lt;/p>
&lt;p>Over the past two years, eBPF has become a trending technology, and many projects based on &lt;a href="https://ebpf.io/">eBPF&lt;/a> have been released to the community. Tools like &lt;a href="https://cilium.io/">Cilium&lt;/a> and &lt;a href="https://px.dev/">Pixie&lt;/a> show great use cases for eBPF in observability and network packet processing. With eBPF’s &lt;code>sockops&lt;/code> and &lt;code>redir&lt;/code> capabilities, data packets can be processed efficiently by directly being transported from an inbound socket to an outbound socket. In an Istio mesh, it is possible to use eBPF to replace iptables rules, and accelerate the data plane by shortening the data path.&lt;/p>
&lt;p>We have created an open source project called Merbridge, and by applying the following command to your Istio-managed cluster, you can use eBPF to achieve such network acceleration.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>Attention: Merbridge uses eBPF functions which require a Linux kernel version ≥ 5.7.&lt;/p>
&lt;/blockquote>
&lt;p>With Merbridge, the packet datapath can be shortened directly from one socket to another destination socket, and here’s how it works.&lt;/p>
&lt;h3 id="using-ebpf-sockops-for-performance-optimization">Using eBPF &lt;code>sockops&lt;/code> for performance optimization&lt;/h3>
&lt;p>Network connection is essentially socket communication. eBPF provides a function &lt;a href="https://man7.org/linux/man-pages/man7/bpf-helpers.7.html">bpf_msg_redirect_hash&lt;/a>, to directly forward the packets sent by the application in the inbound socket to the outbound socket. By entering the function mentioned before, developers can perform any logic to decide the packet destination. According to this characteristic, the datapath of packets can noticeably be optimized in the kernel.&lt;/p>
&lt;p>The &lt;code>sock_map&lt;/code> is the crucial piece in recording information for packet forwarding. When a packet arrives, an existing socket is selected from the &lt;code>sock_map&lt;/code> to forward the packet. As a result, we need to save all the socket information for packets to make the transportation process function properly. When there are new socket operations — like a new socket being created — the &lt;code>sock_ops&lt;/code> function is executed. The socket metadata is obtained and stored in the &lt;code>sock_map&lt;/code> to be used when processing packets. The common key type in the &lt;code>sock_map&lt;/code> is a “quadruple” of source and destination addresses and ports. With the key and the rules stored in the map, the destination socket will be found when a new packet arrives.&lt;/p>
&lt;h2 id="the-merbridge-approach">The Merbridge approach&lt;/h2>
&lt;p>Let’s introduce the detailed design and implementation principles of Merbridge step by step, with a real scenario.&lt;/p>
&lt;h3 id="istio-sidecar-traffic-interception-based-on-iptables">Istio sidecar traffic interception based on iptables&lt;/h3>
&lt;p>&lt;img src="./imgs/1.png" alt="Istio Sidecar Traffic Interception Based on iptables">&lt;/p>
&lt;p>When external traffic hits your application’s ports, it will be intercepted by a &lt;code>PREROUTING&lt;/code> rule in iptables, forwarded to port 15006 of the sidecar container, and handed over to Envoy for processing. This is shown as steps 1-4 in the red path in the above diagram.&lt;/p>
&lt;p>Envoy processes the traffic using the policies issued by the Istio control plane. If allowed, the traffic will be sent to the actual container port of the application container.&lt;/p>
&lt;p>When the application tries to access other services, it will be intercepted by an &lt;code>OUTPUT&lt;/code> rule in iptables, and then be forwarded to port 15001 of the sidecar container, where Envoy is listening. This is steps 9-12 in the red path, similar to inbound traffic processing.&lt;/p>
&lt;p>Traffic to the application port needs to be forwarded to the sidecar, then sent to the container port from the sidecar port, which is overhead. Moreover, iptables’ versatility determines that its performance is not always ideal because it inevitably adds delays to the whole datapath with different filtering rules applied. Although iptables is the common way to do packet filtering, in the Envoy proxy case, the longer datapath amplifies the bottleneck of packet filtering process in the kernel.&lt;/p>
&lt;p>If we use &lt;code>sockops&lt;/code> to directly connect the sidecar’s socket to the application’s socket, the traffic will not need to go through iptables rules, and thus performance can be improved.&lt;/p>
&lt;h3 id="processing-outbound-traffic">Processing outbound traffic&lt;/h3>
&lt;p>As mentioned above, we would like to use eBPF’s &lt;code>sockops&lt;/code> to bypass iptables to accelerate network requests. At the same time, we also do not want to modify any parts of Istio, to make Merbridge fully adaptive to the community version. As a result, we need to simulate what iptables does in eBPF.&lt;/p>
&lt;p>Traffic redirection in iptables utilizes its &lt;code>DNAT&lt;/code> function. When trying to simulate the capabilities of iptables using eBPF, there are two main things we need to do:&lt;/p>
&lt;ol>
&lt;li>Modify the destination address, when the connection is initiated, so that traffic can be sent to the new interface.&lt;/li>
&lt;li>Enable Envoy to identify the original destination address, to be able to identify the traffic.&lt;/li>
&lt;/ol>
&lt;p>For the first part, we can use eBPF’s &lt;code>connect&lt;/code> program to process it, by modifying &lt;code>user_ip&lt;/code> and &lt;code>user_port&lt;/code>.&lt;/p>
&lt;p>For the second part, we need to understand the concept of &lt;code>ORIGINAL_DST&lt;/code> which belongs to the &lt;code>netfilter&lt;/code> module in the kernel.&lt;/p>
&lt;p>When an application (including Envoy) receives a connection, it will call the &lt;code>get_sockopt&lt;/code> function to obtain &lt;code>ORIGINAL_DST&lt;/code>. If going through the iptables &lt;code>DNAT&lt;/code> process, iptables will set this parameter, with the “original IP + port” value, to the current socket. Thus, the application can get the original destination address according to the connection.&lt;/p>
&lt;p>We have to modify this call process through eBPF’s &lt;code>get_sockopts&lt;/code> function. (&lt;code>bpf_setsockopt&lt;/code> is not used here because this parameter does not currently support the optname of &lt;code>SO_ORIGINAL_DST&lt;/code>).&lt;/p>
&lt;p>Referring to the figure below, when an application initiates a request, it will go through the following steps:&lt;/p>
&lt;ol>
&lt;li>When the application initiates a connection, the &lt;code>connect&lt;/code> program will modify the destination address to &lt;code>127.x.y.z:15001&lt;/code>, and use &lt;code>cookie_original_dst&lt;/code> to save the original destination address.&lt;/li>
&lt;li>In the &lt;code>sockops&lt;/code> program, the current socket information and the quadruple are saved in &lt;code>sock_pair_map&lt;/code>. At the same time, the same quadruple and its corresponding original destination address will be written to &lt;code>pair_original_dst&lt;/code>. (Cookie is not used here because it cannot be obtained in the &lt;code>get_sockopt&lt;/code> program).&lt;/li>
&lt;li>After Envoy receives the connection, it will call the &lt;code>get_sockopt&lt;/code> function to read the destination address of the current connection. &lt;code>get_sockopt&lt;/code> will extract and return the original destination address from &lt;code>pair_original_dst&lt;/code>, according to the quadruple information. Thus, the connection is completely established.&lt;/li>
&lt;li>In the data transport step, the &lt;code>redir&lt;/code> program will read the sock information from &lt;code>sock_pair_map&lt;/code> according to the quadruple information, and then forward it directly through &lt;code>bpf_msg_redirect_hash&lt;/code> to speed up the request.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="./imgs/2.png" alt="Processing Outbound Traffic">&lt;/p>
&lt;p>Why do we set the destination address to &lt;code>127.x.y.z&lt;/code> instead of &lt;code>127.0.0.1&lt;/code>? When different pods exist, there might be conflicting quadruples, and this gracefully avoids conflict. (Pods’ IPs are different, and they will not be in the conflicting condition at any time.)&lt;/p>
&lt;h3 id="inbound-traffic-processing">Inbound traffic processing&lt;/h3>
&lt;p>The processing of inbound traffic is basically similar to outbound traffic, with the only difference: revising the port of the destination to 15006.&lt;/p>
&lt;p>It should be noted that since eBPF cannot take effect in a specified namespace like iptables, the change will be global, which means that if we use a Pod that is not originally managed by Istio, or an external IP address, serious problems will be encountered — like the connection not being established at all.&lt;/p>
&lt;p>As a result, we designed a tiny control plane (deployed as a DaemonSet), which watches all pods — similar to the kubelet watching pods on the node — to write the pod IP addresses that have been injected into the sidecar to the &lt;code>local_pod_ips&lt;/code> map.&lt;/p>
&lt;p>When processing inbound traffic, if the destination address is not in the map, we will not do anything to the traffic.&lt;/p>
&lt;p>The other steps are the same as for outbound traffic.&lt;/p>
&lt;p>&lt;img src="./imgs/3.png" alt="Processing Inbound Traffic">&lt;/p>
&lt;h3 id="same-node-acceleration">Same-node acceleration&lt;/h3>
&lt;p>Theoretically, acceleration between Envoy sidecars on the same node can be achieved directly through inbound traffic processing. However, Envoy will raise an error when accessing the application of the current pod in this scenario.&lt;/p>
&lt;p>In Istio, Envoy accesses the application by using the current pod IP and port number. With the above scenario, we realized that the pod IP exists in the &lt;code>local_pod_ips&lt;/code> map as well, and the traffic will be redirected to the pod IP on port 15006 again because it is the same address that the inbound traffic comes from. Redirecting to the same inbound address causes an infinite loop.&lt;/p>
&lt;p>Here comes the question: are there any ways to get the IP address in the current namespace with eBPF? The answer is yes!&lt;/p>
&lt;p>We have designed a feedback mechanism: When Envoy tries to establish the connection, we redirect it to port 15006. However, in the &lt;code>sockops&lt;/code> step, we will determine if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard this connection in the &lt;code>sockops&lt;/code> process. In the meantime, the current &lt;code>ProcessID&lt;/code> and &lt;code>IP&lt;/code> information will be written into the &lt;code>process_ip&lt;/code> map, to allow eBPF to support correspondence between processes and IPs.&lt;/p>
&lt;p>When the next request is sent, the same process need not be performed again. We will check directly from the &lt;code>process_ip&lt;/code> map if the destination address is the same as the current IP address.&lt;/p>
&lt;blockquote>
&lt;p>Envoy will retry when the request fails, and this retry process will only occur once, meaning subsequent requests will be accelerated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="./imgs/4.png" alt="Same-node acceleration">&lt;/p>
&lt;h3 id="connection-relationship">Connection relationship&lt;/h3>
&lt;p>Before applying eBPF using Merbridge, the data path between pods is like:&lt;/p>
&lt;p>&lt;img src="./imgs/5.png" alt="iptables&amp;rsquo;s data path">&lt;/p>
&lt;blockquote>
&lt;p>Diagram From: &lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>After applying Merbridge, the outbound traffic will skip many filter steps to improve the performance:&lt;/p>
&lt;p>&lt;img src="./imgs/6.png" alt="eBPF&amp;rsquo;s data path">&lt;/p>
&lt;blockquote>
&lt;p>Diagram From: &lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>If two pods are on the same machine, the connection can even be faster:&lt;/p>
&lt;p>&lt;img src="./imgs/7.png" alt="eBPF&amp;rsquo;s data path on the same machine">&lt;/p>
&lt;blockquote>
&lt;p>Diagram From: &lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="performance-results">Performance results&lt;/h2>
&lt;blockquote>
&lt;p>The below tests are from our development, and not yet validated in production use cases.&lt;/p>
&lt;/blockquote>
&lt;p>Let’s see the effect on overall latency using eBPF instead of iptables (lower is better):&lt;/p>
&lt;p>&lt;img src="./imgs/8.png" alt="Latency vs Client Connections Graph">&lt;/p>
&lt;p>We can also see overall QPS after using eBPF (higher is better):&lt;/p>
&lt;p>&lt;img src="./imgs/9.png" alt="QPS vs Client Connections Graph">&lt;/p>
&lt;blockquote>
&lt;p>Test results are generated with &lt;code>wrk&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>We have introduced the core ideas of Merbridge in this post. By replacing iptables with eBPF, the data transportation process can be accelerated in a mesh scenario. At the same time, Istio will not be changed at all. This means if you do not want to use eBPF any more, just delete the DaemonSet, and the datapath will be reverted to the traditional iptables-based routing without any problems.&lt;/p>
&lt;p>Merbridge is a completely independent open source project. It is still at an early stage, and we are looking forward to having more users and developers to get engaged. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!&lt;/p>
&lt;p>Merbridge Project: &lt;a href="https://github.com/merbridge/merbridge">https://github.com/merbridge/merbridge&lt;/a>&lt;/p>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://ebpf.io/">https://ebpf.io/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://cilium.io/">https://cilium.io/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/merbridge/merbridge">Merbridge on GitHub&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://developpaper.com/kubecon-2021-%EF%BD%9C-using-ebpf-instead-of-iptables-to-optimize-the-performance-of-service-grid-data-plane/">Using eBPF instead of iptables to optimize the performance of service grid data plane&lt;/a> by Liu Xu, Tencent&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/">Sidecar injection and transparent traffic hijacking process in Istio explained in detail&lt;/a> by Jimmy Song, Tetrate&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://01.org/blogs/xuyizhou/2021/accelerate-istio-dataplane-ebpf-part-1">Accelerate the Istio data plane with eBPF&lt;/a> by Yizhou Xu, Intel&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/original_dst_filter">Envoy&amp;rsquo;s Original Destination filter&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Blog: Release 0.7.0</title><link>/blog/2022/07/20/release-0.7.0/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>/blog/2022/07/20/release-0.7.0/</guid><description>
&lt;p>&lt;strong>We are pleased to announce the release of Merbridge 0.7.0!&lt;/strong>&lt;/p>
&lt;p>This release mainly includes two updates as below:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Replaced XDP with &lt;code>tc&lt;/code> (Traffic Control) to handle the ingress/egress traffic. This replacement comes from a situation that the XDP Generic mode has some problems and is not recommended to use in a production environment.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Merbridge now supports &lt;a href="https://kuma.io/">Kuma&lt;/a>, a universal Envoy service mesh. Similar to Istio, you can use all capabilities provided by Merbridge when you are working in the Kuma environment. Thanks &lt;a href="https://github.com/bartsmykla">@bartsmykla&lt;/a>, an excellent engineer from Kuma.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>For release notes, see &lt;a href="https://github.com/merbridge/merbridge/releases/tag/0.7.0">Merbridge 0.7.0&lt;/a>.&lt;/p></description></item><item><title>Blog: Release 0.6.0</title><link>/blog/2022/05/23/release-0.6.0/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>/blog/2022/05/23/release-0.6.0/</guid><description>
&lt;p>&lt;strong>We are pleased to announce the release of Merbridge 0.6.0!&lt;/strong>&lt;/p>
&lt;p>In this release, we have introduced CNI mode for the first time to support the capabilities of forwarding all Istio traffic. For details about the CNI mode support, see &lt;a href="/blog/2022/05/18/cni-mode/">Merbridge CNI Mode&lt;/a>&lt;/p>
&lt;p>For release notes, see: &lt;a href="https://github.com/merbridge/merbridge/releases/tag/0.6.0">Merbridge 0.6.0&lt;/a>&lt;/p></description></item><item><title>Blog: Release 0.5.0</title><link>/blog/2022/03/30/release-0.5.0/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/30/release-0.5.0/</guid><description>
&lt;h2 id="added">Added&lt;/h2>
&lt;ol>
&lt;li>Support passive sockops. (&lt;a href="https://github.com/merbridge/merbridge/pull/77">#77&lt;/a>) &lt;a href="https://github.com/kebe7jun">@kebe7jun&lt;/a> .&lt;/li>
&lt;li>Use ingress path for message redirection and forwarding. (&lt;a href="https://github.com/merbridge/merbridge/pull/82">#82&lt;/a>) &lt;a href="https://github.com/dddddai">@dddddai&lt;/a> .&lt;/li>
&lt;li>Support helm-based deployment of Merbridge (&lt;a href="https://github.com/merbridge/merbridge/pull/65">#65&lt;/a>) &lt;a href="https://github.com/Xunzhuo">@Xunzhuo&lt;/a> .&lt;/li>
&lt;/ol>
&lt;h2 id="fixed">Fixed&lt;/h2>
&lt;ol>
&lt;li>Fix the key size of &lt;code>cookie_original_dst&lt;/code>(&lt;a href="https://github.com/merbridge/merbridge/pull/75">#75&lt;/a>) &lt;a href="https://github.com/dddddai">@dddddai&lt;/a>.&lt;/li>
&lt;/ol></description></item></channel></rss>
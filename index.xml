<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Merbridge – Merbridge</title><link>/</link><description>Recent content on Merbridge</description><generator>Hugo -- gohugo.io</generator><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Merbridge CNI Mode</title><link>/blog/2022/05/18/cni-mode/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>/blog/2022/05/18/cni-mode/</guid><description>
&lt;!--Merbridge CNI 模式的出现，旨在能够更好地适配服务网格的功能。之前没有 CNI 模式时，Merbridge 能够做得事情比较有限。其中最大的问题是不能适配注入 Istio 的 Sidecar Annotation，这就导致 Merbridge 无法排除某些端口或 IP 段的流量等。同时，由于之前 Merbridge 只处理 Pod 内部的连接请求，这就导致，如果是外部发送到 Pod 的流量，Merbridge 将无法处理。-->
&lt;p>The CNI mode is designed to better adapt to the service mesh functions. Before having this mode, Merbridge was limited to certain scenarios. The biggest problem was that it could not adapt to the sidecar annotations from the container injected by Istio, which led to Merbridge cannot exclude traffic from certain ports and IP ranges. Furthermore, Merbridge was only able to handle requests inside the pod, which means the external traffic sent to the pod was not handled.&lt;/p>
&lt;!--为此，我们精心设计了 Merbridge CNI，旨在解决这些问题。-->
&lt;p>Therefore, we have implemented the Merbridge CNI to address these issues.&lt;/p>
&lt;!--## 为什么需要 CNI 模式？-->
&lt;h2 id="why-cni-mode-is-needed">Why CNI mode is needed&lt;/h2>
&lt;!--其一，之前的 Merbridge 只有一个很小的控制面，其监听 Pod 资源，将当前节点的 IP 信息写入 `local_pod_ips` 的 map，以供 connect 使用。但是，connect 程序由于工作在主机内核层，其无法知道当前正在处理的是哪个 Pod 的流量，就没法处理如 `excludeOutboundPorts` 等配置。为了能够适配注入 `excludeOutboundPorts` 的 Sidecar Annotation，我们需要让 eBPF 程序能够得知当前正在处理哪个 Pod 的请求。-->
&lt;p>First, Merbridge had a small control plane before, which listened to pods resources, and wrote the current node IP into the map of &lt;code>local_pod_ips&lt;/code> for use by &lt;code>connect&lt;/code>. However, since the &lt;code>connect&lt;/code> program only works at the host kernel layer, it won&amp;rsquo;t know which pod&amp;rsquo;s traffic is being processed. Thus, configurations like &lt;code>excludeOutboundPorts&lt;/code> cannot be handled. In order to be able to adapt to the injected sidecar annotation &lt;code>excludeOutboundPorts&lt;/code>, we need to let the eBPF program know which Pod&amp;rsquo;s request is currently being processed.&lt;/p>
&lt;!--为此，我们设计了一套方法，与 CNI 配合，能够获取当前 Pod 的 IP，以适配针对 Pod 的特殊配置。-->
&lt;p>To this end, we have designed a method to cooperate with the CNI, through which you can get the current Pod IP to validate special configurations for the Pod.&lt;/p>
&lt;!--其二，在之前的 Merbridge 版本中，只有 connect 会处理主机发起的请求，这在同一台主机上的 Pod 互相通讯时，是没有问题的。但是在不同主机之间通讯时就会出现问题，因为按照之前的逻辑，在跨节点通讯时流量不会被修改，这会导致在接收端还是离不开 iptables。-->
&lt;p>Second, for early versions of Merbridge, only &lt;code>connect&lt;/code> would process requests from the host, which had no problem for intra-node pod communication. However, it becomes problematic when traffic flows between different nodes. According to the previous logic, the traffic will not be modified during the cross-node communication, which will lead to the use of &lt;code>iptables&lt;/code> at the end.&lt;/p>
&lt;!--这次，我们依靠 XDP 程序，解决入口流量处理的问题。因为 XDP 程序需要挂载网卡，所以也需要借助 CNI。-->
&lt;p>Here, we turned to the XDP program for processing the inbound traffic. The XDP program needs to mount a network card, which also needs to use CNI.&lt;/p>
&lt;!--## CNI 如何解决问题？-->
&lt;h2 id="how-does-cni-work">How does CNI work&lt;/h2>
&lt;!--这里我们将探讨 CNI 的工作原理，以及如何使用 CNI 来解决问题。-->
&lt;p>This section will explore how CNI works and how to use CNI to solve the issues mentioned above.&lt;/p>
&lt;!--### 如何通过 CNI 让 eBPF 程序获取当前正在处理的 Pod IP？-->
&lt;h3 id="how-to-use-cni-to-let-ebpf-have-the-current-pod-ip">How to use CNI to let eBPF have the current Pod IP&lt;/h3>
&lt;!--我们通过 CNI，在 Pod 创建的时候，将 Pod 的 IP 信息写入一个 Map（`mark_pod_ips_map`），其 Key 为一个随机的值，Value 为 Pod 的 IP。然后，在当前 Pod 的 NetNS 里面监听一个特殊的端口 39807，将 Key 使用 `setsockopt` 写入这个端口 socket 的 mark。-->
&lt;p>When a pod is created, we write Pod IP into the map &lt;code>mark_pod_ips_map&lt;/code> through CNI, where the key is a random value, and the value is the Pod IP. Then, we listen to a special port &lt;code>39807&lt;/code> in the NetNS of the current Pod, and write the key to the mark of this port socket using &lt;code>setsockopt&lt;/code>.&lt;/p>
&lt;!--在 eBPF 中，我们通过 `bpf_sk_lookup_tcp` 取得端口 39807 的 Mark 信息，然后从 `mark_pod_ips_map` 中即可取得当前 NetNS（也是当期 Pod）的 IP。-->
&lt;p>In eBPF, we get the recorded mark information of port &lt;code>39807&lt;/code> through &lt;code>bpf_sk_lookup_tcp&lt;/code>, and use it to get the current Pod IP (also the current NetNS) from &lt;code>mark_pod_ips_map&lt;/code>.&lt;/p>
&lt;!--有了当前 Pod IP 之后，我们可以根据这个 Pod 的配置，确认流量处理路径（比如 `excludeOutboundPorts`）。-->
&lt;p>With the current Pod IP, we can determine the path to route traffic (such as &lt;code>excludeOutboundPorts&lt;/code>) according to the configuration of this Pod.&lt;/p>
&lt;!--同时，我们还使用 Pod 优化了之前解决四元组冲突的方案，改为使用 `bpf_bind` 绑定源 IP，目的 IP 直接使用 `127.0.0.1`，为了后续支持 IPv6 做准备。-->
&lt;p>In addition, we also optimized the quadruple conflicts by using &lt;code>bpf_bind&lt;/code> to bind the source IP and using &lt;code>127.0.0.1&lt;/code> as the destination IP, which also prepares for future support of IPv6.&lt;/p>
&lt;!--### 如何处理入口流量？-->
&lt;h3 id="how-to-handle-ingress-traffic">How to handle ingress traffic&lt;/h3>
&lt;!--为了能够处理入口流量，我们引入了 XDP 程序，XDP 程序作用在网卡上，能够对原始数据包做修改。-->
&lt;p>In order to handle inbound traffic, we introduced the XDP program, which works on the network card and can modify the original data packets.&lt;/p>
&lt;!--我们借助 XDP 程序，在流量到达 Pod 的时候，修改目的端口为 15006 以完成流量转发。-->
&lt;p>We use the XDP program to modify the destination port as &lt;code>15006&lt;/code> when the traffic reaches the Pod, so as to complete traffic forwarding.&lt;/p>
&lt;!--同时考虑到可能存在主机直接访问 Pod 的情况，也为了减小影响范围，我们选择将 XDP 程序附加到 Pod 的网卡上。这就需要借助 CNI 的能力，在创建 Pod 时进行附加操作。-->
&lt;p>At the same time, considering the possibility that the host directly accesses the Pod, and in order to reduce the scope of influence, we choose to attach the XDP program to the Pod&amp;rsquo;s network card. This requires the ability of CNI to perform additional operations when creating Pods&lt;/p>
&lt;!--## 如何体验 CNI 模式？-->
&lt;h2 id="how-to-use-cni-mode">How to use CNI mode?&lt;/h2>
&lt;!--CNI 模式默认被关闭，需要手动开启。-->
&lt;p>CNI mode is disabled by default. You need to enable it manually with the following command.&lt;/p>
&lt;!--可以使用以下命令一键开启：-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml &lt;span style="color:#000;font-weight:bold">|&lt;/span> sed &lt;span style="color:#4e9a06">&amp;#39;s/--cni-mode=false/--cni-mode=true/g&amp;#39;&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> kubectl apply -f -
&lt;/code>&lt;/pre>&lt;/div>&lt;!--## 注意事项-->
&lt;h2 id="notes">Notes&lt;/h2>
&lt;!--### CNI 模式处于测试阶段-->
&lt;h3 id="cni-mode-is-in-beta">CNI mode is in beta&lt;/h3>
&lt;!--CNI 模式刚被设计和开发出来，可能存在不少问题，我们欢迎大家在测试阶段进行反馈，或者提出更好的建议，以帮助我们改进 Merbridge！-->
&lt;p>The CNI mode is a new feature that may not be perfect. We welcome your feedback and suggestions to help improve Merbridge.&lt;/p>
&lt;!--如果需要使用注入 Istio perf benchmark 等工具进行测试性能，请开启 CNI 模式，否则会导致性能测试结果不准确。-->
&lt;p>If you are trying to do benchmark test using tools like &lt;code>Istio perf benchmark&lt;/code>, it is suggested to enable the CNI mode. Otherwise the test results will be inaccurate.&lt;/p>
&lt;!--### 需要注意主机是否可开启 hardware-checksum 能力-->
&lt;h3 id="check-whether-the-host-can-enable-the-hardware-checksum-capability">Check whether the host can enable the hardware-checksum capability&lt;/h3>
&lt;!--为了保证 CNI 模式的正常运行，我们默认关闭了 hardware-checksum 能力，这可能会影响到网络性能。建议大家在开启 CNI 模式前，先确认主机是否可开启 hardware-checksum 能力。如果可以开启，建议设置 `--hardware-checksum=true` 以获得最佳的性能表现。-->
&lt;p>In order to ensure the CNI mode works properly, the &lt;code>hardware-checksum&lt;/code> capability is disabled by default, which may affect network performance. It is recommended to check whether you can enable this capability on the host before enabling the CNI mode. If yes, we suggest to set &lt;code>--hardware-checksum=true&lt;/code> for best performance.&lt;/p>
&lt;!--测试方法：`ethtool -k &lt;网卡> | grep tx-checksum-ipv4` 为 on 表示开启。-->
&lt;p>Test method: if &lt;code>ethtool -k &amp;lt;network card&amp;gt; | grep tx-checksum-ipv4&lt;/code> is on, it means enabled.&lt;/p></description></item><item><title>Blog: Merbridge and Cilium</title><link>/blog/2022/04/23/merbridge-and-cilium/</link><pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate><guid>/blog/2022/04/23/merbridge-and-cilium/</guid><description>
&lt;h1 id="merbridge-and-cilium">Merbridge and Cilium&lt;/h1>
&lt;p>&lt;a href="https://cilium.io/">Cilium&lt;/a> is a great open source software that provides a lot of networking capabilities for cloud native applications based on eBPF, with a lot of great designs. Among others, Cilium designed a set of sockmap-based redir capabilities to help accelerate network communications, which inspired us and is the basis for Merbridge to provide network acceleration. It is a really great design.&lt;/p>
&lt;p>Merbridge leverages the great foundation that Cilium has provided, along with some targeted adaptations we&amp;rsquo;ve made in the Service Mesh, to make it easier to apply eBPF technology to Service Mesh.&lt;/p>
&lt;p>Our development team have learned a lot eBPF theoretical knowledge, practical methods, and testing methods, from Cilium&amp;rsquo;s detailed documentation and our frequent exchanges with the Cilium technical team. All these together helps make Merbridge possible.&lt;/p>
&lt;p>Thanks again to the Cilium project and community, and to Cilium for these great designs.&lt;/p></description></item><item><title>Blog: Livestream with Solo.io</title><link>/blog/2022/03/29/solo-io-livestream/</link><pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/29/solo-io-livestream/</guid><description>
&lt;p>On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.&lt;/p>
&lt;p>In this livestream, we discussed a lot of Merbridge-related issues, including a live demo that will help you get a quick overview of Merbridge&amp;rsquo;s features and usage.&lt;/p>
&lt;p>Also, the PPT is available &lt;a href="./merbridge.pdf">here&lt;/a> for download.&lt;/p>
&lt;p>If you are interested, see:&lt;/p>
&lt;iframe width="1280" height="720" src="https://www.youtube.com/embed/r2wgInmsqsU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Blog: Merbridge - Accelerate your mesh with eBPF</title><link>/blog/2022/03/01/merbridge-introduce/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/01/merbridge-introduce/</guid><description>
&lt;h1 id="merbridge---accelerate-your-mesh-with-ebpf">Merbridge - Accelerate your mesh with eBPF&lt;/h1>
&lt;p>&lt;em>Replacing iptables rules with eBPF allows transporting data directly from inbound sockets to outbound sockets, shortening the datapath between sidecars and services.&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The secret of Istio’s abilities in traffic management, security, observability and policy is all in the Envoy proxy. Istio uses Envoy as the “sidecar” to intercept service traffic, with the kernel’s &lt;code>netfilter&lt;/code> packet filter functionality configured by iptables.&lt;/p>
&lt;p>There are shortcomings in using iptables to perform this interception. Since netfilter is a highly versatile tool for filtering packets, several routing rules and data filtering processes are applied before reaching the destination socket. For example, from the network layer to the transport layer, netfilter will be used for processing for several times with the rules predefined, like &lt;code>pre_routing&lt;/code>, &lt;code>post_routing&lt;/code> and etc. When the packet becomes a TCP packet or UDP packet, and is forwarded to user space, some additional steps like packet validation, protocol policy processing and destination socket searching will be performed. When a sidecar is configured to intercept traffic, the original data path can become very long, since duplicated steps are performed several times.&lt;/p>
&lt;p>Over the past two years, eBPF has become a trending technology, and many projects based on &lt;a href="https://ebpf.io/">eBPF&lt;/a> have been released to the community. Tools like &lt;a href="https://cilium.io/">Cilium&lt;/a> and &lt;a href="https://px.dev/">Pixie&lt;/a> show great use cases for eBPF in observability and network packet processing. With eBPF’s &lt;code>sockops&lt;/code> and &lt;code>redir&lt;/code> capabilities, data packets can be processed efficiently by directly being transported from an inbound socket to an outbound socket. In an Istio mesh, it is possible to use eBPF to replace iptables rules, and accelerate the data plane by shortening the data path.&lt;/p>
&lt;p>We have created an open source project called Merbridge, and by applying the following command to your Istio-managed cluster, you can use eBPF to achieve such network acceleration.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>Attention: Merbridge uses eBPF functions which require a Linux kernel version ≥ 5.7.&lt;/p>
&lt;/blockquote>
&lt;p>With Merbridge, the packet datapath can be shortened directly from one socket to another destination socket, and here’s how it works.&lt;/p>
&lt;h3 id="using-ebpf-sockops-for-performance-optimization">Using eBPF &lt;code>sockops&lt;/code> for performance optimization&lt;/h3>
&lt;p>Network connection is essentially socket communication. eBPF provides a function &lt;a href="https://man7.org/linux/man-pages/man7/bpf-helpers.7.html">bpf_msg_redirect_hash&lt;/a>, to directly forward the packets sent by the application in the inbound socket to the outbound socket. By entering the function mentioned before, developers can perform any logic to decide the packet destination. According to this characteristic, the datapath of packets can noticeably be optimized in the kernel.&lt;/p>
&lt;p>The &lt;code>sock_map&lt;/code> is the crucial piece in recording information for packet forwarding. When a packet arrives, an existing socket is selected from the &lt;code>sock_map&lt;/code> to forward the packet. As a result, we need to save all the socket information for packets to make the transportation process function properly. When there are new socket operations — like a new socket being created — the &lt;code>sock_ops&lt;/code> function is executed. The socket metadata is obtained and stored in the &lt;code>sock_map&lt;/code> to be used when processing packets. The common key type in the &lt;code>sock_map&lt;/code> is a “quadruple” of source and destination addresses and ports. With the key and the rules stored in the map, the destination socket will be found when a new packet arrives.&lt;/p>
&lt;h2 id="the-merbridge-approach">The Merbridge approach&lt;/h2>
&lt;p>Let’s introduce the detailed design and implementation principles of Merbridge step by step, with a real scenario.&lt;/p>
&lt;h3 id="istio-sidecar-traffic-interception-based-on-iptables">Istio sidecar traffic interception based on iptables&lt;/h3>
&lt;p>&lt;img src="./imgs/1.png" alt="Istio Sidecar Traffic Interception Based on iptables">&lt;/p>
&lt;p>When external traffic hits your application’s ports, it will be intercepted by a &lt;code>PREROUTING&lt;/code> rule in iptables, forwarded to port 15006 of the sidecar container, and handed over to Envoy for processing. This is shown as steps 1-4 in the red path in the above diagram.&lt;/p>
&lt;p>Envoy processes the traffic using the policies issued by the Istio control plane. If allowed, the traffic will be sent to the actual container port of the application container.&lt;/p>
&lt;p>When the application tries to access other services, it will be intercepted by an &lt;code>OUTPUT&lt;/code> rule in iptables, and then be forwarded to port 15001 of the sidecar container, where Envoy is listening. This is steps 9-12 in the red path, similar to inbound traffic processing.&lt;/p>
&lt;p>Traffic to the application port needs to be forwarded to the sidecar, then sent to the container port from the sidecar port, which is overhead. Moreover, iptables’ versatility determines that its performance is not always ideal because it inevitably adds delays to the whole datapath with different filtering rules applied. Although iptables is the common way to do packet filtering, in the Envoy proxy case, the longer datapath amplifies the bottleneck of packet filtering process in the kernel.&lt;/p>
&lt;p>If we use &lt;code>sockops&lt;/code> to directly connect the sidecar’s socket to the application’s socket, the traffic will not need to go through iptables rules, and thus performance can be improved.&lt;/p>
&lt;h3 id="processing-outbound-traffic">Processing outbound traffic&lt;/h3>
&lt;p>As mentioned above, we would like to use eBPF’s &lt;code>sockops&lt;/code> to bypass iptables to accelerate network requests. At the same time, we also do not want to modify any parts of Istio, to make Merbridge fully adaptive to the community version. As a result, we need to simulate what iptables does in eBPF.&lt;/p>
&lt;p>Traffic redirection in iptables utilizes its &lt;code>DNAT&lt;/code> function. When trying to simulate the capabilities of iptables using eBPF, there are two main things we need to do:&lt;/p>
&lt;ol>
&lt;li>Modify the destination address, when the connection is initiated, so that traffic can be sent to the new interface.&lt;/li>
&lt;li>Enable Envoy to identify the original destination address, to be able to identify the traffic.&lt;/li>
&lt;/ol>
&lt;p>For the first part, we can use eBPF’s &lt;code>connect&lt;/code> program to process it, by modifying &lt;code>user_ip&lt;/code> and &lt;code>user_port&lt;/code>.&lt;/p>
&lt;p>For the second part, we need to understand the concept of &lt;code>ORIGINAL_DST&lt;/code> which belongs to the &lt;code>netfilter&lt;/code> module in the kernel.&lt;/p>
&lt;p>When an application (including Envoy) receives a connection, it will call the &lt;code>get_sockopt&lt;/code> function to obtain &lt;code>ORIGINAL_DST&lt;/code>. If going through the iptables &lt;code>DNAT&lt;/code> process, iptables will set this parameter, with the “original IP + port” value, to the current socket. Thus, the application can get the original destination address according to the connection.&lt;/p>
&lt;p>We have to modify this call process through eBPF’s &lt;code>get_sockopts&lt;/code> function. (&lt;code>bpf_setsockopt&lt;/code> is not used here because this parameter does not currently support the optname of &lt;code>SO_ORIGINAL_DST&lt;/code>).&lt;/p>
&lt;p>Referring to the figure below, when an application initiates a request, it will go through the following steps:&lt;/p>
&lt;ol>
&lt;li>When the application initiates a connection, the &lt;code>connect&lt;/code> program will modify the destination address to &lt;code>127.x.y.z:15001&lt;/code>, and use &lt;code>cookie_original_dst&lt;/code> to save the original destination address.&lt;/li>
&lt;li>In the &lt;code>sockops&lt;/code> program, the current socket information and the quadruple are saved in &lt;code>sock_pair_map&lt;/code>. At the same time, the same quadruple and its corresponding original destination address will be written to &lt;code>pair_original_dst&lt;/code>. (Cookie is not used here because it cannot be obtained in the &lt;code>get_sockopt&lt;/code> program).&lt;/li>
&lt;li>After Envoy receives the connection, it will call the &lt;code>get_sockopt&lt;/code> function to read the destination address of the current connection. &lt;code>get_sockopt&lt;/code> will extract and return the original destination address from &lt;code>pair_original_dst&lt;/code>, according to the quadruple information. Thus, the connection is completely established.&lt;/li>
&lt;li>In the data transport step, the &lt;code>redir&lt;/code> program will read the sock information from &lt;code>sock_pair_map&lt;/code> according to the quadruple information, and then forward it directly through &lt;code>bpf_msg_redirect_hash&lt;/code> to speed up the request.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="./imgs/2.png" alt="Processing Outbound Traffic">&lt;/p>
&lt;p>Why do we set the destination address to &lt;code>127.x.y.z&lt;/code> instead of &lt;code>127.0.0.1&lt;/code>? When different pods exist, there might be conflicting quadruples, and this gracefully avoids conflict. (Pods’ IPs are different, and they will not be in the conflicting condition at any time.)&lt;/p>
&lt;h3 id="inbound-traffic-processing">Inbound traffic processing&lt;/h3>
&lt;p>The processing of inbound traffic is basically similar to outbound traffic, with the only difference: revising the port of the destination to 15006.&lt;/p>
&lt;p>It should be noted that since eBPF cannot take effect in a specified namespace like iptables, the change will be global, which means that if we use a Pod that is not originally managed by Istio, or an external IP address, serious problems will be encountered — like the connection not being established at all.&lt;/p>
&lt;p>As a result, we designed a tiny control plane (deployed as a DaemonSet), which watches all pods — similar to the kubelet watching pods on the node — to write the pod IP addresses that have been injected into the sidecar to the &lt;code>local_pod_ips&lt;/code> map.&lt;/p>
&lt;p>When processing inbound traffic, if the destination address is not in the map, we will not do anything to the traffic.&lt;/p>
&lt;p>The other steps are the same as for outbound traffic.&lt;/p>
&lt;p>&lt;img src="./imgs/3.png" alt="Processing Inbound Traffic">&lt;/p>
&lt;h3 id="same-node-acceleration">Same-node acceleration&lt;/h3>
&lt;p>Theoretically, acceleration between Envoy sidecars on the same node can be achieved directly through inbound traffic processing. However, Envoy will raise an error when accessing the application of the current pod in this scenario.&lt;/p>
&lt;p>In Istio, Envoy accesses the application by using the current pod IP and port number. With the above scenario, we realized that the pod IP exists in the &lt;code>local_pod_ips&lt;/code> map as well, and the traffic will be redirected to the pod IP on port 15006 again because it is the same address that the inbound traffic comes from. Redirecting to the same inbound address causes an infinite loop.&lt;/p>
&lt;p>Here comes the question: are there any ways to get the IP address in the current namespace with eBPF? The answer is yes!&lt;/p>
&lt;p>We have designed a feedback mechanism: When Envoy tries to establish the connection, we redirect it to port 15006. However, in the &lt;code>sockops&lt;/code> step, we will determine if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard this connection in the &lt;code>sockops&lt;/code> process. In the meantime, the current &lt;code>ProcessID&lt;/code> and &lt;code>IP&lt;/code> information will be written into the &lt;code>process_ip&lt;/code> map, to allow eBPF to support correspondence between processes and IPs.&lt;/p>
&lt;p>When the next request is sent, the same process need not be performed again. We will check directly from the &lt;code>process_ip&lt;/code> map if the destination address is the same as the current IP address.&lt;/p>
&lt;blockquote>
&lt;p>Envoy will retry when the request fails, and this retry process will only occur once, meaning subsequent requests will be accelerated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="./imgs/4.png" alt="Same-node acceleration">&lt;/p>
&lt;h3 id="connection-relationship">Connection relationship&lt;/h3>
&lt;p>Before applying eBPF using Merbridge, the data path between pods is like:&lt;/p>
&lt;p>&lt;img src="./imgs/5.png" alt="iptables&amp;rsquo;s data path">&lt;/p>
&lt;blockquote>
&lt;p>Diagram From: &lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>After applying Merbridge, the outbound traffic will skip many filter steps to improve the performance:&lt;/p>
&lt;p>&lt;img src="./imgs/6.png" alt="eBPF&amp;rsquo;s data path">&lt;/p>
&lt;blockquote>
&lt;p>Diagram From: &lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>If two pods are on the same machine, the connection can even be faster:&lt;/p>
&lt;p>&lt;img src="./imgs/7.png" alt="eBPF&amp;rsquo;s data path on the same machine">&lt;/p>
&lt;blockquote>
&lt;p>Diagram From: &lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="performance-results">Performance results&lt;/h2>
&lt;blockquote>
&lt;p>The below tests are from our development, and not yet validated in production use cases.&lt;/p>
&lt;/blockquote>
&lt;p>Let’s see the effect on overall latency using eBPF instead of iptables (lower is better):&lt;/p>
&lt;p>&lt;img src="./imgs/8.png" alt="Latency vs Client Connections Graph">&lt;/p>
&lt;p>We can also see overall QPS after using eBPF (higher is better):&lt;/p>
&lt;p>&lt;img src="./imgs/9.png" alt="QPS vs Client Connections Graph">&lt;/p>
&lt;blockquote>
&lt;p>Test results are generated with &lt;code>wrk&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>We have introduced the core ideas of Merbridge in this post. By replacing iptables with eBPF, the data transportation process can be accelerated in a mesh scenario. At the same time, Istio will not be changed at all. This means if you do not want to use eBPF any more, just delete the DaemonSet, and the datapath will be reverted to the traditional iptables-based routing without any problems.&lt;/p>
&lt;p>Merbridge is a completely independent open source project. It is still at an early stage, and we are looking forward to having more users and developers to get engaged. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!&lt;/p>
&lt;p>Merbridge Project: &lt;a href="https://github.com/merbridge/merbridge">https://github.com/merbridge/merbridge&lt;/a>&lt;/p>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://ebpf.io/">https://ebpf.io/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://cilium.io/">https://cilium.io/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/merbridge/merbridge">Merbridge on GitHub&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://developpaper.com/kubecon-2021-%EF%BD%9C-using-ebpf-instead-of-iptables-to-optimize-the-performance-of-service-grid-data-plane/">Using eBPF instead of iptables to optimize the performance of service grid data plane&lt;/a> by Liu Xu, Tencent&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/">Sidecar injection and transparent traffic hijacking process in Istio explained in detail&lt;/a> by Jimmy Song, Tetrate&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://01.org/blogs/xuyizhou/2021/accelerate-istio-dataplane-ebpf-part-1">Accelerate the Istio data plane with eBPF&lt;/a> by Yizhou Xu, Intel&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/original_dst_filter">Envoy&amp;rsquo;s Original Destination filter&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22">Accelerating Envoy and Istio with Cilium and the Linux Kernel&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Blog: Release 0.6.0</title><link>/blog/2022/05/23/release-0.6.0/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>/blog/2022/05/23/release-0.6.0/</guid><description>
&lt;p>&lt;strong>We are pleased to announce the release of Merbridge 0.6.0!&lt;/strong>&lt;/p>
&lt;p>In this release, we have introduced CNI mode for the first time to support the capabilities of forwarding all Istio traffic. For details about the CNI mode support, see &lt;a href="/blog/2022/05/18/cni-mode/">Merbridge CNI Mode&lt;/a>&lt;/p>
&lt;p>For release notes, see: &lt;a href="https://github.com/merbridge/merbridge/releases/tag/0.6.0">Merbridge 0.6.0&lt;/a>&lt;/p></description></item><item><title>Blog: Release 0.5.0</title><link>/blog/2022/03/30/release-0.5.0/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>/blog/2022/03/30/release-0.5.0/</guid><description>
&lt;h2 id="added">Added&lt;/h2>
&lt;ol>
&lt;li>Support passive sockops. (&lt;a href="https://github.com/merbridge/merbridge/pull/77">#77&lt;/a>) &lt;a href="https://github.com/kebe7jun">@kebe7jun&lt;/a> .&lt;/li>
&lt;li>Use ingress path for message redirection and forwarding. (&lt;a href="https://github.com/merbridge/merbridge/pull/82">#82&lt;/a>) &lt;a href="https://github.com/dddddai">@dddddai&lt;/a> .&lt;/li>
&lt;li>Support helm-based deployment of Merbridge (&lt;a href="https://github.com/merbridge/merbridge/pull/65">#65&lt;/a>) &lt;a href="https://github.com/Xunzhuo">@Xunzhuo&lt;/a> .&lt;/li>
&lt;/ol>
&lt;h2 id="fixed">Fixed&lt;/h2>
&lt;ol>
&lt;li>Fix the key size of &lt;code>cookie_original_dst&lt;/code>(&lt;a href="https://github.com/merbridge/merbridge/pull/75">#75&lt;/a>) &lt;a href="https://github.com/dddddai">@dddddai&lt;/a>.&lt;/li>
&lt;/ol></description></item></channel></rss>
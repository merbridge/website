<!doctype html><html lang=en class=no-js>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.87.0"><link rel=canonical type=text/html href=/blog/2022/>
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<link rel="shortcut icon" href=/favicons/favicon.ico>
<link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16>
<link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32>
<link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36>
<link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48>
<link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72>
<link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96>
<link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144>
<link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192>
<title>Blogs of 2022 | Merbridge</title>
<meta name=description content="Merbridge website"><meta property="og:title" content="Blogs of 2022">
<meta property="og:description" content="Merbridge website">
<meta property="og:type" content="website">
<meta property="og:url" content="/blog/2022/"><meta property="og:site_name" content="Merbridge">
<meta itemprop=name content="Blogs of 2022">
<meta itemprop=description content="Merbridge website"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Blogs of 2022">
<meta name=twitter:description content="Merbridge website">
<link rel=preload href=/scss/main.min.957d988e396d495afa3156d4640b99742d961f39c79e92f8d152969969aa9ece.css as=style>
<link href=/scss/main.min.957d988e396d495afa3156d4640b99742d961f39c79e92f8d152969969aa9ece.css rel=stylesheet integrity>
<script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script>
</head>
<body class="td-section td-blog">
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
<a class=navbar-brand href=/>
<span class=navbar-logo><svg id="Graph" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><defs><style>.cls-1{fill:#33de72}.cls-2{fill:#fff}.cls-3{fill:none}</style></defs><g id="Merbridge_-_Graph_-_Color_Light" data-name="Merbridge - Graph - Color Light"><path id="Primary" class="cls-1" d="M440 86.059V454l-48-48V201.941l-44 44V178.059zm-184 184c-34.6-34.6-151.526-151.526-184-184V454l48-48V201.941l136 136 44-44V226.059z"/><path id="Secondary" class="cls-2" d="M212 382l-48 48V245.941l48 48zm0-155.941V48L164 96v82.059zM3e2 48V382l48 48V96z"/><rect id="Frame" class="cls-3" width="512" height="512"/></g></svg></span><span class="text-uppercase font-weight-bold">Merbridge</span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/about/><span>About</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/docs/><span>Documentation</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class="nav-link active" href=/blog/><span class=active>Blog</span></a>
</li>
<li class="nav-item dropdown mr-4 d-none d-lg-block">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>
English
</a>
<div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink>
<a class=dropdown-item href=/zh/blog/2022/>中文 Chinese</a>
</div>
</li>
</ul>
</div>
<div class="navbar-nav d-none d-lg-block">
<input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.00a50030e6c242b73d3cdfdbd352cf64.json data-offline-search-base-href=/ data-offline-search-max-results=10>
</div>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
</div>
<div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
</div>
<main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.
</p><p>
<a href=/blog/2022/>Return to the regular view of this page</a>.
</p>
</div>
<h1 class=title>Blogs of 2022</h1>
<ul>
<li><a href=#pg-1ca19c17b948e8cd4d6c551fe443f48e>Deep Dive into Ambient Mesh - Traffic Path</a></li>
<li><a href=#pg-ddae09d9f86ef7a0b6eecd3628b18eb5>Merbridge CNI Mode</a></li>
<li><a href=#pg-f8bd1de928cfdde2faaf3cb4d5cf2dfd>Merbridge and Cilium</a></li>
<li><a href=#pg-9c52cc92a2a851c972fc1e7d42cec0af>Livestream with Solo.io</a></li>
<li><a href=#pg-df869185d0e53dcbdf1283e406176894>Merbridge - Accelerate your mesh with eBPF</a></li>
</ul>
<div class=content>
</div>
</div>
<div class=td-content>
<h1 id=pg-1ca19c17b948e8cd4d6c551fe443f48e>Deep Dive into Ambient Mesh - Traffic Path</h1>
<div class=lead>This blog analyzes the traffic path on data plane in the ambient mesh.</div>
<div class="td-byline mb-4">
By <b>Kebe Liu</b> |
<time datetime=2022-10-13 class=text-muted>Thursday, October 13, 2022</time>
</div>
<p>Ambient Mesh has been released for a while, and some online articles have talked much about its usage and architecture.
This blog will dive into the traffic path on data plane in the ambient mesh to help you fully understand
the implementations of the ambient data plane.</p>
<p>Before start, you shall carefully read through <a href=https://istio.io/latest/blog/2022/introducing-ambient-mesh/>introducing ambient mesh</a>
to learn the basic knowledge of the ambient mesh.</p>
<blockquote>
<p>For your convenience, the test environment can be deployed by following
<a href=https://istio.io/latest/blog/2022/get-started-ambient/>Get Started with Istio Ambient Mesh</a>.</p>
</blockquote>
<h2 id=start-from-the-moment-you-make-a-request>Start from the moment you make a request</h2>
<p>In order to explore the traffic path, we first analyze the scenario where two services access each other in
the ambient mesh (only for L4 mode on different nodes).</p>
<p>After enabling the ambient mesh in the <code>default</code> namespace, all services will have capabilities of mesh governance.</p>
<p>Our analysis starts from this command: <code>kubectl exec deploy/sleep -- curl -s http://productpage:9080/ | head -n1</code></p>
<p>In the sidecar mode, Istio intercepts traffic through iptables.
When you run the curl command in a sleep pod, the traffic will be forwarded by iptables to port 15001 of sidecar.
However, in the ambient mesh, no sidecar exists in a pod, and it does not need restart to enable the ambient mesh.
How to make sure the request is processed by ztunnel?</p>
<h2 id=egress-traffic-interception>Egress traffic interception</h2>
<p>To learn details about intercepting the egress traffic, let&rsquo;s check the control plane components:</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system get po
NAME                                   READY   STATUS    RESTARTS   AGE
istio-cni-node-5rh5z                   1/1     Running   0          20h
istio-cni-node-qsvsz                   1/1     Running   0          20h
istio-cni-node-wdffp                   1/1     Running   0          20h
istio-ingressgateway-5cfcb57bd-kx9hx   1/1     Running   0          20h
istiod-6b84499b75-ncmn7                1/1     Running   0          20h
ztunnel-nptf6                          1/1     Running   0          20h
ztunnel-vxv4b                          1/1     Running   0          20h
ztunnel-xkz4s                          1/1     Running   0          20h
</code></pre><p>In the sidecar mode, istio-cni is mainly a CNI to avoid permission
leakage caused by using the istio-init container to process iptables rules.
However, in the ambient mesh, istio-cni becomes a required component.
Sidecars are theoretically not needed. Why is the istio-cni component being required?</p>
<p>Let&rsquo;s check the logs:</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system logs istio-cni-node-qsvsz
...
2022-10-12T07:34:33.224957Z	info	ambient	Adding route for reviews-v1-6494d87c7b-zrpks/default: [table 100 10.244.1.4/32 via 192.168.126.2 dev istioin src 10.244.1.1]
2022-10-12T07:34:33.226054Z	info	ambient	Adding pod 'reviews-v2-79857b95b-m4q2g/default' (0ff78312-3a13-4a02-b39d-644bfb91e861) to ipset
2022-10-12T07:34:33.228305Z	info	ambient	Adding route for reviews-v2-79857b95b-m4q2g/default: [table 100 10.244.1.5/32 via 192.168.126.2 dev istioin src 10.244.1.1]
2022-10-12T07:34:33.229967Z	info	ambient	Adding pod 'reviews-v3-75f494fccb-92nq5/default' (e41edf7c-a347-45cb-a144-97492faa77bf) to ipset
2022-10-12T07:34:33.232236Z	info	ambient	Adding route for reviews-v3-75f494fccb-92nq5/default: [table 100 10.244.1.6/32 via 192.168.126.2 dev istioin src 10.244.1.1]
</code></pre><p>As shown in the above output, for a pod in the ambient mesh, istio-cni performs the following actions:</p>
<ol>
<li>Add the pod to ipset</li>
<li>Add a routing rule to table 100 (for its usage see below)</li>
</ol>
<p>You can view the ipset contents on the node (note that the kind cluster is used here,
you need to use <code>docker exec</code> to enter the host first):</p>
<pre><code class=language-console data-lang=console>kebe@pc $ docker exec -it ambient-worker2 bash
root@ambient-worker2:/# ipset list
Name: ztunnel-pods-ips
Type: hash:ip
Revision: 0
Header: family inet hashsize 1024 maxelem 65536
Size in memory: 520
References: 1
Number of entries: 5
Members:
10.244.1.5
10.244.1.7
10.244.1.8
10.244.1.4
10.244.1.6
</code></pre><p>It is found that an ipset exists on the node where this pod is running. ipset holds many IPs for pods.</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl get po -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE              NOMINATED NODE   READINESS GATES
details-v1-76778d6644-wn4d2       1/1     Running   0          20h   10.244.1.9   ambient-worker2   &lt;none&gt;           &lt;none&gt;
notsleep-6d6c8669b5-pngxg         1/1     Running   0          20h   10.244.2.5   ambient-worker    &lt;none&gt;           &lt;none&gt;
productpage-v1-7c548b785b-w9zl6   1/1     Running   0          20h   10.244.1.7   ambient-worker2   &lt;none&gt;           &lt;none&gt;
ratings-v1-85c74b6cb4-57m52       1/1     Running   0          20h   10.244.1.8   ambient-worker2   &lt;none&gt;           &lt;none&gt;
reviews-v1-6494d87c7b-zrpks       1/1     Running   0          20h   10.244.1.4   ambient-worker2   &lt;none&gt;           &lt;none&gt;
reviews-v2-79857b95b-m4q2g        1/1     Running   0          20h   10.244.1.5   ambient-worker2   &lt;none&gt;           &lt;none&gt;
reviews-v3-75f494fccb-92nq5       1/1     Running   0          20h   10.244.1.6   ambient-worker2   &lt;none&gt;           &lt;none&gt;
sleep-7b85956664-z6qh7            1/1     Running   0          20h   10.244.2.4   ambient-worker    &lt;none&gt;           &lt;none&gt;
</code></pre><p>Therefore, this ipset holds a list of all PodIPs in the ambient mesh on the current node.</p>
<p>Where can this ipset be used?</p>
<p>Let&rsquo;s take a look at the iptables rules and you can find:</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# iptables-save
*mangle
...
-A POSTROUTING -j ztunnel-POSTROUTING
...
-A ztunnel-PREROUTING -p tcp -m set --match-set ztunnel-pods-ips src -j MARK --set-xmark 0x100/0x100
</code></pre><p>You now learn that when a pod in the ambient mesh on a node (in the <code>ztunnel-pods-ips</code> ipset) initiates a request,
its connection will be marked with <code>0x100/0x100</code>.</p>
<p>Generally, it will be related to routing. Let&rsquo;s check the routing rules:</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip rule
0: from all lookup local
100: from all fwmark 0x200/0x200 goto 32766
101: from all fwmark 0x100/0x100 lookup 101
102: from all fwmark 0x40/0x40 lookup 102
103: from all lookup 100
32766: from all lookup main
32767: from all lookup default
</code></pre><p>The traffic marked with <code>0x100/0x100</code> goes via the routing table 101. Let&rsquo;s check the routing table:</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip r show table 101
default via 192.168.127.2 dev istioout
10.244.1.2 dev veth5db63c11 scope link
</code></pre><p>It can be clearly seen that the default gateway has been replaced with <code>192.168.127.2</code> via the istioout NIC (network interface card).</p>
<p><code>192.168.127.2</code> does not belong to any of NodeIP, PodIP, and ClusterIP.
The istioout NIC should not exist by default, then where does this IP come from?
Since the traffic ultimately needs to go to ztunnel,
you can check the ztunnel configuration to see if you can find the answer.</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system get po ztunnel-vxv4b -o yaml
apiVersion: v1
kind: Pod
metadata:
  ...
  name: ztunnel-vxv4b
  namespace: istio-system
	...
spec:
  ...
  initContainers:
  - command:
			...
      OUTBOUND_TUN=istioout
			...
      OUTBOUND_TUN_IP=192.168.127.1
      ZTUNNEL_OUTBOUND_TUN_IP=192.168.127.2

      ip link add name p$INBOUND_TUN type geneve id 1000 remote $HOST_IP
      ip addr add $ZTUNNEL_INBOUND_TUN_IP/$TUN_PREFIX dev p$INBOUND_TUN

      ip link add name p$OUTBOUND_TUN type geneve id 1001 remote $HOST_IP
      ip addr add $ZTUNNEL_OUTBOUND_TUN_IP/$TUN_PREFIX dev p$OUTBOUND_TUN

      ip link set p$INBOUND_TUN up
      ip link set p$OUTBOUND_TUN up
      ...
</code></pre><p>As above, ztunnel will be responsible for creating the istioout NIC, you now go to the node to check the NIC.</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip a
11: istioout: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether 0a:ea:4e:e0:8d:26 brd ff:ff:ff:ff:ff:ff
    inet 192.168.127.1/30 brd 192.168.127.3 scope global istioout
       valid_lft forever preferred_lft forever
</code></pre><p>Where is the gateway IP of <code>192.168.127.2</code>? It is allocated in ztunnel.</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a
Defaulted container &quot;istio-proxy&quot; out of: istio-proxy, istio-init (init)
2: eth0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::448a:46ff:fe72:1d3b/64 scope link
       valid_lft forever preferred_lft forever
4: pistioout: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether c2:d0:18:20:3b:97 brd ff:ff:ff:ff:ff:ff
    inet 192.168.127.2/30 scope global pistioout
       valid_lft forever preferred_lft forever
    inet6 fe80::c0d0:18ff:fe20:3b97/64 scope link
       valid_lft forever preferred_lft forever
</code></pre><p>You can now see that the traffic is going to ztunnel, but nothing else is done to the traffic at this time,
it is simply routed to ztunnel. How to does Envoy in ztunnel process the traffic?</p>
<p>Let&rsquo;s continue to check the ztunnel configuration with many iptables rules. Let&rsquo;s check the specific rules in ztunnel.</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save
Defaulted container &quot;istio-proxy&quot; out of: istio-proxy, istio-init (init)
...
*mangle
-A PREROUTING -i pistioout -p tcp -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
...
COMMIT
</code></pre><p>When traffic enters ztunnel, it will use TPROXY to transfer the traffic to port 15001 for processing,
where 15001 is the port that Envoy actually listens to and process the pod egress traffic.
As for TPROXY, you can learn relevant reference, and this blog will not repeat it further.</p>
<p>So when a pod is running in the ambient mesh, its egress traffic path is as follows:</p>
<ol>
<li>Initiate traffic from a process in pod</li>
<li>The traffic flows via the node network and get marks by iptables on the node</li>
<li>The traffic is forwarded to the ztunnel pod on current node by the routing table</li>
<li>When the traffic reaches ztunnel, it will go through iptables for TPROXY (transparent proxy),
and send the traffic to port 15001 of Envoy in the current pod.</li>
</ol>
<p>So far in the ambient mesh, it is clear that the processing of pod egress traffic is relatively complex.
The path is also relatively long, unlike the sidecar mode in which a traffic forwarding is directly completed in the pod.</p>
<h2 id=ingress-traffic-interception>Ingress traffic interception</h2>
<p>With the above experience, it is easy to learn that in the ambient mesh, the traffic interception is mainly
through the method of MARK routing + TPROXY, and the ingress traffic should be similar.</p>
<p>Let&rsquo;s analyze it in the simplest way. When a process on a node, or a program on another host accesses
a pod on the current node, the traffic goes through the host&rsquo;s routing table.
Let&rsquo;s check the routing info when the response arrives at <code>productpage-v1-7c548b785b-w9zl6(10.244.1.7)</code>:</p>
<pre><code class=language-console data-lang=console>root@ambient-worker2:/# ip r get 10.244.1.7
10.244.1.7 via 192.168.126.2 dev istioin table 100 src 10.244.1.1 uid 0
    cache
</code></pre><p>When accessing <code>10.244.1.7</code>, the traffic will be routed to <code>192.168.126.2</code>, and this rule is added by istio-cni.</p>
<p>Similarly <code>192.168.126.2</code> belongs to ztunnel:</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a
Defaulted container &quot;istio-proxy&quot; out of: istio-proxy, istio-init (init)
2: eth0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::448a:46ff:fe72:1d3b/64 scope link
       valid_lft forever preferred_lft forever
3: pistioin: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 7e:b2:e6:f9:a4:92 brd ff:ff:ff:ff:ff:ff
    inet 192.168.126.2/30 scope global pistioin
       valid_lft forever preferred_lft forever
    inet6 fe80::7cb2:e6ff:fef9:a492/64 scope link
       valid_lft forever preferred_lft forever
</code></pre><p>By using the same analysis method, let&rsquo;s check the iptables rules:</p>
<pre><code class=language-console data-lang=console>kebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save
...
-A PREROUTING -i pistioin -p tcp -m tcp --dport 15008 -j TPROXY --on-port 15008 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
-A PREROUTING -i pistioin -p tcp -j TPROXY --on-port 15006 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff
...
</code></pre><p>If you directly access the node via PodIP + pod port, the traffic will be forwarded to port 15006 of ztunnel,
which is the port to handle the ingress traffic in Istio.</p>
<p>As for the traffic whose destination port is port 15008, this is the port used by ztunnel for L4 traffic tunneling.
This blog will not explain this further.</p>
<h2 id=handle-traffic-for-envoy-itself>Handle traffic for Envoy itself</h2>
<p>In the sidecar mode, Envoy and user containers run in the same network namespace.
For the traffic from user containers, you need to intercept all the traffic to
guarantee complete control of the traffic. However, is it also required in the ambient mesh?</p>
<p>The answer is no, because Envoy has been isolated from other pods.
The traffic sent by Envoy does not require special notice.
In other words, you only need to handle ingress traffic for ztunnel,
so the rules in ztunnel seem relatively simple.</p>
<h2 id=wrapping-up>Wrapping up</h2>
<p>As above explained, this blog mainly analyzed the scheme for Pod traffic interception in the ambient mesh,
but this blog has not involved with how to handle L7 traffic and the specific principles of ztunnel implementation.
The next plan is to analyze the detailed traffic paths in ztunnel and waypoint proxy.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ddae09d9f86ef7a0b6eecd3628b18eb5>Merbridge CNI Mode</h1>
<div class=lead>This blog explains how CNI works in Merbridge.</div>
<div class="td-byline mb-4">
<time datetime=2022-05-18 class=text-muted>Wednesday, May 18, 2022</time>
</div>
<p>The CNI mode is designed to better adapt to the service mesh functions. Before having this mode, Merbridge was limited to certain scenarios. The biggest problem was that it could not adapt to the sidecar annotations from the container injected by Istio, which led to Merbridge cannot exclude traffic from certain ports and IP ranges. Furthermore, Merbridge was only able to handle requests inside the pod, which means the external traffic sent to the pod was not handled.</p>
<p>Therefore, we have implemented the Merbridge CNI to address these issues.</p><!--## 为什么需要 CNI 模式？-->
<h2 id=why-cni-mode-is-needed>Why CNI mode is needed</h2>
<p>First, Merbridge had a small control plane before, which listened to pods resources, and wrote the current node IP into the map of <code>local_pod_ips</code> for use by <code>connect</code>. However, since the <code>connect</code> program only works at the host kernel layer, it won&rsquo;t know which pod&rsquo;s traffic is being processed. Thus, configurations like <code>excludeOutboundPorts</code> cannot be handled. In order to be able to adapt to the injected sidecar annotation <code>excludeOutboundPorts</code>, we need to let the eBPF program know which Pod&rsquo;s request is currently being processed.</p>
<p>To this end, we have designed a method to cooperate with the CNI, through which you can get the current Pod IP to validate special configurations for the Pod.</p>
<p>Second, for early versions of Merbridge, only <code>connect</code> would process requests from the host, which had no problem for intra-node pod communication. However, it becomes problematic when traffic flows between different nodes. According to the previous logic, the traffic will not be modified during the cross-node communication, which will lead to the use of <code>iptables</code> at the end.</p>
<p>Here, we turned to the XDP program for processing the inbound traffic. The XDP program needs to mount a network card, which also needs to use CNI.</p><!--## CNI 如何解决问题？-->
<h2 id=how-does-cni-work>How does CNI work</h2>
<p>This section will explore how CNI works and how to use CNI to solve the issues mentioned above.</p><!--### 如何通过 CNI 让 eBPF 程序获取当前正在处理的 Pod IP？-->
<h3 id=how-to-use-cni-to-let-ebpf-have-the-current-pod-ip>How to use CNI to let eBPF have the current Pod IP</h3>
<p>When a pod is created, we write Pod IP into the map <code>mark_pod_ips_map</code> through CNI, where the key is a random value, and the value is the Pod IP. Then, we listen to a special port <code>39807</code> in the NetNS of the current Pod, and write the key to the mark of this port socket using <code>setsockopt</code>.</p>
<p>In eBPF, we get the recorded mark information of port <code>39807</code> through <code>bpf_sk_lookup_tcp</code>, and use it to get the current Pod IP (also the current NetNS) from <code>mark_pod_ips_map</code>.</p>
<p>With the current Pod IP, we can determine the path to route traffic (such as <code>excludeOutboundPorts</code>) according to the configuration of this Pod.</p>
<p>In addition, we also optimized the quadruple conflicts by using <code>bpf_bind</code> to bind the source IP and using <code>127.0.0.1</code> as the destination IP, which also prepares for future support of IPv6.</p><!--### 如何处理入口流量？-->
<h3 id=how-to-handle-ingress-traffic>How to handle ingress traffic</h3>
<p>In order to handle inbound traffic, we introduced the XDP program, which works on the network card and can modify the original data packets.</p>
<p>We use the XDP program to modify the destination port as <code>15006</code> when the traffic reaches the Pod, so as to complete traffic forwarding.</p>
<p>At the same time, considering the possibility that the host directly accesses the Pod, and in order to reduce the scope of influence, we choose to attach the XDP program to the Pod&rsquo;s network card. This requires the ability of CNI to perform additional operations when creating Pods</p><!--## 如何体验 CNI 模式？-->
<h2 id=how-to-use-cni-mode>How to use CNI mode?</h2>
<p>CNI mode is disabled by default. You need to enable it manually with the following command.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>curl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml <span style=color:#000;font-weight:700>|</span> sed <span style=color:#4e9a06>&#39;s/--cni-mode=false/--cni-mode=true/g&#39;</span> <span style=color:#000;font-weight:700>|</span> kubectl apply -f -
</code></pre></div><!--## 注意事项-->
<h2 id=notes>Notes</h2><!--### CNI 模式处于测试阶段-->
<h3 id=cni-mode-is-in-beta>CNI mode is in beta</h3>
<p>The CNI mode is a new feature that may not be perfect. We welcome your feedback and suggestions to help improve Merbridge.</p>
<p>If you are trying to do benchmark test using tools like <code>Istio perf benchmark</code>, it is suggested to enable the CNI mode. Otherwise the test results will be inaccurate.</p><!--### 需要注意主机是否可开启 hardware-checksum 能力-->
<h3 id=check-whether-the-host-can-enable-the-hardware-checksum-capability>Check whether the host can enable the hardware-checksum capability</h3>
<p>In order to ensure the CNI mode works properly, the <code>hardware-checksum</code> capability is disabled by default, which may affect network performance. It is recommended to check whether you can enable this capability on the host before enabling the CNI mode. If yes, we suggest to set <code>--hardware-checksum=true</code> for best performance.</p>
<p>Test method: if <code>ethtool -k &lt;network card> | grep tx-checksum-ipv4</code> is on, it means enabled.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-f8bd1de928cfdde2faaf3cb4d5cf2dfd>Merbridge and Cilium</h1>
<div class="td-byline mb-4">
<time datetime=2022-04-23 class=text-muted>Saturday, April 23, 2022</time>
</div>
<h1 id=merbridge-and-cilium>Merbridge and Cilium</h1>
<p><a href=https://cilium.io/>Cilium</a> is a great open source software that provides a lot of networking capabilities for cloud native applications based on eBPF, with a lot of great designs. Among others, Cilium designed a set of sockmap-based redir capabilities to help accelerate network communications, which inspired us and is the basis for Merbridge to provide network acceleration. It is a really great design.</p>
<p>Merbridge leverages the great foundation that Cilium has provided, along with some targeted adaptations we&rsquo;ve made in the Service Mesh, to make it easier to apply eBPF technology to Service Mesh.</p>
<p>Our development team have learned a lot eBPF theoretical knowledge, practical methods, and testing methods, from Cilium&rsquo;s detailed documentation and our frequent exchanges with the Cilium technical team. All these together helps make Merbridge possible.</p>
<p>Thanks again to the Cilium project and community, and to Cilium for these great designs.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-9c52cc92a2a851c972fc1e7d42cec0af>Livestream with Solo.io</h1>
<div class="td-byline mb-4">
<time datetime=2022-03-29 class=text-muted>Tuesday, March 29, 2022</time>
</div>
<p>On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.</p>
<p>In this livestream, we discussed a lot of Merbridge-related issues, including a live demo that will help you get a quick overview of Merbridge&rsquo;s features and usage.</p>
<p>Also, the PPT is available <a href=./merbridge.pdf>here</a> for download.</p>
<p>If you are interested, see:</p>
<iframe width=1280 height=720 src=https://www.youtube.com/embed/r2wgInmsqsU title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-df869185d0e53dcbdf1283e406176894>Merbridge - Accelerate your mesh with eBPF</h1>
<div class="td-byline mb-4">
<time datetime=2022-03-01 class=text-muted>Tuesday, March 01, 2022</time>
</div>
<h1 id=merbridge---accelerate-your-mesh-with-ebpf>Merbridge - Accelerate your mesh with eBPF</h1>
<p><em>Replacing iptables rules with eBPF allows transporting data directly from inbound sockets to outbound sockets, shortening the datapath between sidecars and services.</em></p>
<h2 id=introduction>Introduction</h2>
<p>The secret of Istio’s abilities in traffic management, security, observability and policy is all in the Envoy proxy. Istio uses Envoy as the “sidecar” to intercept service traffic, with the kernel’s <code>netfilter</code> packet filter functionality configured by iptables.</p>
<p>There are shortcomings in using iptables to perform this interception. Since netfilter is a highly versatile tool for filtering packets, several routing rules and data filtering processes are applied before reaching the destination socket. For example, from the network layer to the transport layer, netfilter will be used for processing for several times with the rules predefined, like <code>pre_routing</code>, <code>post_routing</code> and etc. When the packet becomes a TCP packet or UDP packet, and is forwarded to user space, some additional steps like packet validation, protocol policy processing and destination socket searching will be performed. When a sidecar is configured to intercept traffic, the original data path can become very long, since duplicated steps are performed several times.</p>
<p>Over the past two years, eBPF has become a trending technology, and many projects based on <a href=https://ebpf.io/>eBPF</a> have been released to the community. Tools like <a href=https://cilium.io/>Cilium</a> and <a href=https://px.dev/>Pixie</a> show great use cases for eBPF in observability and network packet processing. With eBPF’s <code>sockops</code> and <code>redir</code> capabilities, data packets can be processed efficiently by directly being transported from an inbound socket to an outbound socket. In an Istio mesh, it is possible to use eBPF to replace iptables rules, and accelerate the data plane by shortening the data path.</p>
<p>We have created an open source project called Merbridge, and by applying the following command to your Istio-managed cluster, you can use eBPF to achieve such network acceleration.</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml
</code></pre></div><blockquote>
<p>Attention: Merbridge uses eBPF functions which require a Linux kernel version ≥ 5.7.</p>
</blockquote>
<p>With Merbridge, the packet datapath can be shortened directly from one socket to another destination socket, and here’s how it works.</p>
<h3 id=using-ebpf-sockops-for-performance-optimization>Using eBPF <code>sockops</code> for performance optimization</h3>
<p>Network connection is essentially socket communication. eBPF provides a function <a href=https://man7.org/linux/man-pages/man7/bpf-helpers.7.html>bpf_msg_redirect_hash</a>, to directly forward the packets sent by the application in the inbound socket to the outbound socket. By entering the function mentioned before, developers can perform any logic to decide the packet destination. According to this characteristic, the datapath of packets can noticeably be optimized in the kernel.</p>
<p>The <code>sock_map</code> is the crucial piece in recording information for packet forwarding. When a packet arrives, an existing socket is selected from the <code>sock_map</code> to forward the packet. As a result, we need to save all the socket information for packets to make the transportation process function properly. When there are new socket operations — like a new socket being created — the <code>sock_ops</code> function is executed. The socket metadata is obtained and stored in the <code>sock_map</code> to be used when processing packets. The common key type in the <code>sock_map</code> is a “quadruple” of source and destination addresses and ports. With the key and the rules stored in the map, the destination socket will be found when a new packet arrives.</p>
<h2 id=the-merbridge-approach>The Merbridge approach</h2>
<p>Let’s introduce the detailed design and implementation principles of Merbridge step by step, with a real scenario.</p>
<h3 id=istio-sidecar-traffic-interception-based-on-iptables>Istio sidecar traffic interception based on iptables</h3>
<p><img src=./imgs/1.png alt="Istio Sidecar Traffic Interception Based on iptables"></p>
<p>When external traffic hits your application’s ports, it will be intercepted by a <code>PREROUTING</code> rule in iptables, forwarded to port 15006 of the sidecar container, and handed over to Envoy for processing. This is shown as steps 1-4 in the red path in the above diagram.</p>
<p>Envoy processes the traffic using the policies issued by the Istio control plane. If allowed, the traffic will be sent to the actual container port of the application container.</p>
<p>When the application tries to access other services, it will be intercepted by an <code>OUTPUT</code> rule in iptables, and then be forwarded to port 15001 of the sidecar container, where Envoy is listening. This is steps 9-12 in the red path, similar to inbound traffic processing.</p>
<p>Traffic to the application port needs to be forwarded to the sidecar, then sent to the container port from the sidecar port, which is overhead. Moreover, iptables’ versatility determines that its performance is not always ideal because it inevitably adds delays to the whole datapath with different filtering rules applied. Although iptables is the common way to do packet filtering, in the Envoy proxy case, the longer datapath amplifies the bottleneck of packet filtering process in the kernel.</p>
<p>If we use <code>sockops</code> to directly connect the sidecar’s socket to the application’s socket, the traffic will not need to go through iptables rules, and thus performance can be improved.</p>
<h3 id=processing-outbound-traffic>Processing outbound traffic</h3>
<p>As mentioned above, we would like to use eBPF’s <code>sockops</code> to bypass iptables to accelerate network requests. At the same time, we also do not want to modify any parts of Istio, to make Merbridge fully adaptive to the community version. As a result, we need to simulate what iptables does in eBPF.</p>
<p>Traffic redirection in iptables utilizes its <code>DNAT</code> function. When trying to simulate the capabilities of iptables using eBPF, there are two main things we need to do:</p>
<ol>
<li>Modify the destination address, when the connection is initiated, so that traffic can be sent to the new interface.</li>
<li>Enable Envoy to identify the original destination address, to be able to identify the traffic.</li>
</ol>
<p>For the first part, we can use eBPF’s <code>connect</code> program to process it, by modifying <code>user_ip</code> and <code>user_port</code>.</p>
<p>For the second part, we need to understand the concept of <code>ORIGINAL_DST</code> which belongs to the <code>netfilter</code> module in the kernel.</p>
<p>When an application (including Envoy) receives a connection, it will call the <code>get_sockopt</code> function to obtain <code>ORIGINAL_DST</code>. If going through the iptables <code>DNAT</code> process, iptables will set this parameter, with the “original IP + port” value, to the current socket. Thus, the application can get the original destination address according to the connection.</p>
<p>We have to modify this call process through eBPF’s <code>get_sockopts</code> function. (<code>bpf_setsockopt</code> is not used here because this parameter does not currently support the optname of <code>SO_ORIGINAL_DST</code>).</p>
<p>Referring to the figure below, when an application initiates a request, it will go through the following steps:</p>
<ol>
<li>When the application initiates a connection, the <code>connect</code> program will modify the destination address to <code>127.x.y.z:15001</code>, and use <code>cookie_original_dst</code> to save the original destination address.</li>
<li>In the <code>sockops</code> program, the current socket information and the quadruple are saved in <code>sock_pair_map</code>. At the same time, the same quadruple and its corresponding original destination address will be written to <code>pair_original_dst</code>. (Cookie is not used here because it cannot be obtained in the <code>get_sockopt</code> program).</li>
<li>After Envoy receives the connection, it will call the <code>get_sockopt</code> function to read the destination address of the current connection. <code>get_sockopt</code> will extract and return the original destination address from <code>pair_original_dst</code>, according to the quadruple information. Thus, the connection is completely established.</li>
<li>In the data transport step, the <code>redir</code> program will read the sock information from <code>sock_pair_map</code> according to the quadruple information, and then forward it directly through <code>bpf_msg_redirect_hash</code> to speed up the request.</li>
</ol>
<p><img src=./imgs/2.png alt="Processing Outbound Traffic"></p>
<p>Why do we set the destination address to <code>127.x.y.z</code> instead of <code>127.0.0.1</code>? When different pods exist, there might be conflicting quadruples, and this gracefully avoids conflict. (Pods’ IPs are different, and they will not be in the conflicting condition at any time.)</p>
<h3 id=inbound-traffic-processing>Inbound traffic processing</h3>
<p>The processing of inbound traffic is basically similar to outbound traffic, with the only difference: revising the port of the destination to 15006.</p>
<p>It should be noted that since eBPF cannot take effect in a specified namespace like iptables, the change will be global, which means that if we use a Pod that is not originally managed by Istio, or an external IP address, serious problems will be encountered — like the connection not being established at all.</p>
<p>As a result, we designed a tiny control plane (deployed as a DaemonSet), which watches all pods — similar to the kubelet watching pods on the node — to write the pod IP addresses that have been injected into the sidecar to the <code>local_pod_ips</code> map.</p>
<p>When processing inbound traffic, if the destination address is not in the map, we will not do anything to the traffic.</p>
<p>The other steps are the same as for outbound traffic.</p>
<p><img src=./imgs/3.png alt="Processing Inbound Traffic"></p>
<h3 id=same-node-acceleration>Same-node acceleration</h3>
<p>Theoretically, acceleration between Envoy sidecars on the same node can be achieved directly through inbound traffic processing. However, Envoy will raise an error when accessing the application of the current pod in this scenario.</p>
<p>In Istio, Envoy accesses the application by using the current pod IP and port number. With the above scenario, we realized that the pod IP exists in the <code>local_pod_ips</code> map as well, and the traffic will be redirected to the pod IP on port 15006 again because it is the same address that the inbound traffic comes from. Redirecting to the same inbound address causes an infinite loop.</p>
<p>Here comes the question: are there any ways to get the IP address in the current namespace with eBPF? The answer is yes!</p>
<p>We have designed a feedback mechanism: When Envoy tries to establish the connection, we redirect it to port 15006. However, in the <code>sockops</code> step, we will determine if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard this connection in the <code>sockops</code> process. In the meantime, the current <code>ProcessID</code> and <code>IP</code> information will be written into the <code>process_ip</code> map, to allow eBPF to support correspondence between processes and IPs.</p>
<p>When the next request is sent, the same process need not be performed again. We will check directly from the <code>process_ip</code> map if the destination address is the same as the current IP address.</p>
<blockquote>
<p>Envoy will retry when the request fails, and this retry process will only occur once, meaning subsequent requests will be accelerated.</p>
</blockquote>
<p><img src=./imgs/4.png alt="Same-node acceleration"></p>
<h3 id=connection-relationship>Connection relationship</h3>
<p>Before applying eBPF using Merbridge, the data path between pods is like:</p>
<p><img src=./imgs/5.png alt="iptables&rsquo;s data path"></p>
<blockquote>
<p>Diagram From: <a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></p>
</blockquote>
<p>After applying Merbridge, the outbound traffic will skip many filter steps to improve the performance:</p>
<p><img src=./imgs/6.png alt="eBPF&rsquo;s data path"></p>
<blockquote>
<p>Diagram From: <a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></p>
</blockquote>
<p>If two pods are on the same machine, the connection can even be faster:</p>
<p><img src=./imgs/7.png alt="eBPF&rsquo;s data path on the same machine"></p>
<blockquote>
<p>Diagram From: <a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></p>
</blockquote>
<h2 id=performance-results>Performance results</h2>
<blockquote>
<p>The below tests are from our development, and not yet validated in production use cases.</p>
</blockquote>
<p>Let’s see the effect on overall latency using eBPF instead of iptables (lower is better):</p>
<p><img src=./imgs/8.png alt="Latency vs Client Connections Graph"></p>
<p>We can also see overall QPS after using eBPF (higher is better):</p>
<p><img src=./imgs/9.png alt="QPS vs Client Connections Graph"></p>
<blockquote>
<p>Test results are generated with <code>wrk</code>.</p>
</blockquote>
<h2 id=summary>Summary</h2>
<p>We have introduced the core ideas of Merbridge in this post. By replacing iptables with eBPF, the data transportation process can be accelerated in a mesh scenario. At the same time, Istio will not be changed at all. This means if you do not want to use eBPF any more, just delete the DaemonSet, and the datapath will be reverted to the traditional iptables-based routing without any problems.</p>
<p>Merbridge is a completely independent open source project. It is still at an early stage, and we are looking forward to having more users and developers to get engaged. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!</p>
<p>Merbridge Project: <a href=https://github.com/merbridge/merbridge>https://github.com/merbridge/merbridge</a></p>
<h2 id=see-also>See also</h2>
<ul>
<li>
<p><a href=https://ebpf.io/>https://ebpf.io/</a></p>
</li>
<li>
<p><a href=https://cilium.io/>https://cilium.io/</a></p>
</li>
<li>
<p><a href=https://github.com/merbridge/merbridge>Merbridge on GitHub</a></p>
</li>
<li>
<p><a href=https://developpaper.com/kubecon-2021-%EF%BD%9C-using-ebpf-instead-of-iptables-to-optimize-the-performance-of-service-grid-data-plane/>Using eBPF instead of iptables to optimize the performance of service grid data plane</a> by Liu Xu, Tencent</p>
</li>
<li>
<p><a href=https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/>Sidecar injection and transparent traffic hijacking process in Istio explained in detail</a> by Jimmy Song, Tetrate</p>
</li>
<li>
<p><a href=https://01.org/blogs/xuyizhou/2021/accelerate-istio-dataplane-ebpf-part-1>Accelerate the Istio data plane with eBPF</a> by Yizhou Xu, Intel</p>
</li>
<li>
<p><a href=https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/original_dst_filter>Envoy&rsquo;s Original Destination filter</a></p>
</li>
<li>
<p><a href=https://pt.slideshare.net/ThomasGraf5/accelerating-envoy-and-istio-with-cilium-and-the-linux-kernel/22>Accelerating Envoy and Istio with Cilium and the Linux Kernel</a></p>
</li>
</ul>
</div>
</main>
</div>
</div>
<footer class="bg-dark py-5 row d-print-none">
<div class="container-fluid mx-sm-5">
<div class=row>
<div class="col-6 col-sm-4 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list">
<a class=text-white target=_blank rel=noopener href=https://groups.google.com/g/merbridge aria-label="User mailing list">
<i class="fa fa-envelope"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank rel=noopener href=https://github.com/merbridge/merbridge aria-label=GitHub>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank rel=noopener href=https://join.slack.com/t/merbridge/shared_invite/zt-11uc3z0w7-DMyv42eQ6s5YUxO5mZ5hwQ aria-label=Slack>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Developer mailing list" aria-label="Developer mailing list">
<a class=text-white target=_blank rel=noopener href=https://groups.google.com/g/merbridge aria-label="Developer mailing list">
<i class="fa fa-envelope"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
<small class=text-white>&copy; 2022 The Merbridge Authors All Rights Reserved</small>
<p class=mt-2><a href=/about/>Merbridge</a></p>
</div>
</div>
</div>
</footer>
</div>
<script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script>
<script src=/js/main.min.3b172c13b62c2bea8b1c9d2599cddc8cf89718a92d792c680871c81ba43d8c85.js integrity="sha256-OxcsE7YsK+qLHJ0lmc3cjPiXGKkteSxoCHHIG6Q9jIU=" crossorigin=anonymous></script>
</body>
</html>
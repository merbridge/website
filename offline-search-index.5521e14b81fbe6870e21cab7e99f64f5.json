










































[{"body":"In the blog Deep Dive into Ambient Mesh - Traffic Path, we analyzed how Ambient Mesh forwards the ingress and egress traffic of Pod to ztunnel. It is implemented by iptables + TPROXY + routing table. The traffic datapath is relatively long compared to sidecar mode, and the principle is complicated. Moreover, it uses routing marks, which may cause unexpected behaviors in some cases when it relies on CNI or it is running in a CNI with the bridge mode. These severely limit the applicable scope of ambient mesh.\nThe main purpose of Merbridge is to replace iptables with eBPF to accelerate applications running in a service mesh. Ambient Mesh is a new mode of Istio. It is necessary for Merbridge to support this new mode. iptables is a powerful tool to block unwanted traffic, allow desired traffic, and redirect packets to specific addresses and ports, but it also has some weaknesses. First, iptables uses a linear matching method. When several applications simultaneously call a same program, conflicts may arise and make some features become unavailable. Second, although it is flexible enough, it still cannot be programmed as freely as eBPF. Therefore, replacing iptables with eBPF can help Ambient Mesh achieve traffic interception.\nObjectives As mentioned in Deep Dive into Ambient Mesh - Traffic Path, we set two objectives:\n Outgoing traffic from pods in Ambient Mesh should be intercepted and redirected to port 15001 of ztunnel. Traffic sent from host applications to pods in Ambient Mesh should be redirected to port 15006 of ztunnel.  Since istioin and other network interface cards (NICs) are completely designed to adapt to the native Ambient Mesh, we don’t need to make any changes.\nPain points analysis Ambient Mesh has a different operation mechanism from sidecars. According to the official definition of Istio, adding a Pod to Ambient Mesh does not require restarting the Pod and no any sidecar-related process is running in the Pod. It means:\n Merbridge used the CNI mode to enable the eBPF program to get the current Pod IP to make policy decisions, which is incompatible with ambient mesh. The reason is a pod will not be restarted after joining or leaving the ambient mesh, nor will it call the CNI plug-in. In the sidecar mode, the only thing you need to change is the destination address to 127.0.0.1:15001 in the connect hook of eBPF, but in the ambient mesh you need to replace the desitination IP with that of ztunnel.  In addtion, no sidecar-related process exists in a Pod running in an ambient mesh, so the legacy method of checking whether a port such as 15006 is listening in the current Pod is no longer applicable. It is necessary to redesign the scheme to check the environment where processes are running.\nTherefore, based on the above analysis, it is required to redesign the entire interception scheme so that Merbridge can support the ambient mesh.\nIn summary, we need to implement the following features:\n Redesign a scheme for judging whether a Pod is running in the ambient mesh Use eBPF to perceive current Pod IP regardless of CNIs Enable eBPF programs to know the ztunnel IP on the current node  Solution In version 0.7.2, cgroup id is used to improve the performance of the connect program. Usually, each container in a Pod has a proper cgroup id, which can be obtained through the bpf_get_current_cgroup_id function in the BPF program. The speed of the connect program can be optimized by writing IP information to a specific cgroup_info_map.\nAn ambient mesh is different from the legacy CNI listening on a special port in the network namespace for storing Pod-related information. In the ambient mesh, cgroup id is useful. If cgroup id can be associated with the Pod IP, you can get the current Pod IP in eBPF.\nSince CNI cannot be relied on anymore, we need change the scheme for obtaining the information of Pod status. For this reason, we detect the creation and revocation actions of local Pods by watching the process creation and revocation. We created a new tool to watch the process changes on a host: process-watcher project.\nRead the cgroup id and ip information from the process ID and writing it to the cgroup_info_map.\ntcg := cgroupInfo{ ID: cgroupInode, IsInMesh: in, CgroupIp: *(*[4]uint32)(_ip), Flags: flag, DetectedFlags: cgrinfo.DetectedFlags | AMBIENT_MESH_MODE_FLAG | ZTUNNEL_FLAG, } return ebpfs.GetCgroupInfoMap().Update(\u0026cgroupInode, \u0026tcg, ebpf.UpdateAny) Then get the current cgroup-related information in eBPF:\n__u64 cgroup_id = bpf_get_current_cgroup_id(); void *info = bpf_map_lookup_elem(\u0026cgroup_info_map, \u0026cgroup_id); Now, we can learn whether the current container has the ambient mesh enabled and it is located in a mesh or not.\nSecond, for the ztunnel IP, Istio implements it by adding NIC and binding fixed IPs. This scheme may have the risk of conflict, and the original addresses may be lost in some cases (such as SNAT). So Merbridge gives up the scheme and directly obtains the ztunnel IPs on the control plane, writes it into the map, and enables the eBPF program read it (this is faster).\nstatic inline __u32 *get_ztunnel_ip() { __u32 ztunnel_ip_key = ZTUNNEL_KEY; return (__u32 *)bpf_map_lookup_elem(\u0026settings, \u0026ztunnel_ip_key); } Then use the connect program to rewrite the destination address:\nctx-\u003euser_ip4 = ztunnel_ip[3]; ctx-\u003euser_port = bpf_htons(OUT_REDIRECT_PORT); With the association with the cgroup id, the Pod IP of current processes can be obtained in eBPF, so as to enforce policies. Forward the traffic from the Pod in the ambient mesh to the ztunnel, so that Merbridge can be compatible with the ambient mesh.\nThis will be a capability that is adaptable to all CNIs and can avoid the problem that the native ambient mesh cannot work well in most CNI modes.\nUsage and feedback Since the ambient mesh is still in its early stage and the support for ambient mode is relatively preliminary, some problems have not been well resolved, so the code of supporting for the ambient mode has not been merged into the main branch. If you want to experience the capability of Merbridge to implement traffic interception for ambient mesh instead of iptables, you can perform the following steps (it is required to install the ambient mesh in advance):\n Disable Istio CNI (set --set components.cni.enabled=false during installation, or delete Istio CNI’s DaemonSet kubectl -n istio-system delete ds istio-cni). Remove the init container of ztunnel (because it initializes iptables rules and NICs, which is not required for Merbridge). Install Merbridge by running kubectl apply -f https://github.com/merbridge/merbridge/raw/ambient/deploy/all-in-one.yaml  After the Merbridge is ready, you can use all capabilities of ambient mesh.\n*Attentions:\n The Ambient mode under Kind is not supported currently (we have a plan to support it in the future) The host kernel version needs to be not less than 5.7 cgroup v2 is required to be enabled This mode is also compatible with sidecars The debug mode will be enabled by default in an ambient mesh, which will have certain impact on performance  For more details see source code.\nIf you have any question, please reach out to us with Slack or add the wechat group to chat.\n","categories":"","description":"This blog describes how Merbridge supports Ambient Mesh.","excerpt":"This blog describes how Merbridge supports Ambient Mesh.","ref":"/blog/2022/11/11/ambient-mesh-support/","tags":"","title":"Merbridge now supports Ambient Mesh, no worry about CNI compatibility!"},{"body":"在 深入 Ambient Mesh - 流量路径 一文中，我们分析了 Ambient Mesh 如何实现在不引入 Sidecar 的情况下，将特定 Pod 的出入口流量转发到 ztunnel 进行处理。我们可以发现，其通过 iptables + TPROXY + 路由表的方式进行实现，路径比较长，理解起来比较复杂。而且，由于其使用 mark 做路由标记，这可能导致在某些同样依赖 CNI 的网络下无法正常工作，或者在一些 Bridge 模式的网络 CNI 下无法工作，会极大的限制 Ambient Mesh 使用场景。\nMerbridge 主要场景就是使用 eBPF 代替 iptables 为服务网格应用加速。Ambient Mesh 作为 Istio 服务网格的全新模式，Merbridge 自然也要支持这种新模式。 iptables 技术强大，为很多软件实现了各种功能，但在实际应用中也存在一些缺陷。首先，iptables 使用线性匹配方式，当多个应用使用相同能力时可能会产生冲突，进而导致某些功能不可用。其次，虽然其足够灵活，但是仍旧无法实现像 eBPF 一样自由编程的能力。 所以，使用 eBPF 技术代替 iptables 的能力，帮助 Ambient Mesh 实现流量拦截，这应该是一项令人兴奋的技术。\n确定目标 通过 深入 Ambient Mesh - 流量路径 一文，我们可以知道的是，我们最终的目标有两个：\n 将位于 Ambient Mesh 模式下的 Pod 向外发出的流量，拦截到 ztunnel 的 15001 端口。 当主机程序向 Ambient Mesh 模式下的 Pod 发送流量时，应该将流量重定向到 ztunnel 的 15006 端口。  至于其中的 istioin 等网卡，完全是为了配合 Ambient Mesh 原有模式设计的，所以无需关注。\n分析难点 在运行模式上，Ambient Mesh 模式和 Sidecar 模式存在很大区别。根据 Istio 官方的定义，将一个 Pod 加入 Ambient Mesh 并不需要重启 Pod，在 Pod 中也不存在 Sidecar 相关进程，这就意味着两个问题：\n 之前 Merbridge 使用 CNI 的方案，让 eBPF 程序能够感知到当前 Pod 的 IP 地址从而做出策略判断的方式将失效，因为 Pod 加入或移除 Ambient Mesh 模式并不会重启，也不会调用 CNI 插件，需要重新考量。 以前的流量拦截，只需要在 eBPF 的 connect 钩子中，将目标地址改为 127.0.0.1:15001，但是在 Ambient Mesh 模式下，需要将目标 IP 换成 ztunnel 的 IP 地址。  同时，由于 Ambient Mesh 模式下的 Pod 中，并不存在 Sidecar 相关进程，所以之前 Merbridge 用查看当前 Pod 中是否监听了 15006 等端口的方式也不再适用，需要重新设计方案判断进程的运行环境。\n所以，基于上述分析，基本上需要重新设计整个拦截方案，以便让 Merbridge 支持 Ambient Mesh 模式。\n总结下来，我们需要做这些事情：\n 重新设计判断 Pod 是否加入 Ambient Mesh 模式的方案 不依赖 CNI，实现 eBPF 感知当前 Pod IP 的方案 让 eBPF 程序知道当前节点的 ztunnel 的 IP 地址方案  解决方案 在 0.7.2 版本中，我们引入了使用 cgroup id 加速 connect 程序的性能。通常每一个 Pod 中的容器都有一个对应的 cgroup id，我们可以在 bpf 程序中，通过 bpf_get_current_cgroup_id 函数获取到。通过将 IP 信息写入专用的 cgroup_info_map ，可以优化 connect 程序的运行速度。\n之前 CNI 在网络命名空间中监听一个特殊的端口，用于存储 Pod 的信息，而 Ambient Mesh 模式与此不同。在 Ambient Mesh 模式中可以借助 cgroup id，如果能将 cgroup id 与 Pod IP 产生关联，那就可以在 eBPF 中获取到当前 Pod IP 信息。\n由于不能依赖 CNI，所以需要改变获取 Pod 状态变更信息的方案。为此，我们通过观测进程的创建和销毁信息来探测本地 Pod 的创建和销毁等操作，创建了一个新的工具，用来观测主机上进程的变更信息：process-watcher 项目。\n通过从进程 ID 读取所在的 cgroup id 和 ip 信息并将其写入 cgroup_info_map。\ntcg := cgroupInfo{ ID: cgroupInode, IsInMesh: in, CgroupIp: *(*[4]uint32)(_ip), Flags: flag, DetectedFlags: cgrinfo.DetectedFlags | AMBIENT_MESH_MODE_FLAG | ZTUNNEL_FLAG, } return ebpfs.GetCgroupInfoMap().Update(\u0026cgroupInode, \u0026tcg, ebpf.UpdateAny) 然后在 eBPF 中获取到当前 cgroup 所关联的信息：\n__u64 cgroup_id = bpf_get_current_cgroup_id(); void *info = bpf_map_lookup_elem(\u0026cgroup_info_map, \u0026cgroup_id); 通过这里，我们可以知道，当前的容器是否启用了 Ambient Mesh 模式、是否在网格中等信息。\n其次，对于 ztunnel 的 IP 地址，Istio 通过增加网卡绑定固定 IP 的方式实现，这种方案可能存在冲突的风险，也可能在某些情况下（比如 SNAT）会造成原地址信息丢失的情况。所以 Merbridge 放弃了此方案，直接在控制面获取 ztunnel 的 IP 地址，写入 map，让 eBPF 程序读取（这样速度更快）。\nstatic inline __u32 *get_ztunnel_ip() { __u32 ztunnel_ip_key = ZTUNNEL_KEY; return (__u32 *)bpf_map_lookup_elem(\u0026settings, \u0026ztunnel_ip_key); } 然后，可以利用 connect 程序，重写目标地址：\nctx-\u003euser_ip4 = ztunnel_ip[3]; ctx-\u003euser_port = bpf_htons(OUT_REDIRECT_PORT); 通过与 cgroup id 的关联，可以实现在 eBPF 中获取当前进程所在的 Pod IP 地址，从而进行策略操作，将 Ambient Mesh 模式下 Pod 发出的流量转发到 ztunnel 进行处理，从而实现 Merbridge 在 Ambient Mesh 模式下的兼容。\n这将是对所有 CNI 都适配的一种能力，可以避免原有 Ambient Mesh 模式不能很好的在大多数 CNI 模式下无法工作的问题。\n使用与反馈 由于目前 Ambient 仍处于早期阶段，且 Merbridge 对于 Ambient 模式的支持也相对初级，还有一些问题没有得到很好的解决，所以 Ambient 模式还没有合并入主分支。如果想要体验 Merbridge 代替 iptables 为 Ambient Mesh 实现流量拦截的能力，可以按照如下的方式操作（首先需要安装好 Ambient Mesh 模式的网格）：\n 禁用 Istio CNI（安装时设置 --set components.cni.enabled=false ，或删除 Istio CNI 的 DaemonSet kubectl -n istio-system delete ds istio-cni 的方式）。 删除 ztunnel 的 init 容器（因为它会初始化 iptables 规则、网卡等，而 Merbridge 不需要这个操作）。 使用 kubectl apply -f https://github.com/merbridge/merbridge/raw/ambient/deploy/all-in-one.yaml 安装支持 Merbridge 。  等待 Merbridge 就绪之后，即可以使用 Ambient Mesh 的所有能力。\n*注意：\n 当前不支持在 Kind 下使用 Ambient 模式（将在后续支持）； 主机内核版本需要大于等于 5.7； 需要开启 cgroup v2； 此模式也兼容 Sidecar 模式； Ambient 模式下安装默认会开启 debug 模式，会对性能造成一定影响。  更多实现细节可以查看源码。\n如果遇到任何问题，可在 Slack 中与我们反馈，或加入我们的技术交流微信群。\n","categories":"","description":"此篇博客介绍 Merbridge 支持 Ambient Mesh 的新特性。","excerpt":"此篇博客介绍 Merbridge 支持 Ambient Mesh 的新特性。","ref":"/zh/blog/2022/11/11/ambient-mesh-support/","tags":"","title":"Merbridge 支持 Ambient Mesh，无惧 CNI 兼容性！"},{"body":"最近 Kuma 正式发布了 2.0.0 版本，宣布了几项重大功能，其中第一条就是 Kuma 使用 eBPF 来加速应用访问。\n根据 Kuma 官方的描述，Kuma 正是使用了 Merbridge，来实现 eBPF 的能力。\n We are utilizing the Merbridge OSS project within our eBPF capabilities and are very excited that we have been able to contribute back to that library and become co-maintainers. We look forward to working more with the Merbridge team as we continue to explore different areas to include eBPF functionality in Kuma.\n 我们非常高兴 Merbridge 作为一个开源项目，能够为 Kuma 提供如此令人兴奋的能力。这意味着，基本不需要任何开销，即可降低网格应用通讯延迟！\nKuma 从 6 月开始，就在着手于集成 Merbridge 项目，尝试使用社区现有的能力来为 Kuma 提供加速能力。\n得益于 Merbridge 比较清晰的架构设计，Kuma 在很短的时间内就完成了对 Merbridge 的兼容。非常感谢 Kuma 社区能够为 Merbridge 贡献如此重要的兼容能力，这有助于双方社区共同成长！\n截止目前，Merbridge 已经支持了 Istio、Linkerd2 和 Kuma 等主流的服务网格，也计划了很多新的特性，比如 IPv4/IPv6 双栈支持、Ambient Mesh 支持、更低的内核版本要求等。希望 Merbridge 能够被越来越广泛的应用，并且能够真实地帮助到大家。\n","categories":"","description":"","excerpt":"最近 Kuma 正式发布了 2.0.0 版本，宣布了几项重大功能，其中第一条就是 Kuma 使用 eBPF 来加速应用访问。\n根据 Kuma …","ref":"/zh/blog/2022/11/08/kuma-2.0-with-merbridge/","tags":"","title":"Kuma 2.0 集成 Merbridge 降低 12% 的网络延迟"},{"body":"Recently, Kuma announced a major release of v2.0 with several new major features. A notable feature is that Kuma is using eBPF to improve the traffic flow.\nBased on the official release notes and blogs, Kuma implements the eBPF capabilities by integrating with Merbridge.\nA quote from Kuma 2.0 release blog:\n We are utilizing the Merbridge OSS project within our eBPF capabilities and are very excited that we have been able to contribute back to that library and become co-maintainers. We look forward to working more with the Merbridge team as we continue to explore different areas to include eBPF functionality in Kuma.\n As an open source project, we are very excited to see that Merbridge brings such capabitilities to Kuma. This case proves that traffic latency can be reduced without any extra overhead if you use Merbridge in a service mesh.\nSince June this year, Kuma developers have been working on integrating with Merbridge, trying to get the eBPF-based acceleration capabilities.\nThanks to the clear architecture of Merbridge, Kuma is smoothly adapted with Merbridge in days. A big thanks to the Kuma community for contributing such an important compatibility capability to Merbridge, which helps both communities grow together!\nSo far, Merbridge has the capabilities to support popular service mesh products like Istio, Linkerd2, and Kuma, and also has a clear plan to develop new features to support IPv4/IPv6 dual-stack, ambient mesh, and earlier versions of kernel. It is exciting to see that Merbridge gets used more widely. We really hope the project can help you land your project with eBPF technologies. We are looking forward to receiving more comments, and having more developers get involved.\n","categories":"","description":"","excerpt":"Recently, Kuma announced a major release of v2.0 with several new …","ref":"/blog/2022/11/08/kuma-2.0-with-merbridge/","tags":"","title":"Merbridge helps Kuma reduce network latency by 12%"},{"body":"Ambient Mesh has been released for a while, and some online articles have talked much about its usage and architecture. This blog will dive into the traffic path on data plane in the ambient mesh to help you fully understand the implementations of the ambient data plane.\nBefore start, you shall carefully read through introducing ambient mesh to learn the basic knowledge of the ambient mesh.\n For your convenience, the test environment can be deployed by following Get Started with Istio Ambient Mesh.\n Start from the moment you make a request In order to explore the traffic path, we first analyze the scenario where two services access each other in the ambient mesh (only for L4 mode on different nodes).\nAfter enabling the ambient mesh in the default namespace, all services will have capabilities of mesh governance.\nOur analysis starts from this command: kubectl exec deploy/sleep -- curl -s http://productpage:9080/ | head -n1\nIn the sidecar mode, Istio intercepts traffic through iptables. When you run the curl command in a sleep pod, the traffic will be forwarded by iptables to port 15001 of sidecar. However, in the ambient mesh, no sidecar exists in a pod, and it does not need restart to enable the ambient mesh. How to make sure the request is processed by ztunnel?\nEgress traffic interception To learn details about intercepting the egress traffic, let’s check the control plane components:\nkebe@pc $ kubectl -n istio-system get po NAME READY STATUS RESTARTS AGE istio-cni-node-5rh5z 1/1 Running 0 20h istio-cni-node-qsvsz 1/1 Running 0 20h istio-cni-node-wdffp 1/1 Running 0 20h istio-ingressgateway-5cfcb57bd-kx9hx 1/1 Running 0 20h istiod-6b84499b75-ncmn7 1/1 Running 0 20h ztunnel-nptf6 1/1 Running 0 20h ztunnel-vxv4b 1/1 Running 0 20h ztunnel-xkz4s 1/1 Running 0 20h In the sidecar mode, istio-cni is mainly a CNI to avoid permission leakage caused by using the istio-init container to process iptables rules. However, in the ambient mesh, istio-cni becomes a required component. Sidecars are theoretically not needed. Why is the istio-cni component being required?\nLet’s check the logs:\nkebe@pc $ kubectl -n istio-system logs istio-cni-node-qsvsz ... 2022-10-12T07:34:33.224957Z\tinfo\tambient\tAdding route for reviews-v1-6494d87c7b-zrpks/default: [table 100 10.244.1.4/32 via 192.168.126.2 dev istioin src 10.244.1.1] 2022-10-12T07:34:33.226054Z\tinfo\tambient\tAdding pod 'reviews-v2-79857b95b-m4q2g/default' (0ff78312-3a13-4a02-b39d-644bfb91e861) to ipset 2022-10-12T07:34:33.228305Z\tinfo\tambient\tAdding route for reviews-v2-79857b95b-m4q2g/default: [table 100 10.244.1.5/32 via 192.168.126.2 dev istioin src 10.244.1.1] 2022-10-12T07:34:33.229967Z\tinfo\tambient\tAdding pod 'reviews-v3-75f494fccb-92nq5/default' (e41edf7c-a347-45cb-a144-97492faa77bf) to ipset 2022-10-12T07:34:33.232236Z\tinfo\tambient\tAdding route for reviews-v3-75f494fccb-92nq5/default: [table 100 10.244.1.6/32 via 192.168.126.2 dev istioin src 10.244.1.1] As shown in the above output, for a pod in the ambient mesh, istio-cni performs the following actions:\n Add the pod to ipset Add a routing rule to table 100 (for its usage see below)  You can view the ipset contents on the node (note that the kind cluster is used here, you need to use docker exec to enter the host first):\nkebe@pc $ docker exec -it ambient-worker2 bash root@ambient-worker2:/# ipset list Name: ztunnel-pods-ips Type: hash:ip Revision: 0 Header: family inet hashsize 1024 maxelem 65536 Size in memory: 520 References: 1 Number of entries: 5 Members: 10.244.1.5 10.244.1.7 10.244.1.8 10.244.1.4 10.244.1.6 It is found that an ipset exists on the node where this pod is running. ipset holds many IPs for pods.\nkebe@pc $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES details-v1-76778d6644-wn4d2 1/1 Running 0 20h 10.244.1.9 ambient-worker2 \u003cnone\u003e \u003cnone\u003e notsleep-6d6c8669b5-pngxg 1/1 Running 0 20h 10.244.2.5 ambient-worker \u003cnone\u003e \u003cnone\u003e productpage-v1-7c548b785b-w9zl6 1/1 Running 0 20h 10.244.1.7 ambient-worker2 \u003cnone\u003e \u003cnone\u003e ratings-v1-85c74b6cb4-57m52 1/1 Running 0 20h 10.244.1.8 ambient-worker2 \u003cnone\u003e \u003cnone\u003e reviews-v1-6494d87c7b-zrpks 1/1 Running 0 20h 10.244.1.4 ambient-worker2 \u003cnone\u003e \u003cnone\u003e reviews-v2-79857b95b-m4q2g 1/1 Running 0 20h 10.244.1.5 ambient-worker2 \u003cnone\u003e \u003cnone\u003e reviews-v3-75f494fccb-92nq5 1/1 Running 0 20h 10.244.1.6 ambient-worker2 \u003cnone\u003e \u003cnone\u003e sleep-7b85956664-z6qh7 1/1 Running 0 20h 10.244.2.4 ambient-worker \u003cnone\u003e \u003cnone\u003e Therefore, this ipset holds a list of all PodIPs in the ambient mesh on the current node.\nWhere can this ipset be used?\nLet’s take a look at the iptables rules and you can find:\nroot@ambient-worker2:/# iptables-save *mangle ... -A POSTROUTING -j ztunnel-POSTROUTING ... -A ztunnel-PREROUTING -p tcp -m set --match-set ztunnel-pods-ips src -j MARK --set-xmark 0x100/0x100 You now learn that when a pod in the ambient mesh on a node (in the ztunnel-pods-ips ipset) initiates a request, its connection will be marked with 0x100/0x100.\nGenerally, it will be related to routing. Let’s check the routing rules:\nroot@ambient-worker2:/# ip rule 0: from all lookup local 100: from all fwmark 0x200/0x200 goto 32766 101: from all fwmark 0x100/0x100 lookup 101 102: from all fwmark 0x40/0x40 lookup 102 103: from all lookup 100 32766: from all lookup main 32767: from all lookup default The traffic marked with 0x100/0x100 goes via the routing table 101. Let’s check the routing table:\nroot@ambient-worker2:/# ip r show table 101 default via 192.168.127.2 dev istioout 10.244.1.2 dev veth5db63c11 scope link It can be clearly seen that the default gateway has been replaced with 192.168.127.2 via the istioout NIC (network interface card).\n192.168.127.2 does not belong to any of NodeIP, PodIP, and ClusterIP. The istioout NIC should not exist by default, then where does this IP come from? Since the traffic ultimately needs to go to ztunnel, you can check the ztunnel configuration to see if you can find the answer.\nkebe@pc $ kubectl -n istio-system get po ztunnel-vxv4b -o yaml apiVersion: v1 kind: Pod metadata: ... name: ztunnel-vxv4b namespace: istio-system ... spec: ... initContainers: - command: ... OUTBOUND_TUN=istioout ... OUTBOUND_TUN_IP=192.168.127.1 ZTUNNEL_OUTBOUND_TUN_IP=192.168.127.2 ip link add name p$INBOUND_TUN type geneve id 1000 remote $HOST_IP ip addr add $ZTUNNEL_INBOUND_TUN_IP/$TUN_PREFIX dev p$INBOUND_TUN ip link add name p$OUTBOUND_TUN type geneve id 1001 remote $HOST_IP ip addr add $ZTUNNEL_OUTBOUND_TUN_IP/$TUN_PREFIX dev p$OUTBOUND_TUN ip link set p$INBOUND_TUN up ip link set p$OUTBOUND_TUN up ... As above, ztunnel will be responsible for creating the istioout NIC, you now go to the node to check the NIC.\nroot@ambient-worker2:/# ip a 11: istioout: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 0a:ea:4e:e0:8d:26 brd ff:ff:ff:ff:ff:ff inet 192.168.127.1/30 brd 192.168.127.3 scope global istioout valid_lft forever preferred_lft forever Where is the gateway IP of 192.168.127.2? It is allocated in ztunnel.\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a Defaulted container \"istio-proxy\" out of: istio-proxy, istio-init (init) 2: eth0@if3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::448a:46ff:fe72:1d3b/64 scope link valid_lft forever preferred_lft forever 4: pistioout: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether c2:d0:18:20:3b:97 brd ff:ff:ff:ff:ff:ff inet 192.168.127.2/30 scope global pistioout valid_lft forever preferred_lft forever inet6 fe80::c0d0:18ff:fe20:3b97/64 scope link valid_lft forever preferred_lft forever You can now see that the traffic is going to ztunnel, but nothing else is done to the traffic at this time, it is simply routed to ztunnel. How to does Envoy in ztunnel process the traffic?\nLet’s continue to check the ztunnel configuration with many iptables rules. Let’s check the specific rules in ztunnel.\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save Defaulted container \"istio-proxy\" out of: istio-proxy, istio-init (init) ... *mangle -A PREROUTING -i pistioout -p tcp -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff ... COMMIT When traffic enters ztunnel, it will use TPROXY to transfer the traffic to port 15001 for processing, where 15001 is the port that Envoy actually listens to and process the pod egress traffic. As for TPROXY, you can learn relevant reference, and this blog will not repeat it further.\nSo when a pod is running in the ambient mesh, its egress traffic path is as follows:\n Initiate traffic from a process in pod The traffic flows via the node network and get marks by iptables on the node The traffic is forwarded to the ztunnel pod on current node by the routing table When the traffic reaches ztunnel, it will go through iptables for TPROXY (transparent proxy), and send the traffic to port 15001 of Envoy in the current pod.  So far in the ambient mesh, it is clear that the processing of pod egress traffic is relatively complex. The path is also relatively long, unlike the sidecar mode in which a traffic forwarding is directly completed in the pod.\nIngress traffic interception With the above experience, it is easy to learn that in the ambient mesh, the traffic interception is mainly through the method of MARK routing + TPROXY, and the ingress traffic should be similar.\nLet’s analyze it in the simplest way. When a process on a node, or a program on another host accesses a pod on the current node, the traffic goes through the host’s routing table. Let’s check the routing info when the response arrives at productpage-v1-7c548b785b-w9zl6(10.244.1.7):\nroot@ambient-worker2:/# ip r get 10.244.1.7 10.244.1.7 via 192.168.126.2 dev istioin table 100 src 10.244.1.1 uid 0 cache When accessing 10.244.1.7, the traffic will be routed to 192.168.126.2, and this rule is added by istio-cni.\nSimilarly 192.168.126.2 belongs to ztunnel:\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a Defaulted container \"istio-proxy\" out of: istio-proxy, istio-init (init) 2: eth0@if3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::448a:46ff:fe72:1d3b/64 scope link valid_lft forever preferred_lft forever 3: pistioin: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 7e:b2:e6:f9:a4:92 brd ff:ff:ff:ff:ff:ff inet 192.168.126.2/30 scope global pistioin valid_lft forever preferred_lft forever inet6 fe80::7cb2:e6ff:fef9:a492/64 scope link valid_lft forever preferred_lft forever By using the same analysis method, let’s check the iptables rules:\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save ... -A PREROUTING -i pistioin -p tcp -m tcp --dport 15008 -j TPROXY --on-port 15008 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff -A PREROUTING -i pistioin -p tcp -j TPROXY --on-port 15006 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff ... If you directly access the node via PodIP + pod port, the traffic will be forwarded to port 15006 of ztunnel, which is the port to handle the ingress traffic in Istio.\nAs for the traffic whose destination port is port 15008, this is the port used by ztunnel for L4 traffic tunneling. This blog will not explain this further.\nHandle traffic for Envoy itself In the sidecar mode, Envoy and user containers run in the same network namespace. For the traffic from user containers, you need to intercept all the traffic to guarantee complete control of the traffic. However, is it also required in the ambient mesh?\nThe answer is no, because Envoy has been isolated from other pods. The traffic sent by Envoy does not require special notice. In other words, you only need to handle ingress traffic for ztunnel, so the rules in ztunnel seem relatively simple.\nWrapping up As above explained, this blog mainly analyzed the scheme for Pod traffic interception in the ambient mesh, but this blog has not involved with how to handle L7 traffic and the specific principles of ztunnel implementation. The next plan is to analyze the detailed traffic paths in ztunnel and waypoint proxy.\n","categories":"","description":"This blog analyzes the traffic path on data plane in the ambient mesh.","excerpt":"This blog analyzes the traffic path on data plane in the ambient mesh.","ref":"/blog/2022/10/13/ambient-mesh-data-path/","tags":"","title":"Deep Dive into Ambient Mesh - Traffic Path"},{"body":"Ambient Mesh 发布已经有一段时间，也有不少文章讲述了其用法和架构。本文将深入梳理数据面流量在 Ambient 模式下的路径，帮助大家全面地理解 Ambient 数据面的实现方案。\n在阅读本文之前，请先阅读 Ambient Mesh 介绍 了解 Ambient Mesh 的基本架构。\n 为了方便阅读和同步实践，本文使用的环境按照 Ambient 使用 的方式进行部署。\n 从发起请求的一刻开始 为了探究流量路径，首先我们分析同在 Ambient 模式下的两个服务互相访问的情况（仅 L4 模式，不同节点）。\n在 default 命名空间启用 Ambient 模式后，所有的服务都将具备网格治理的能力。\n我们的分析从这条命令开始： kubectl exec deploy/sleep -- curl -s http://productpage:9080/ | head -n1\n在 Sidecar 模式下，Istio 通过 iptables 进行流量拦截，当在 sleep 的 Pod 中执行 curl 时，流量会被 iptables 转发到 Sidecar 的 15001 端口进行处理。 但是在 Ambient 模式下，在 Pod 中不存在 Sidecar，且开启 Ambient 模式也不需要重启 Pod，那它的请求如何确保被 ztunnel 处理呢？\n出口流量拦截 要了解出口流量拦截的方案，我们首先可以看一下控制面组件：\nkebe@pc $ kubectl -n istio-system get po NAME READY STATUS RESTARTS AGE istio-cni-node-5rh5z 1/1 Running 0 20h istio-cni-node-qsvsz 1/1 Running 0 20h istio-cni-node-wdffp 1/1 Running 0 20h istio-ingressgateway-5cfcb57bd-kx9hx 1/1 Running 0 20h istiod-6b84499b75-ncmn7 1/1 Running 0 20h ztunnel-nptf6 1/1 Running 0 20h ztunnel-vxv4b 1/1 Running 0 20h ztunnel-xkz4s 1/1 Running 0 20h 在 Ambient 模式下 istio-cni 变成了默认组件。 而在 Sidecar 模式下，istio-cni 主要是为了避免使用 istio-init 容器处理 iptables 规则而造成权限泄露等情况推出的 CNI 插件。 但是在 Ambient 模式下，理论上不需要 Sidecar，为什么还需要 istio-cni 呢？\n我们可以看一下日志：\nkebe@pc $ kubectl -n istio-system logs istio-cni-node-qsvsz ... 2022-10-12T07:34:33.224957Z\tinfo\tambient\tAdding route for reviews-v1-6494d87c7b-zrpks/default: [table 100 10.244.1.4/32 via 192.168.126.2 dev istioin src 10.244.1.1] 2022-10-12T07:34:33.226054Z\tinfo\tambient\tAdding pod 'reviews-v2-79857b95b-m4q2g/default' (0ff78312-3a13-4a02-b39d-644bfb91e861) to ipset 2022-10-12T07:34:33.228305Z\tinfo\tambient\tAdding route for reviews-v2-79857b95b-m4q2g/default: [table 100 10.244.1.5/32 via 192.168.126.2 dev istioin src 10.244.1.1] 2022-10-12T07:34:33.229967Z\tinfo\tambient\tAdding pod 'reviews-v3-75f494fccb-92nq5/default' (e41edf7c-a347-45cb-a144-97492faa77bf) to ipset 2022-10-12T07:34:33.232236Z\tinfo\tambient\tAdding route for reviews-v3-75f494fccb-92nq5/default: [table 100 10.244.1.6/32 via 192.168.126.2 dev istioin src 10.244.1.1] 我们可以看到，对于在 Ambient 模式下的 Pod，istio-cni 做了两件事情：\n 添加 Pod 到 ipset 添加了一个路由规则到 table 100（后面介绍用途）  我们可以在其所在的节点上查看一下 ipset 里面的内容（注意，这里使用 kind 集群，需要用 docker exec 先进入所在主机）：\nkebe@pc $ docker exec -it ambient-worker2 bash root@ambient-worker2:/# ipset list Name: ztunnel-pods-ips Type: hash:ip Revision: 0 Header: family inet hashsize 1024 maxelem 65536 Size in memory: 520 References: 1 Number of entries: 5 Members: 10.244.1.5 10.244.1.7 10.244.1.8 10.244.1.4 10.244.1.6 我们发现这个 Pod 所在的节点上有一个 ipset，其中保存了很多 IP。这些 IP 是 PodIP：\nkebe@pc $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES details-v1-76778d6644-wn4d2 1/1 Running 0 20h 10.244.1.9 ambient-worker2 \u003cnone\u003e \u003cnone\u003e notsleep-6d6c8669b5-pngxg 1/1 Running 0 20h 10.244.2.5 ambient-worker \u003cnone\u003e \u003cnone\u003e productpage-v1-7c548b785b-w9zl6 1/1 Running 0 20h 10.244.1.7 ambient-worker2 \u003cnone\u003e \u003cnone\u003e ratings-v1-85c74b6cb4-57m52 1/1 Running 0 20h 10.244.1.8 ambient-worker2 \u003cnone\u003e \u003cnone\u003e reviews-v1-6494d87c7b-zrpks 1/1 Running 0 20h 10.244.1.4 ambient-worker2 \u003cnone\u003e \u003cnone\u003e reviews-v2-79857b95b-m4q2g 1/1 Running 0 20h 10.244.1.5 ambient-worker2 \u003cnone\u003e \u003cnone\u003e reviews-v3-75f494fccb-92nq5 1/1 Running 0 20h 10.244.1.6 ambient-worker2 \u003cnone\u003e \u003cnone\u003e sleep-7b85956664-z6qh7 1/1 Running 0 20h 10.244.2.4 ambient-worker \u003cnone\u003e \u003cnone\u003e 所以，这个 ipset 保存了当前节点上所有处于 Ambient 模式下的 PodIP 列表。\n那这个 ipset 在哪可以用到呢？\n我们看一下 iptables 规则，可以发现：\nroot@ambient-worker2:/# iptables-save *mangle ... -A POSTROUTING -j ztunnel-POSTROUTING ... -A ztunnel-PREROUTING -p tcp -m set --match-set ztunnel-pods-ips src -j MARK --set-xmark 0x100/0x100 通过这个我们知道，当节点上处于 Ambient 模式的 Pod（ztunnel-pods-ips ipset 中）发起请求时，其连接会被打上 0x100/0x100 的标记。\n一般在这种情况下，会与路由相关，我们看一下路由规则：\nroot@ambient-worker2:/# ip rule 0:\tfrom all lookup local 100:\tfrom all fwmark 0x200/0x200 goto 32766 101:\tfrom all fwmark 0x100/0x100 lookup 101 102:\tfrom all fwmark 0x40/0x40 lookup 102 103:\tfrom all lookup 100 32766:\tfrom all lookup main 32767:\tfrom all lookup default 可以看到，被标记了 0x100/0x100 的流量会走 table 101 的路由表，我们可以查看路由表：\nroot@ambient-worker2:/# ip r show table 101 default via 192.168.127.2 dev istioout 10.244.1.2 dev veth5db63c11 scope link 可以明显看到，默认网关被换成了 192.168.127.2，且走了 istioout 网卡。\n这里就有问题了，192.168.127.2 这个 IP 并不属于 NodeIP、PodIP、ClusterIP 中的任意一种，istioout 网卡默认应该也不存在，那这个 IP 是谁创建的呢？ 因为流量最终需要发往 ztunnel，我们可以查看 ztunnel 的配置，看看能否找到答案。\nkebe@pc $ kubectl -n istio-system get po ztunnel-vxv4b -o yaml apiVersion: v1 kind: Pod metadata: ... name: ztunnel-vxv4b namespace: istio-system ... spec: ... initContainers: - command: ... OUTBOUND_TUN=istioout ... OUTBOUND_TUN_IP=192.168.127.1 ZTUNNEL_OUTBOUND_TUN_IP=192.168.127.2 ip link add name p$INBOUND_TUN type geneve id 1000 remote $HOST_IP ip addr add $ZTUNNEL_INBOUND_TUN_IP/$TUN_PREFIX dev p$INBOUND_TUN ip link add name p$OUTBOUND_TUN type geneve id 1001 remote $HOST_IP ip addr add $ZTUNNEL_OUTBOUND_TUN_IP/$TUN_PREFIX dev p$OUTBOUND_TUN ip link set p$INBOUND_TUN up ip link set p$OUTBOUND_TUN up ... 如上，ztunnel 会负责创建 istioout 网卡，我们现在去节点上查看对应网卡。\nroot@ambient-worker2:/# ip a 11: istioout: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 0a:ea:4e:e0:8d:26 brd ff:ff:ff:ff:ff:ff inet 192.168.127.1/30 brd 192.168.127.3 scope global istioout valid_lft forever preferred_lft forever 那 192.168.127.2 这个网关 IP 在哪呢？它被分配在了 ztunnel 里面。\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a Defaulted container \"istio-proxy\" out of: istio-proxy, istio-init (init) 2: eth0@if3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::448a:46ff:fe72:1d3b/64 scope link valid_lft forever preferred_lft forever 4: pistioout: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether c2:d0:18:20:3b:97 brd ff:ff:ff:ff:ff:ff inet 192.168.127.2/30 scope global pistioout valid_lft forever preferred_lft forever inet6 fe80::c0d0:18ff:fe20:3b97/64 scope link valid_lft forever preferred_lft forever 现在可以看到，流量会到 ztunnel 里面，但此时并没有对流量做任何其它操作，只是简单地路由到了 ztunnel。如何才能让 ztunnel 里面的 Envoy 对流量进行处理呢？\n我们继续看一看 ztunnel 的配置，其中写了很多 iptables 规则。我们可以进入 ztunnel 看一下具体的规则：\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save Defaulted container \"istio-proxy\" out of: istio-proxy, istio-init (init) ... *mangle -A PREROUTING -i pistioout -p tcp -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff ... COMMIT 现在可以看到，当流量进入 ztunnel 时，会使用 TPROXY 将流量转入 15001 端口进行处理，此处的 15001 即为 Envoy 实际监听用于处理 Pod 出口流量的端口。 关于 TPROXY，大家可以自行学习相关信息，本文不再赘述。\n所以，总结下来，当 Pod 处于 Ambient 模式下，其出口流量路径大致为：\n 从 Pod 里面的进程发起流量。 流量流经所在节点网络，经节点的 iptables 进行标记。 节点上的路由表会将流量转发到当前节点的 ztunnel Pod。 流量到达 ztunnel 时，会经过 iptables 进行 TPROXY 透明代理，将流量交给当前 Pod 中的 Envoy 的 15001 端口进行处理。  到此我们可以看出，在 Ambient 模式下，对于 Pod 出口流量的处理相对复杂。路径也比较长，不像 Sidecar 模式，直接在 Pod 内部完成流量转发。\n入口流量拦截 有了上面的经验，我们不难发现，Ambient 模式下，对于流量的拦截主要通过 MARK 路由 + TPROXY 的方式，入口流量应该也差不多。\n我们采用最简单的方式分析一下。当节点上的进程，或者其他主机上的程序相应访问当前节点上的 Pod 时，流量会经过主机的路由表。 我们查看一下当响应访问 productpage-v1-7c548b785b-w9zl6(10.244.1.7) 时的路由信息：\nroot@ambient-worker2:/# ip r get 10.244.1.7 10.244.1.7 via 192.168.126.2 dev istioin table 100 src 10.244.1.1 uid 0 cache 我们可以看到，当访问 10.244.1.7 时，流量会被路由到 192.168.126.2，而这条规则正是由上面 istio-cni 添加的。\n同样地 192.168.126.2 这个 IP 属于 ztunnel：\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- ip a Defaulted container \"istio-proxy\" out of: istio-proxy, istio-init (init) 2: eth0@if3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 46:8a:46:72:1d:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.2.3/24 brd 10.244.2.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::448a:46ff:fe72:1d3b/64 scope link valid_lft forever preferred_lft forever 3: pistioin: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 7e:b2:e6:f9:a4:92 brd ff:ff:ff:ff:ff:ff inet 192.168.126.2/30 scope global pistioin valid_lft forever preferred_lft forever inet6 fe80::7cb2:e6ff:fef9:a492/64 scope link valid_lft forever preferred_lft forever 按照相同的分析方法，我们看一下 iptables 规则：\nkebe@pc $ kubectl -n istio-system exec -it ztunnel-nptf6 -- iptables-save ... -A PREROUTING -i pistioin -p tcp -m tcp --dport 15008 -j TPROXY --on-port 15008 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff -A PREROUTING -i pistioin -p tcp -j TPROXY --on-port 15006 --on-ip 127.0.0.1 --tproxy-mark 0x400/0xfff ... 如果直接在节点上访问 PodIP + Pod 端口，流量会被转发到 ztunnel 的 15006 端口，而这就是 Istio 处理入口流量的端口。\n至于目标端口为 15008 端口的流量，这是 ztunnel 用来做四层流量隧道的端口。本文暂不细述。\n对于 Envoy 自身的流量处理 我们知道，在 Sidecar 模式下，Envoy 和业务容器运行在相同的网络命名空间中。对于业务容器的流量，我们需要全部拦截，以保证对流量的完全掌控，但是在 Ambient 模式下是否需要呢？\n答案是否定的，因为 Envoy 已经被独立到其它 Pod 中，Envoy 发出的流量是不需要特殊处理的。换言之，对于 ztunnel，我们只需要处理入口流量即可，所以 ztunnel 中的规则看起来相对简单。\n未完待续… 上面我们主要分析了在 Ambient 模式下对于 Pod 流量拦截的方案，还没有涉及到七层流量的处理以及 ztunnel 实现的具体原理，后续将分析流量在 ztunnel 和 waypoint proxy 中详细的处理路径。\n","categories":"","description":"此篇博客介绍 Ambient Mesh 中数据面的流量路径。","excerpt":"此篇博客介绍 Ambient Mesh 中数据面的流量路径。","ref":"/zh/blog/2022/10/13/ambient-mesh-data-path/","tags":"","title":"深入 Ambient Mesh - 流量路径"},{"body":"The CNI mode is designed to better adapt to the service mesh functions. Before having this mode, Merbridge was limited to certain scenarios. The biggest problem was that it could not adapt to the sidecar annotations from the container injected by Istio, which led to Merbridge cannot exclude traffic from certain ports and IP ranges. Furthermore, Merbridge was only able to handle requests inside the pod, which means the external traffic sent to the pod was not handled.\nTherefore, we have implemented the Merbridge CNI to address these issues.\nWhy CNI mode is needed First, Merbridge had a small control plane before, which listened to pods resources, and wrote the current node IP into the map of local_pod_ips for use by connect. However, since the connect program only works at the host kernel layer, it won’t know which pod’s traffic is being processed. Thus, configurations like excludeOutboundPorts cannot be handled. In order to be able to adapt to the injected sidecar annotation excludeOutboundPorts, we need to let the eBPF program know which Pod’s request is currently being processed.\nTo this end, we have designed a method to cooperate with the CNI, through which you can get the current Pod IP to validate special configurations for the Pod.\nSecond, for early versions of Merbridge, only connect would process requests from the host, which had no problem for intra-node pod communication. However, it becomes problematic when traffic flows between different nodes. According to the previous logic, the traffic will not be modified during the cross-node communication, which will lead to the use of iptables at the end.\nHere, we turned to the XDP program for processing the inbound traffic. The XDP program needs to mount a network card, which also needs to use CNI.\nHow does CNI work This section will explore how CNI works and how to use CNI to solve the issues mentioned above.\nHow to use CNI to let eBPF have the current Pod IP When a pod is created, we write Pod IP into the map mark_pod_ips_map through CNI, where the key is a random value, and the value is the Pod IP. Then, we listen to a special port 39807 in the NetNS of the current Pod, and write the key to the mark of this port socket using setsockopt.\nIn eBPF, we get the recorded mark information of port 39807 through bpf_sk_lookup_tcp, and use it to get the current Pod IP (also the current NetNS) from mark_pod_ips_map.\nWith the current Pod IP, we can determine the path to route traffic (such as excludeOutboundPorts) according to the configuration of this Pod.\nIn addition, we also optimized the quadruple conflicts by using bpf_bind to bind the source IP and using 127.0.0.1 as the destination IP, which also prepares for future support of IPv6.\nHow to handle ingress traffic In order to handle inbound traffic, we introduced the XDP program, which works on the network card and can modify the original data packets.\nWe use the XDP program to modify the destination port as 15006 when the traffic reaches the Pod, so as to complete traffic forwarding.\nAt the same time, considering the possibility that the host directly accesses the Pod, and in order to reduce the scope of influence, we choose to attach the XDP program to the Pod’s network card. This requires the ability of CNI to perform additional operations when creating Pods\nHow to use CNI mode? CNI mode is disabled by default. You need to enable it manually with the following command.\n curl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml | sed 's/--cni-mode=false/--cni-mode=true/g' | kubectl apply -f - Notes CNI mode is in beta The CNI mode is a new feature that may not be perfect. We welcome your feedback and suggestions to help improve Merbridge.\nIf you are trying to do benchmark test using tools like Istio perf benchmark, it is suggested to enable the CNI mode. Otherwise the test results will be inaccurate.\nCheck whether the host can enable the hardware-checksum capability In order to ensure the CNI mode works properly, the hardware-checksum capability is disabled by default, which may affect network performance. It is recommended to check whether you can enable this capability on the host before enabling the CNI mode. If yes, we suggest to set --hardware-checksum=true for best performance.\n| grep tx-checksum-ipv4` 为 on 表示开启。-- Test method: if ethtool -k \u003cnetwork card\u003e | grep tx-checksum-ipv4 is on, it means enabled.\n","categories":"","description":"This blog explains how CNI works in Merbridge.","excerpt":"This blog explains how CNI works in Merbridge.","ref":"/blog/2022/05/18/cni-mode/","tags":"","title":"Merbridge CNI Mode"},{"body":"Merbridge CNI 模式的出现，旨在能够更好地适配服务网格的功能。之前没有 CNI 模式时，Merbridge 能够做得事情比较有限。其中最大的问题是不能适配注入 Istio 的 Sidecar Annotation，这就导致 Merbridge 无法排除某些端口或 IP 段的流量等。 同时，由于之前 Merbridge 只处理 Pod 内部的连接请求，这就导致，如果是外部发送到 Pod 的流量，Merbridge 将无法处理。\n为此，我们精心设计了 Merbridge CNI，旨在解决这些问题。\n为什么需要 CNI 模式？ 其一，之前的 Merbridge 只有一个很小的控制面，其监听 Pod 资源，将当前节点的 IP 信息写入 local_pod_ips 的 map，以供 connect 使用。 但是，connect 程序由于工作在主机内核层，其无法知道当前正在处理的是哪个 Pod 的流量，就没法处理如 excludeOutboundPorts 等配置。 为了能够适配注入 excludeOutboundPorts 的 Sidecar Annotation，我们需要让 eBPF 程序能够得知当前正在处理哪个 Pod 的请求。\n为此，我们设计了一套方法，与 CNI 配合，能够获取当前 Pod 的 IP，以适配针对 Pod 的特殊配置。\n其二，在之前的 Merbridge 版本中，只有 connect 会处理主机发起的请求，这在同一台主机上的 Pod 互相通讯时，是没有问题的。但是在不同主机之间通讯时就会出现问题，因为按照之前的逻辑，在跨节点通讯时流量不会被修改，这会导致在接收端还是离不开 iptables。\n这次，我们依靠 XDP 程序，解决入口流量处理的问题。因为 XDP 程序需要挂载网卡，所以也需要借助 CNI。\nCNI 如何解决问题？ 这里我们将探讨 CNI 的工作原理，以及如何使用 CNI 来解决问题。\n如何通过 CNI 让 eBPF 程序获取当前正在处理的 Pod IP？ 我们通过 CNI，在 Pod 创建的时候，将 Pod 的 IP 信息写入一个 Map（mark_pod_ips_map），其 Key 为一个随机的值，Value 为 Pod 的 IP。然后，在当前 Pod 的 NetNS 里面监听一个特殊的端口 39807，将 Key 使用 setsockopt 写入这个端口 socket 的 mark。\n在 eBPF 中，我们通过 bpf_sk_lookup_tcp 取得端口 39807 的 Mark 信息，然后从 mark_pod_ips_map 中即可取得当前 NetNS（也是当期 Pod）的 IP。\n有了当前 Pod IP 之后，我们可以根据这个 Pod 的配置，确认流量处理路径（比如 excludeOutboundPorts）。\n同时，我们还使用 Pod 优化了之前解决四元组冲突的方案，改为使用 bpf_bind 绑定源 IP，目的 IP 直接使用 127.0.0.1，为了后续支持 IPv6 做准备。\n如何处理入口流量？ 为了能够处理入口流量，我们引入了 XDP 程序，XDP 程序作用在网卡上，能够对原始数据包做修改。 我们借助 XDP 程序，在流量到达 Pod 的时候，修改目的端口为 15006 以完成流量转发。\n同时考虑到可能存在主机直接访问 Pod 的情况，也为了减小影响范围，我们选择将 XDP 程序附加到 Pod 的网卡上。这就需要借助 CNI 的能力，在创建 Pod 时进行附加操作。\n如何体验 CNI 模式？ CNI 模式默认被关闭，需要手动开启。\n可以使用以下命令一键开启：\ncurl -sSL https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml | sed 's/--cni-mode=false/--cni-mode=true/g' | kubectl apply -f - 注意事项 CNI 模式处于测试阶段 CNI 模式刚被设计和开发出来，可能存在不少问题，我们欢迎大家在测试阶段进行反馈，或者提出更好的建议，以帮助我们改进 Merbridge！\n如果需要使用注入 Istio perf benchmark 等工具进行测试性能，请开启 CNI 模式，否则会导致性能测试结果不准确。\n需要注意主机是否可开启 hardware-checksum 能力 为了保证 CNI 模式的正常运行，我们默认关闭了 hardware-checksum 能力，这可能会影响到网络性能。建议大家在开启 CNI 模式前，先确认主机是否可开启 hardware-checksum 能力。如果可以开启，建议设置 --hardware-checksum=true 以获得最佳的性能表现。\n测试方法：ethtool -k \u003c网卡\u003e | grep tx-checksum-ipv4 为 on 表示开启。\n","categories":"","description":"此篇博客将向您介绍 Merbridge CNI 的工作原理。","excerpt":"此篇博客将向您介绍 Merbridge CNI 的工作原理。","ref":"/zh/blog/2022/05/18/cni-mode/","tags":"","title":"Merbridge CNI 模式"},{"body":"Merbridge and Cilium Cilium is a great open source software that provides a lot of networking capabilities for cloud native applications based on eBPF, with a lot of great designs. Among others, Cilium designed a set of sockmap-based redir capabilities to help accelerate network communications, which inspired us and is the basis for Merbridge to provide network acceleration. It is a really great design.\nMerbridge leverages the great foundation that Cilium has provided, along with some targeted adaptations we’ve made in the Service Mesh, to make it easier to apply eBPF technology to Service Mesh.\nOur development team have learned a lot eBPF theoretical knowledge, practical methods, and testing methods, from Cilium’s detailed documentation and our frequent exchanges with the Cilium technical team. All these together helps make Merbridge possible.\nThanks again to the Cilium project and community, and to Cilium for these great designs.\n","categories":"","description":"","excerpt":"Merbridge and Cilium Cilium is a great open source software that …","ref":"/blog/2022/04/23/merbridge-and-cilium/","tags":"","title":"Merbridge and Cilium"},{"body":"Merbridge 与 Cilium Cilium 是一款基于 eBPF 为云原生应用提供诸多网络能力的优秀开源软件，有很多很棒的设计。例如，Cilium 设计了一套基于 sockmap 的 redir 能力，帮助加速网络通讯，这给了我们很大的启发，也是 Merbridge 提供网络加速的基础，这真是一个非常棒的设计。\nMerbridge 借助于 Cilium 打下的良好基础，加上我们在服务网格领域做地一些针对性的适配，让大家可以更加方便地将 eBPF 技术应用于服务网格。\n我们的开发团队从 Cilium 提供的资料中学习了很多关于 eBPF 的理论知识、实践方法和测试方法等，也与 Cilium 技术团队多有交流，也就是因为这些经历，才能有 Merbridge 项目的诞生。\n再次衷心感谢 Cilium 项目和社区，以及 Cilium 的这些优秀设计。\n","categories":"","description":"","excerpt":"Merbridge 与 Cilium Cilium 是一款基于 eBPF 为云原生应用提供诸多网络能力的优秀开源软件，有很多很棒的设计。例 …","ref":"/zh/blog/2022/04/23/merbridge-and-cilium/","tags":"","title":"Merbridge 和 Cilium"},{"body":"On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.\nIn this livestream, we discussed a lot of Merbridge-related issues, including a live demo that will help you get a quick overview of Merbridge’s features and usage.\nAlso, the PPT is available here for download.\nIf you are interested, see:\n ","categories":"","description":"","excerpt":"On March 29, 2022, Solo.io and Merbridge co-hosted a livestream.\nIn …","ref":"/blog/2022/03/29/solo-io-livestream/","tags":"","title":"Livestream with Solo.io"},{"body":"2022 年 3 月 29 日，Solo.io 和 Merbridge 共同举办了一场直播活动。\n在这次直播中，我们一起探讨了很多与 Merbridge 相关的问题，其中包含了一个线上 Demo，可以帮你快速了解 Merbridge 的功能和使用方法。\n同时，PPT 可以在这里下载。\n如果您有兴趣，可以查看：\n ","categories":"","description":"","excerpt":"2022 年 3 月 29 日，Solo.io 和 Merbridge 共同举办了一场直播活动。\n在这次直播中， …","ref":"/zh/blog/2022/03/29/solo-io-livestream/","tags":"","title":"与 Solo.io 一起举办的直播活动"},{"body":"Merbridge - Accelerate your mesh with eBPF Replacing iptables rules with eBPF allows transporting data directly from inbound sockets to outbound sockets, shortening the datapath between sidecars and services.\nIntroduction The secret of Istio’s abilities in traffic management, security, observability and policy is all in the Envoy proxy. Istio uses Envoy as the “sidecar” to intercept service traffic, with the kernel’s netfilter packet filter functionality configured by iptables.\nThere are shortcomings in using iptables to perform this interception. Since netfilter is a highly versatile tool for filtering packets, several routing rules and data filtering processes are applied before reaching the destination socket. For example, from the network layer to the transport layer, netfilter will be used for processing for several times with the rules predefined, like pre_routing, post_routing and etc. When the packet becomes a TCP packet or UDP packet, and is forwarded to user space, some additional steps like packet validation, protocol policy processing and destination socket searching will be performed. When a sidecar is configured to intercept traffic, the original data path can become very long, since duplicated steps are performed several times.\nOver the past two years, eBPF has become a trending technology, and many projects based on eBPF have been released to the community. Tools like Cilium and Pixie show great use cases for eBPF in observability and network packet processing. With eBPF’s sockops and redir capabilities, data packets can be processed efficiently by directly being transported from an inbound socket to an outbound socket. In an Istio mesh, it is possible to use eBPF to replace iptables rules, and accelerate the data plane by shortening the data path.\nWe have created an open source project called Merbridge, and by applying the following command to your Istio-managed cluster, you can use eBPF to achieve such network acceleration.\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml  Attention: Merbridge uses eBPF functions which require a Linux kernel version ≥ 5.7.\n With Merbridge, the packet datapath can be shortened directly from one socket to another destination socket, and here’s how it works.\nUsing eBPF sockops for performance optimization Network connection is essentially socket communication. eBPF provides a function bpf_msg_redirect_hash, to directly forward the packets sent by the application in the inbound socket to the outbound socket. By entering the function mentioned before, developers can perform any logic to decide the packet destination. According to this characteristic, the datapath of packets can noticeably be optimized in the kernel.\nThe sock_map is the crucial piece in recording information for packet forwarding. When a packet arrives, an existing socket is selected from the sock_map to forward the packet. As a result, we need to save all the socket information for packets to make the transportation process function properly. When there are new socket operations — like a new socket being created — the sock_ops function is executed. The socket metadata is obtained and stored in the sock_map to be used when processing packets. The common key type in the sock_map is a “quadruple” of source and destination addresses and ports. With the key and the rules stored in the map, the destination socket will be found when a new packet arrives.\nThe Merbridge approach Let’s introduce the detailed design and implementation principles of Merbridge step by step, with a real scenario.\nIstio sidecar traffic interception based on iptables When external traffic hits your application’s ports, it will be intercepted by a PREROUTING rule in iptables, forwarded to port 15006 of the sidecar container, and handed over to Envoy for processing. This is shown as steps 1-4 in the red path in the above diagram.\nEnvoy processes the traffic using the policies issued by the Istio control plane. If allowed, the traffic will be sent to the actual container port of the application container.\nWhen the application tries to access other services, it will be intercepted by an OUTPUT rule in iptables, and then be forwarded to port 15001 of the sidecar container, where Envoy is listening. This is steps 9-12 in the red path, similar to inbound traffic processing.\nTraffic to the application port needs to be forwarded to the sidecar, then sent to the container port from the sidecar port, which is overhead. Moreover, iptables’ versatility determines that its performance is not always ideal because it inevitably adds delays to the whole datapath with different filtering rules applied. Although iptables is the common way to do packet filtering, in the Envoy proxy case, the longer datapath amplifies the bottleneck of packet filtering process in the kernel.\nIf we use sockops to directly connect the sidecar’s socket to the application’s socket, the traffic will not need to go through iptables rules, and thus performance can be improved.\nProcessing outbound traffic As mentioned above, we would like to use eBPF’s sockops to bypass iptables to accelerate network requests. At the same time, we also do not want to modify any parts of Istio, to make Merbridge fully adaptive to the community version. As a result, we need to simulate what iptables does in eBPF.\nTraffic redirection in iptables utilizes its DNAT function. When trying to simulate the capabilities of iptables using eBPF, there are two main things we need to do:\n Modify the destination address, when the connection is initiated, so that traffic can be sent to the new interface. Enable Envoy to identify the original destination address, to be able to identify the traffic.  For the first part, we can use eBPF’s connect program to process it, by modifying user_ip and user_port.\nFor the second part, we need to understand the concept of ORIGINAL_DST which belongs to the netfilter module in the kernel.\nWhen an application (including Envoy) receives a connection, it will call the get_sockopt function to obtain ORIGINAL_DST. If going through the iptables DNAT process, iptables will set this parameter, with the “original IP + port” value, to the current socket. Thus, the application can get the original destination address according to the connection.\nWe have to modify this call process through eBPF’s get_sockopts function. (bpf_setsockopt is not used here because this parameter does not currently support the optname of SO_ORIGINAL_DST).\nReferring to the figure below, when an application initiates a request, it will go through the following steps:\n When the application initiates a connection, the connect program will modify the destination address to 127.x.y.z:15001, and use cookie_original_dst to save the original destination address. In the sockops program, the current socket information and the quadruple are saved in sock_pair_map. At the same time, the same quadruple and its corresponding original destination address will be written to pair_original_dst. (Cookie is not used here because it cannot be obtained in the get_sockopt program). After Envoy receives the connection, it will call the get_sockopt function to read the destination address of the current connection. get_sockopt will extract and return the original destination address from pair_original_dst, according to the quadruple information. Thus, the connection is completely established. In the data transport step, the redir program will read the sock information from sock_pair_map according to the quadruple information, and then forward it directly through bpf_msg_redirect_hash to speed up the request.  Why do we set the destination address to 127.x.y.z instead of 127.0.0.1? When different pods exist, there might be conflicting quadruples, and this gracefully avoids conflict. (Pods’ IPs are different, and they will not be in the conflicting condition at any time.)\nInbound traffic processing The processing of inbound traffic is basically similar to outbound traffic, with the only difference: revising the port of the destination to 15006.\nIt should be noted that since eBPF cannot take effect in a specified namespace like iptables, the change will be global, which means that if we use a Pod that is not originally managed by Istio, or an external IP address, serious problems will be encountered — like the connection not being established at all.\nAs a result, we designed a tiny control plane (deployed as a DaemonSet), which watches all pods — similar to the kubelet watching pods on the node — to write the pod IP addresses that have been injected into the sidecar to the local_pod_ips map.\nWhen processing inbound traffic, if the destination address is not in the map, we will not do anything to the traffic.\nThe other steps are the same as for outbound traffic.\nSame-node acceleration Theoretically, acceleration between Envoy sidecars on the same node can be achieved directly through inbound traffic processing. However, Envoy will raise an error when accessing the application of the current pod in this scenario.\nIn Istio, Envoy accesses the application by using the current pod IP and port number. With the above scenario, we realized that the pod IP exists in the local_pod_ips map as well, and the traffic will be redirected to the pod IP on port 15006 again because it is the same address that the inbound traffic comes from. Redirecting to the same inbound address causes an infinite loop.\nHere comes the question: are there any ways to get the IP address in the current namespace with eBPF? The answer is yes!\nWe have designed a feedback mechanism: When Envoy tries to establish the connection, we redirect it to port 15006. However, in the sockops step, we will determine if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard this connection in the sockops process. In the meantime, the current ProcessID and IP information will be written into the process_ip map, to allow eBPF to support correspondence between processes and IPs.\nWhen the next request is sent, the same process need not be performed again. We will check directly from the process_ip map if the destination address is the same as the current IP address.\n Envoy will retry when the request fails, and this retry process will only occur once, meaning subsequent requests will be accelerated.\n Connection relationship Before applying eBPF using Merbridge, the data path between pods is like:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n After applying Merbridge, the outbound traffic will skip many filter steps to improve the performance:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n If two pods are on the same machine, the connection can even be faster:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n Performance results  The below tests are from our development, and not yet validated in production use cases.\n Let’s see the effect on overall latency using eBPF instead of iptables (lower is better):\nWe can also see overall QPS after using eBPF (higher is better):\n Test results are generated with wrk.\n Summary We have introduced the core ideas of Merbridge in this post. By replacing iptables with eBPF, the data transportation process can be accelerated in a mesh scenario. At the same time, Istio will not be changed at all. This means if you do not want to use eBPF any more, just delete the DaemonSet, and the datapath will be reverted to the traditional iptables-based routing without any problems.\nMerbridge is a completely independent open source project. It is still at an early stage, and we are looking forward to having more users and developers to get engaged. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!\nMerbridge Project: https://github.com/merbridge/merbridge\nSee also   https://ebpf.io/\n  https://cilium.io/\n  Merbridge on GitHub\n  Using eBPF instead of iptables to optimize the performance of service grid data plane by Liu Xu, Tencent\n  Sidecar injection and transparent traffic hijacking process in Istio explained in detail by Jimmy Song, Tetrate\n  Accelerate the Istio data plane with eBPF by Yizhou Xu, Intel\n  Envoy’s Original Destination filter\n  Accelerating Envoy and Istio with Cilium and the Linux Kernel\n  ","categories":"","description":"","excerpt":"Merbridge - Accelerate your mesh with eBPF Replacing iptables rules …","ref":"/blog/2022/03/01/merbridge-introduce/","tags":"","title":"Merbridge - Accelerate your mesh with eBPF"},{"body":"一行代码使用 eBPF 代替 iptables 加速 Istio 介绍 以 Istio 为首的服务网格技术正在被越来越多的企业关注，其使用 Sidecar 借助 iptables 技术实现流量拦截，可以处理所有应用的出入口流量，以实现诸如治理、观测、加密等能力。\n但是使用 iptables 的方式进行拦截，由于需要对出入口都拦截，会让原本只需要在内核态处理两次的链路变成四次，会损失不少性能，这在一些要求高性能的场景下显然是有影响的。\n近两年，由于 eBPF 技术的兴起，不少围绕 eBPF 的项目也应声而出，eBPF 在可观测性和网络包的处理方面也有不少优秀的案例。如 Cilium、px.dev 等项目。\n借助 eBPF 的 sockops 和 redir 能力，可以高效的处理数据包，再结合实际场景，那么我们就可以使用 eBPF 去代替 iptables 为 Istio 进行加速。\n现在，我们开源了 Merbridge 项目，只需要在您的 Istio 集群执行以下命令，即可直接使用 eBPF 代替 iptables 实现网络加速！\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml  注意：当前仅支持在 5.7 版本及以上的内核下运行，请事先升级您的内核版本。\n eBPF 的 sockops 加速 网络连接本质上是 socket 的通讯，eBPF 提供了一个 bpf_msg_redirect_hash 函数，用来将应用发出的包，直接转发到对端的 socket 上面，可以极大的加速包在内核中的处理流程。\n这里需要一个 sock_map，需要根据当前的数据包信息，从 sock_map 中挑选一个存在的 socket 连接，转发请求，所以，需要在 sockops 的 hook 处或者其它地方将 socket 信息保存到 sock_map，并提供根据 key 查到 socket 的规则（一般为四元组）。\n原理 下面，将按照实际的场景，逐步的介绍 Merbridge 详细的设计和实现原理，这将让你对 Merbridge 或者 eBPF 有一个初步的了解。\nIstio 基于 iptables 的原理 如上图所示，当外部流量相应访问应用的端口时，会在 iptables 中被 PREROUTING 拦截，最后转发到 Sidecar 容器的 15006 端口，然后交给 Envoy 来进行处理。（图中红色 1 2 3 4 的路径）\nEnvoy 根据从控制平面下发的规则进行处理，处理完成后，会发送请求给实际的容器端口。\n当应用想要访问其它服务时，会在 iptables 中 OUTPUT 拦截，然后转发给 Sidecar 容器的 15001 端口（Envoy 监听）。（图中红色 9 10 11 12 的路径）然后和入口流量处理差不多。\n由此可以看到，原本流量可以直接到应用端口，但是中间需要通过 iptables 转发到 Sidecar，然后又让 Sidecar 发送给应用，这无疑增加了开销。并且，iptables 的通用性决定了它的性能没有很理想。会在整条链路上增加不少延迟。\n如果我们能使用 sockops 去直接连接 Sidecar 到应用的 Socket，这样可以使流量不经过 iptables，可以提高性能。\n出口流量处理 如上所述，我们希望使用 eBPF 的 sockops 来绕过 iptables 以加速网络请求。同时，我们希望创造的是一个能够完全适配社区版 Istio，不做任何改造。所以，我们需要模拟 iptables 所做的操作。\n这个时候我们在看回 iptables 本身，其使用 DNAT 功能做流量转发。\n想要用 eBPF 模拟 iptables 的能力，那么就需要使用 eBPF 实现类似 iptables DNAT 的能力。\n这里主要有两个点：\n 修改连接发起时的目的地址，让流量能够发送到新的接口； 让 Envoy 能识别原始的目的地址，以能够识别流量；  对于其中第一点，我们可以使用 eBPF 的 connect 程序来做，通过修改 user_ip 和 user_port 实现。\n对于其中第二点，需要用到 ORIGINAL_DST 的概念。这个在内核中其实是在 netfilter 模块专属的。\n其原理就是，应用程序（包括 Envoy）会在收到连接之后，调用 get_sockopts 函数，获取 ORIGINAL_DST，如果经过了 iptables 的 DNAT，那么 iptables 就会给当前的 socket 设置这个值，并把原有的 IP + 端口写入这个值，应用程序就可以根据连接拿到原有的目的地址。\n那么我们就需要通过 eBPF 的 get_sockopt 程序来修改这个调用。（不用 **bpf_setsockopt** 的原因是因为目前这个参数并不支持 SO_ORIGINAL_DST` 的 optname）\n参见下图，在应用向外发起请求时，会经过如下阶段：\n 在应用向外发起连接时，connect 程序会将目标地址修改为 127.x.y.z:15001，并用 cookie_original_dst 保存原始目的地址。 在 sockops 程序中，将当前 sock 和四元组保存在 sock_pair_map 中。同时，将四元组信息和对应的原始目的地址写入 pair_original_dst 中（之所以不用 cookie，是因为在 get_sockopt 程序中无法获取当前 cookie）。 Envoy 收到连接之后会调用 getsockopt 获取当前连接的目的地址，get_sockopt 程序会根据四元组信息从 pair_original_dst 取出原始目的地址并返回，由此连接完全建立。 在发送数据阶段，redir 程序会根据四元组信息，从 sock_pair_map 中读取 sock，然后通过 bpf_msg_redirect_hash 进行直`接转发，加速请求。  其中，之所以在 connect 的时候，修改目的地址为 127.x.y.z 而不是 127.0.0.1，是因为在不同的 Pod 中，可能产生冲突的四元组，使用此方式即可巧妙的避开。（每个 Pod 间的目的 IP 就已经不同了，不存在冲突的情况）\n入口流量处理 入口流量处理基本和出口流量类似，唯一差别：只需要将目的地址的端口改成 15006 即可。\n但是，需要注意的是，由于 eBPF 不像 iptables 能在指定命名空间生效，它是全局的，这就造成如果我们将一个本来不是 Istio 所管理的 Pod，或者就是一个外部的 IP 地址，也做了这个操作的话，那就会引起严重问题，会请求直接无法建立连接。\n所以这里我们设计了一个小的控制平面（以 DaemonSet 方式部署），其通过 Watch 所有的 Pod，类似于像 kubelet 那样获取当前节点的 Pod 列表，将已经被注入了 Sidecar 的 Pod IP 地址写入 local_pod_ips 这个 map。\n当我们在做入口流量处理的时候，如果目的地址不在这个列表之中，我们就不做处理，让它走原来的逻辑，这样就可以比较灵活且简单的处理入口流量。\n其他的流程和出口流量流程一样。\n同节点加速 通过入口流量处理，理论上，我们已经可以直接加速同节点的 Envoy 到 Envoy 的加速。但是存在一个问题。就是在这种场景下，Envoy 访问当前 Pod 的应用的时候会出错。\n在 Istio 中，Envoy 访问应用的方式是使用当前 PodIP 加服务端口。经过上面入口流量处理章节，其实我们会发现，由于 PodIP 肯定也存在于 local_pod_ips 中，那么这个请求会被转发到 PodIP + 15006 端口，这显然是不行的，会造成无限递归。\n那么我们也没办法在 eBPF 中获取当前 ns 的 IP 地址信息，怎么办？\n为此，我们设计了一套反馈机制：\n即，在 Envoy 尝试建立连接的时候，我们还是会走重定向到 15006 端口，但是，在 sockops 阶段，我们会判断源 IP 和目的地址 IP是否一致，如果一致，代表发送了错误的请求，那么我们会在 sockops 丢弃这个连接，并将当前的 ProcessID 和 IP 地址信息写入 process_ip 这个 map，让 eBPF 支持进程和 IP 的对应关系。\n当下次请求发送时，我们直接从 process_ip 表检查目的地址是否和当前 IP 地址\n Envoy 会在请求失败的时候重试，且这个错误只会发生一次，后续的连接会非常快。\n 连接关系 在没有使用 Merbridge（eBPF） 优化之前，Pod 到 Pod 间的访问入下图所示：\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n 在使用 Merbridge（eBPF）优化之后，出入口流量会使用直接跳过很多内核模块，提高性能：\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n 同时，如果两个 Pod 在同一台机器上，那么他们之间的通讯将更加高效：\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n 以上，通过使用 eBPF 在主机上对相应的连接进行处理，可以大幅度的减少内核处理流量的流程，提升服务之间的通讯质量。\n加速效果  下面的测试只是一个基本的测试，不是非常严谨。\n 下图展示了使用 eBPF 代替 iptables 之后，整体延迟的情况（越低越好）：\n下图展示了使用 eBPF 代替 iptables 之后，整体 QPS 的情况（越高越好）：\n 以上数据使用 wrk 测试得出。\n Merbridge 项目 以上介绍的都是 Merbridge 项目的核心能力，其通过使用 eBPF 代替 iptables，可以在服务网格场景下，完全无感知的对流量通路进行加速。同时，我们不会对现有的 Istio 做任何修改，原有的逻辑依然畅通，这意味着，如果不再希望使用 eBPF，那么可以直接删除掉 DaemonSet，改为传统的 iptables 方式也不会出任何问题。\nMerbridge 是一个完全独立的开源项目，此时还处于早期阶段，我们希望可以有更多的用户或者开发者参与其中，使用先进的技术能力，优化我们的服务网格。\n项目地址：https://github.com/merbridge/merbridge\n参考文档：\n eBPF Cilium Merbridge on GitHub Using eBPF instead of iptables to optimize the performance of service grid data plane by Liu Xu, Tencent Sidecar injection and transparent traffic hijacking process in Istio explained in detail by Jimmy Song, Tetrate Accelerate the Istio data plane with eBPF by Yizhou Xu, Intel Envoy’s Original Destination filter Accelerating Envoy and Istio with Cilium and the Linux Kernel  ","categories":"","description":"","excerpt":"一行代码使用 eBPF 代替 iptables 加速 Istio 介绍 以 Istio 为首的服务网格技术正在被越来越多的企业关注， …","ref":"/zh/blog/2022/03/01/merbridge-introduce/","tags":"","title":"一行代码，使用 eBPF 代替 iptables 加速服务网格"},{"body":"What is Merbridge Merbridge is designed to make traffic interception and forwarding more efficient for service mesh. It replaced iptables with eBPF.\neBPF (extended Berkeley Packet Filter) can run user’s programs in the Linux kernel without modifying the kernel code or loading kernel modules. It is widely used in networking, security, monitoring and other relevant fields. Compared with iptables, Merbridge can shorten the data path between sidecars and services and therefore accelerate networking. Meanwhile, using Merbridge will not change the original architecture of Istio. The original logic is still valid. This means that if you don’t want Merbridge anymore, just delete the DaemonSet. The original iptables will function again without any troubles.\nWhat Merbridge can do Merbridge has following core features:\n  Processing outbound traffic\nMerbridge uses eBPF’s connect program to modify user_ip and user_port, so as to change the destination address of a connection and ensure traffic can be sent to the new interface. In order to help Envoy identify the original destination, the application (incl. Envoy) will call the get_sockopt function to get ORIGINAL_DST when receiving a connection.\n  Processing inbound traffic\nInbound traffic is processed similarly to outbound traffic. Note that eBPF cannot take effect in a specified namespace like iptables, so changes will be global. It means that if we apply eBPF to Pods that are not originally managed by Istio, or an external IP, serious problems will occur, e.g., cannot establish a connection.\nTo address this issue, we designed a tiny control plane, deployed as a DaemonSet. It can help watch and get a list of all pods on the node, similar to kubelet. Then, Pod IPs injected into the sidecar will be written into the local_pod_ips map. For traffic with a destination address not in the map, Merbridge will not intercept it.\n  Accelerating networking\nIn Istio, Envoy visits the application by the current podIP and port number. Because the podIP exists in the local_pod_ips map, traffic will be redirected to the podIP on port 15006, producing an infinite loop. Are there any ways for eBPF to get the IP address in the current namespace? Yes! We have designed a feedback mechanism: When Envoy tries to establish a connection, we redirect it to port 15006. When it moves to sockops, we will check if the source IP and the destination IP are the same. If yes, it means the wrong request is sent, and we will discard it in the sockops process. Meanwhile, the current ProcessID and IP will be written into the process_ip map, allowing eBPF to support corresponding relationship between processes and IPs. When the next request is sent, we will check directly from the process_ip map if the destination is the same as the current IP. Envoy will retry when the request fails. This retry process will only occur once, and subsequent connections will go very fast.\n  Why Merbridge is better In the service mesh scenario, in order to use sidecars for traffic management without the application being aware of it, ingress and egress traffic of Pods should be forwarded to the sidecar. The most common solution is using the redirect capability of iptables (netfilter) to forward the original traffic. However, this approach will increase network latency, because iptables intercept both egress and ingress traffic. For example, the traffic that originally flows directly to the application now is forwarded to the sidecar by iptables (netfilter), and the sidecar will then forward it to the final application. The data path becomes very long, since duplicated steps are performed several times.\nLuckily, eBPF provides a function bpf_msg_redirect_hash to directly forward packets from applications in the inbound socket to the outbound socket. By doing so, packet processing can be greatly accelerated in the kernel. Therefore, we hope to replace iptables with eBPF. That’s how Merbridge came into being.\nWhen to use Merbridge Merbridge is recommended if you have any of following problems:\n In scenarios that require high-performance connections, using iptables will increase latency.  The performance of iptables control plane and data plane degrades dramatically as the number of containers in the cluster increases. It needs to traverse and modify all the rules every time a new rule is added. Systems that use IP addresses for security filtering will come under increasing pressure as Pod lifecycle is getting shorter, sometimes just a few seconds, because it requires more frequent updates of iptables rules. Using iptables to achieve transparent interception needs a conntrack module for connection trace. It will cause a lot of consumption when there are many connections.   The system cannot use iptables for some reasons.  Sometimes it needs to process numerous active connections simultaneously, but using iptables is easily to have a full conntrack table. Sometimes numerous connections should be processed in one second, which will exceed limit of the conntrack table. For example, if you try to process 1100 connections per second with timeout set as 120 seconds and a table capacity of 128k, it would exceed the conntrack table’s limit (128k/120 seconds = 1092 connections/second).   Due to security concerns, some ordinary Pods cannot have too many permissions, but using Istio (without CNI) must allow these Pods to gain more permissions.  Running the init container may require permissions such as NET_ADMIN. Running an iptables command may need CAP_NET_ADMIN permission. Mounting a file system may need CAP_SYS_ADMIN permission.    What Merbridge will change Using eBPF can greatly simplify the kernel’s processing of traffic and make inter-service communication more efficient.\n  Before applying eBPF with Merbridge, the data path between pods is like:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n   After applying Merbridge, the outbound traffic can skip many filter steps to improve performance:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n   If two pods are on the same node, the connection will be even faster:\n Diagram From: Accelerating Envoy and Istio with Cilium and the Linux Kernel\n   Merbridge is a completely independent open source project. It is still at an early stage, and we wish to have more users and developers engaged in. It would be greatly appreciated if you would try this new technology to accelerate your mesh, and provide us with some feedback!\n","categories":"","description":"This page outlines Merbridge and its features, applicable scenarios, and competitiveness.\n","excerpt":"This page outlines Merbridge and its features, applicable scenarios, …","ref":"/docs/overview/","tags":"","title":"Overview"},{"body":"Merbridge 是什么 Merbridge 专为服务网格设计，使用 eBPF 代替传统的 iptables 劫持流量，能够让服务网格的流量拦截和转发能力更加高效。\neBPF (extended Berkeley Packet Filter) 技术可以在 Linux 内核中运行用户编写的程序，而且不需要修改内核代码或加载内核模块，目前广泛应用于网络、安全、监控等领域。相比传统的 iptables 流量劫持技术，基于 eBPF 的 Merbridge 可以绕过很多内核模块，缩短边车和服务间的数据路径，从而加速网络。Merbridge 没有对现有的 Istio 作出任何修改，原有的逻辑依然畅通。这意味着，如果您不想继续使用 eBPF，直接删除相关的 DaemonSet 就能恢复为传统的 iptables 方式，不会出现任何问题。\nMerbridge 有哪些特性 Merbridge 的核心特性包括：\n  出口流量处理\nMerbridge 使用 eBPF 的 connect 程序，修改 user_ip 和 user_port 以改变连接发起时的目的地址，从而让流量能够发送到新的接口。为了让 Envoy 识别出原始目的地址，应用程序（包括 Envoy）会在收到连接之后调用 get_sockopts 函数，获取 ORIGINAL_DST。\n  入口流量处理\n入口流量处理与出口流量处理基本类似。需要注意的是，eBPF 是全局性的，不能在指定的命名空间生效。因此，如果对原本不是由 Istio 管理的 Pod 或者对外部的 IP 地址执行此操作，就会导致请求无法建立连接。为了解决此问题，Merbridge 设计了一个小的控制平面（以 DaemonSet 方式部署），通过 Watch 所有的 Pod，用类似于 kubelet 的方式获取当前节点的 Pod 列表，然后将已经注入 Sidecar 的 Pod IP 地址写入 local_pod_ips map。如果流量的目的地址不在该列表中，Merbridge 就不做处理，转而使用原来的逻辑。这样就可以灵活且便捷地处理入口流量。\n  同节点加速\n在 Istio 中，Envoy 使用当前 PodIP 加服务端口来访问应用程序。由于 PodIP 肯定也存在于 local_pod_ips ，所以请求就会被转发到 PodIP + 15006 端口。这样会造成无限递归，不能在 eBPF 获取当前命名空间的 IP 地址信息。因此，需要一套反馈机制：在 Envoy 尝试建立连接时仍然重定向到 15006 端口，在 sockops 阶段判断源 IP 和目的 IP 是否一致。如果一致，说明发送了错误的请求，需要在 sockops 丢弃该连接，并将当前的 ProcessID 和 IP 地址信息写入 process_ip map，让 eBPF 支持进程和 IP 的对应关系。下次发送请求时直接从 process_ip 表检查目的地址是否与当前 IP 地址一致。Envoy 会在请求失败时重试，且这个错误只会发生一次，后续的连接会非常快。\n  为什么需要 Merbridge 在服务网格场景中，为了能在应用程序完全无感知的情况下利用边车进行流量治理，需要把 Pod 的出入口流量都转发到边车。在这种情况下，最常见的解决方案就是使用 iptables (netfilter) 的重定向能力。这种方案的缺点是增加了网络延迟，因为 iptables 对出口流量和入口流量都进行拦截。以入口流量为例，原本直接流向应用的流量，需要先由 iptables 转发到边车，再由边车将流量转发到实际的应用。原本只需要在内核态处理两次的链路如今变成四次，损失了不少性能。\n幸运的是，eBPF 技术提供了一个 bpf_msg_redirect_hash 函数，可以将应用发出的数据包直接转发到对端的 socket，从而极大地加速数据包在内核中的处理流程。我们希望用 eBPF 技术代替 iptables 提高服务网格的效率，于是 诞生了 Merbridge。\nMerbridge 适用哪些场景 如果您遇到下列任一问题，建议使用 Merbridge：\n 在需要高性能连接的场景下，使用 iptables 会增加延迟。  由于集群中容器数量增加，iptables 控制面和数据面的性能会急剧下降。在 iptables 控制面的接口设计中，每添加一条规则都需要遍历并修改所有的规则。 由于 Pod 生命周期越来越短，有时甚至只有几秒钟，这就需要快速更新 iptables 规则，使用 IP 地址进行安全过滤的系统将承受越来越大的压力。 由于使用 iptables 实现透明劫持需要借助 conntrack 模块跟踪连接，连接较多时会造成大量消耗。   系统因某些原因不能使用 iptables。  有时需同时处理大量活动连接，但使用 iptables 容易出现 conntrack 表满的情况。 有时又需每秒处理极大数量的连接，但超出了 conntrack 表的限制。例如，在超时设置为 120 秒且表容量是 128k 的情况下，如果尝试每秒处理 1100 个连接，就会超出 conntrack 表的限制（128k/120秒 = 1092 连接/秒）。   出于安全考虑不能为普通的 Pod 授予太多权限，但使用 Istio（若无 CNI）必须允许 Pod 获得更多权限。  运行 init 容器，可能需要 NET_ADMIN 等权限。 运行 iptables 命令，对应的进程可能需要 CAP_NET_ADMIN 权限。 挂载文件系统，对应的进程可能需要 CAP_SYS_ADMIN 权限。    Merbridge 如何改变连接关系 使用 eBPF 在主机上处理连接，可以显著简化内核处理流量的流程，提升服务之间的通讯质量。\n  如果不用 Merbridge (eBPF)，Pod 到 Pod 间的访问连接关系如下图。\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n   使用 Merbridge (eBPF) 优化之后，处理出入口流量时会跳过很多内核模块，从而加速网络。\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n   如果两个 Pod 在同一节点上，使用 Merbridge (eBPF) 能让 Pod 之间的通讯更加高效。\n 图片参考：Accelerating Envoy and Istio with Cilium and the Linux Kernel\n   Merbridge 是完全独立的开源项目，目前仍处于早期阶段。希望有更多的用户或开发者参与其中，不断完善 Merbridge，共同优化服务网格。如果您发现了 Merbridge 的漏洞而且有兴趣帮助修复，非常欢迎您提交 Pull Request，附上您的修复代码，我们会及时处理您的 PR。\n","categories":"","description":"本文概述了 Merbridge 的含义、特性、适用场景等内容。\n","excerpt":"本文概述了 Merbridge 的含义、特性、适用场景等内容。\n","ref":"/zh/docs/overview/","tags":"","title":"概览"},{"body":"Prerequisites  Use kernel 5.7 or a higher version. Check your version with uname -r. Activate cgroup2 in your system. Check the status with mount | grep cgroup2.  Installation Merbridge can be installed on Istio and Linkerd2 only.\nInstall on Istio Apply the following command to install Merbridge:\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml Install on Linkerd2 Apply the following command to install Merbridge:\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-linkerd.yaml Install on Kuma Apply the following command to install Merbridge:\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-kuma.yaml Verification Verify installation Before you start this verification, make sure all Pods relevant to Merbridge are running well. You can check Pod status in Istio with the following command:\nkubectl -n istio-system get pods If all these Pods are Running, it means Merbridge is successfully installed.\nVerify connection Use the following methods to check the connectivity of Merbridge:\nInstall sleep and helloworld and wait for a full start kubectl label ns default istio-injection=enabled kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/sleep/sleep.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/helloworld/helloworld.yaml Conduct curl test kubectl exec $(kubectl get po -l app=sleep -o=jsonpath='{..metadata.name}') -c sleep -- curl -s -v helloworld:5000/hello If you see words like * Connected to helloworld (127.128.0.1) port 5000 (#0) in the output, it means Merbridge has managed to replace iptables with eBPF for traffic forwarding.\n","categories":"","description":"This page helps you quickly get started with Merbridge.\n","excerpt":"This page helps you quickly get started with Merbridge.\n","ref":"/docs/getting-started/","tags":"","title":"Quick Start"},{"body":"先决条件  系统的内核版本应大于等于 5.7，可以使用 uname -r 查看。 系统应开启 cgroup2，可以通过 mount | grep cgroup2 进行验证。  安装 目前支持在 Istio 和 Linkerd2 环境下安装 Merbridge。\nIstio 环境 只需要在环境中执行以下命令即可安装 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml Linkerd2 环境 只需要在环境中执行以下命令即可安装 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-linkerd.yaml Kuma 环境 只需要在环境中执行以下命令即可安装 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one-kuma.yaml 验证 验证安装 在验证 Merbridge 是否能正常工作之前，需要先确保 Merbridge 的 Pod 都运行正常。以 Istio 为例，可以使用以下命令查看 Merbridge 的 Pod 状态：\nkubectl -n istio-system get pods 当 Merbridge 相关的所有 Pod 都处于 Running 状态时，表明 Merbridge 已经安装成功。\n连接测试 可以按照如下方案验证 Merbridge 的连接是否正常：\n安装 sleep 和 helloworld 应用并等待其完全启动 kubectl label ns default istio-injection=enabled kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/sleep/sleep.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/helloworld/helloworld.yaml 执行 curl 测试 kubectl exec $(kubectl get po -l app=sleep -o=jsonpath='{..metadata.name}') -c sleep -- curl -s -v helloworld:5000/hello 如果在结果中看到类似 * Connected to helloworld (127.128.0.1) port 5000 (#0) 的字样，表明 Merbridge 已经成功使用 eBPF 代替 iptables 进行流量转发。\n","categories":"","description":"本文将帮助您快速开始使用 Merbridge\n","excerpt":"本文将帮助您快速开始使用 Merbridge\n","ref":"/zh/docs/getting-started/","tags":"","title":"快速开始"},{"body":"eBPF The full name of eBPF is Extended Berkeley Packet Filter. As the name implies, this is a module used to filter network packets. For example, sockops and redir capabilities of eBPF can efficiently filter and intercept packets.\neBPF is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in an operating system kernel. It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules.\niptables iptables is a traffic filter built on netfilter. It implements traffic filtering and interception by registering hook functions on the mount point of netfilter. From the name of iptables, we can guess it may contain some tables. In practice, by mounting rule tables on different chains of netfilter, iptables can filter or modify the traffic packets entering and leaving the kernel protocol stack.\niptables has 4 tables by default:\n Filter NAT Raw Mangle  iptables has 5 chains by default:\n INPUT chain (ingress rules) OUTPUT chain (egress rules) FORWARD chain (rules of forwarding) PREROUTING chain (rules before routing) POSTROUTING chain (rules after routing)  Service Mesh A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. A service mesh can guarantee fast, reliable, and secure communication between containerized application infrastructure services. Key capabilities provided by mesh include service discovery, load balancing, secure encryption and authentication, failover, observability, and more. A service mesh typically injects a sidecar proxy into each service instance. These sidecars handle inter-service communication, monitoring, and security. In this way, developers can focus on the development, support, and maintenance of the application code in the service, while the O\u0026M team is responsible for the maintenance of the service mesh and applications. Today, the most well-known service mesh is Istio.\nIstio Istio is a service mesh technology originally open sourced by IBM, Google, and Lyft. It can be layered transparently onto distributed applications and provides all the benefits of a service mesh, such as traffic governance, security, and observability.\nIstio can adapt to all services hosted with on-premises, cloud, Kubernetes containers, and virtual machines. It is typically used with microservices deployed on a Kubernetes platform.\nFundamentally, Istio works by deploying an extended version of Envoy as a sidecar proxy to each microservice. The proxy network it uses forms a data plane of Istio. The configuration and management of these proxies is done in a control plane, providing discovery, configuration, and certificate management for Envoy proxies in the data plane.\nLinkerd Linkerd is the first service mesh launched on the market, but Istio is more popular today.\nLinkerd is an open source, ultra-lightweight service mesh designed by Buoyant for Kubernetes. It is completely rewritten in Rust, which makes it as small, light and safe as possible. It provides runtime debugging, observability, reliability, and safety without code changes in distributed applications.\nLinkerd has three basic components: UI, data plane, and control plane. Linkerd works by installing a set of ultra-light, transparent proxies next to each service instance that automatically handle all traffic to and from the service.\n","categories":"","description":"This page describes some key concepts about Merbridge.\n","excerpt":"This page describes some key concepts about Merbridge.\n","ref":"/docs/concepts/","tags":"","title":"Concepts"},{"body":"eBPF eBPF 全称为 Extended Berkeley Packet Filter，顾名思义，这是一个用来过滤网络数据包的模块。例如 eBPF 的 sockops 和 redir 能力，就可以高效地过滤和拦截数据包。\neBPF 是一项起源于 Linux 内核的革命性技术，可以在操作系统的内核中运行沙盒程序，能够安全、有效地扩展 Linux 内核的功能，无需改变内核的源代码，也无需加载内核模块。\niptables iptables 是建立在 netfilter 之上的流量过滤器，通过向 netfilter 的挂载点上注册钩子函数来实现对流量过滤和拦截。从 iptables 这个名字上可以看出有表的概念，iptables 通过把这些规则表挂载在 netfilter 的不同链上，对进出内核协议栈的流量数据包进行过滤或者修改。\niptables 默认有 4 个表：\n Filter 表（数据过滤表） NAT 表（地址转换表） Raw 表（状态跟踪表） Mangle 表（包标记表）  iptables 默认有 5 个链：\n INPUT 链（入站规则） OUTPUT 链（出站规则） FORWARD 链（转发规则） PREROUTING 链（路由前规则） POSTROUTING 链（路由后规则）  Service Mesh 中文名为服务网格，这是一个可配置的低延迟基础设施层，通过 API 接口处理应用服务之间的网络进程间通信。服务网格能确保容器化应用基础结构服务之间的通信快速、可靠和安全。网格提供的关键功能包括服务发现、负载均衡、安全加密和身份验证、故障恢复、可观测性等。 服务网格通常会为每个服务实例注入一个 Sidcar 的代理实例。这些 Sidcar 会处理服务间的通信、监控和安全等问题。这样，开发人员就可以专注于服务中应用代码的开发、支持和维护，而运维团队负责服务网格以及应用的维护工作。\n目前最著名的服务网格架构是 Istio。\nIstio Istio 是最初由 IBM、Google 和 Lyft 开源的服务网格技术。它可以透明地分层到分布式应用上，并提供服务网格的所有优点，例如流量治理、安全性和可观测性等。\nIstio 能够适配本地部署、云托管、Kubernetes 容器以及虚拟机上运行的服务程序。通常与 Kubernetes 平台上部署的微服务一起使用。\n从根本上讲，Istio 的工作原理是以 Sidcar 的形式将 Envoy 的扩展版本作为代理布署到每个微服务中。其使用的代理网络构成了 Istio 的数据平面。而这些代理的配置和管理在控制平面完成，为数据平面中的 Envoy 代理提供发现、配置和证书管理。\nLinkerd Linkerd 是市场上出现的第一个服务网格。\nLinkerd 是 Buoyant 为 Kubernetes 设计的开源、超轻量级的服务网格。用 Rust 语言完全重写，使其尽可能小、轻和安全，它提供了运行时调试、可观测性、可靠性和安全性，而无需在分布式应用中更改代码。\nLinkerd 有三个基本组件：UI、数据平面和控制平面。Linkerd 通过在每个服务实例旁安装一组超轻、透明的代理来工作，这些代理会自动处理进出服务的所有流量。\n","categories":"","description":"本文介绍 Merbridge 项目中的一些关键概念\n","excerpt":"本文介绍 Merbridge 项目中的一些关键概念\n","ref":"/zh/docs/concepts/","tags":"","title":"概念"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/2023/","tags":"","title":"2023 年 Blog"},{"body":"Merbridge is currently hosted on GitHub as an open source project. All code-related issues are managed on GitHub.\nIf you have any questions or suggestions about Merbridge, please create a New Issue on GitHub. We will review and process it as soon as possible.\nIf you find a bug in Merbridge and are interested in helping us fix it, you are more than welcome to share your fix code by creating a Pull Request. We will review and process it as soon as possible.\n","categories":"","description":"This page helps you make contributions to Merbridge.\n","excerpt":"This page helps you make contributions to Merbridge.\n","ref":"/docs/contribution-guidelines/","tags":"","title":"Make Contributions"},{"body":"Merbridge 目前托管在 GitHub 上进行开源，所有与代码相关的事情都在 GitHub 上进行管理。\n如果您对 Merbridge 有疑问，需要我们帮忙解决问题，或者想要提供一些新的功能，可以在 GitHub 上创建新的 Issue，我们会及时查看并处理。\n如果您发现了 Merbridge 的 bug，并且有兴趣帮助我们修复，那么非常欢迎您提交 Pull Request，附带上您的修复代码，我们会及时处理您的 PR。\n","categories":"","description":"本文介绍参与 Merbridge 项目的方式\n","excerpt":"本文介绍参与 Merbridge 项目的方式\n","ref":"/zh/docs/contribution-guidelines/","tags":"","title":"参与贡献"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/2022/","tags":"","title":"2022 年 Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2022/","tags":"","title":"Blogs of 2022"},{"body":" Get all information you need to know about Merbridge.\n Merbridge is eBPF-based and can accelerate the data plane of service meshes with a shorter packet datapath than iptables.\n","categories":"","description":"","excerpt":" Get all information you need to know about Merbridge.\n Merbridge is …","ref":"/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/releases/","tags":"","title":"Version Release"},{"body":" 帮助您了解如何使用 Merbridge\n Merbridge 旨在使用 eBPF 代替 iptables 技术，加速服务网格的数据平面。\n","categories":"","description":"","excerpt":" 帮助您了解如何使用 Merbridge\n Merbridge 旨在使用 eBPF 代替 iptables 技术，加速服务网格的数据平面。\n","ref":"/zh/docs/","tags":"","title":"文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/releases/","tags":"","title":"版本发布"},{"body":"We are pleased to announce the release of Merbridge 0.7.0!\nThis release mainly includes two updates as below:\n  Replaced XDP with tc (Traffic Control) to handle the ingress/egress traffic. This replacement comes from a situation that the XDP Generic mode has some problems and is not recommended to use in a production environment.\n  Merbridge now supports Kuma, a universal Envoy service mesh. Similar to Istio, you can use all capabilities provided by Merbridge when you are working in the Kuma environment. Thanks @bartsmykla, an excellent engineer from Kuma.\n  For release notes, see Merbridge 0.7.0.\n","categories":"","description":"What is new in version 0.7.0?\n","excerpt":"What is new in version 0.7.0?\n","ref":"/blog/2022/07/20/release-0.7.0/","tags":"","title":"Release 0.7.0"},{"body":"我们很高兴地发布 Merbridge 0.7.0！\n本次更新主要体现在两个方面：\n 使用 tc（Traffic Control）代替 XDP 来做容器的出入口流量处理，规避 XDP Generic 模式存在的问题，并且可用于生产环境。 增加对 Kuma 的支持，可以在 Kuma 中使用和 Istio 相同的能力。感谢 Kuma 工程师 @bartsmykla 的贡献。  有关其他更新内容，请参考：Merbridge 0.7.0\n","categories":"","description":"0.7.0 版本更新内容。\n","excerpt":"0.7.0 版本更新内容。\n","ref":"/zh/blog/2022/07/20/release-0.7.0/","tags":"","title":"发布 0.7.0"},{"body":"We are pleased to announce the release of Merbridge 0.6.0!\nIn this release, we have introduced CNI mode for the first time to support the capabilities of forwarding all Istio traffic. For details about the CNI mode support, see Merbridge CNI Mode\nFor release notes, see: Merbridge 0.6.0\n","categories":"","description":"What is new in Version 0.6.0?\n","excerpt":"What is new in Version 0.6.0?\n","ref":"/blog/2022/05/23/release-0.6.0/","tags":"","title":"Release 0.6.0"},{"body":"我们很高兴地发布 Merbridge 0.6.0！\n在这个版本中，我们首次引入了 CNI 模式，用于支持全量的 Istio 流量转发相关的能力。有关 CNI 模式的支持功能，请参考 Merbridge CNI 模式\n有关版本发布说明，请参考：Merbridge 0.6.0\n","categories":"","description":"0.6.0 版本更新内容。\n","excerpt":"0.6.0 版本更新内容。\n","ref":"/zh/blog/2022/05/23/release-0.6.0/","tags":"","title":"发布 0.6.0"},{"body":"Added  Support passive sockops. (#77) @kebe7jun . Use ingress path for message redirection and forwarding. (#82) @dddddai . Support helm-based deployment of Merbridge (#65) @Xunzhuo .  Fixed  Fix the key size of cookie_original_dst(#75) @dddddai.  ","categories":"","description":"What is new in Version 0.5.0?\n","excerpt":"What is new in Version 0.5.0?\n","ref":"/blog/2022/03/30/release-0.5.0/","tags":"","title":"Release 0.5.0"},{"body":"新增  增加对 passive sockops 的支持。 (#77) @kebe7jun . 使用 ingress path 进行消息重定向转发(#82) @dddddai . 支持使用 helm 部署 Merbridge (#65) @Xunzhuo .  修复  修复 cookie_original_dst 的 key 大小(#75) @dddddai .  ","categories":"","description":"0.5.0 版本更新内容。\n","excerpt":"0.5.0 版本更新内容。\n","ref":"/zh/blog/2022/01/27/release-0.5.0/","tags":"","title":"Release 0.5.0"},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  Merbridge Accelerate your mesh with eBPF        The service mesh technology represented by Istio is attracting more and more attention from enterprises. It uses sidecars and iptables to intercept traffic at the price of performance. Over the past two years, eBPF has become a trending technology, generating many projects based on eBPF.\n    eBPF’s sockops and redir capabilities allow more efficient processing of packets. Therefore, it is possible to accelerate the mesh with eBPF instead of iptables.      Merbridge is an eBPF-based open source project. It can help accelerate your meshes with eBPF by applying just one command in your Istio-managed cluster.     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/about/","tags":"","title":"Merbridge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Merbridge Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/blog/","tags":"","title":"Merbridge Blog"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_top.jpg); } }  Merbridge: use eBPF to accelerate your mesh Learn More   Get Started   Currently supported Istio, Linkerd2, and Kuma.\n         Merbridge replaces iptables rules with eBPF to intercept traffic. It also combines msg_redirect to reduce latency with a shortened datapath between sidecars and services.\nFor details, see: Blog.\n      Documentation  If you have any problems in using Merbridge, Documentation may offer a quick solution.\n   Contribution  You can create a PR on GitHub to improve Merbridge. Your contribution is a big help！\nRead more …\n   Implementation  If you are interested in details of Merbridge, Blog may tell you more.\nRead more …\n     Merbridge is a Cloud Native Computing Foundation sandbox project    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"Merbridge"},{"body":"  #td-cover-block-0 { background-image: url(/zh/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_top.jpg); } }  Merbridge：使用 eBPF 加速服务网格，提高服务性能 了解更多   快速开始   支持 Istio、Linkerd2 和 Kuma 等环境\n         Merbridge 在服务网格中使用 eBPF 技术代替 iptables，实现流量拦截。借助 eBPF 和 msg_redirect 技术，Merbridge 可以提高 Sidecar 和应用之间的传输速度，降低延迟。\n如需了解更多实现细节或者原理，请参阅 Blog\n      查看文档  如您在使用 Merbridge 时遇到任何问题，请参阅文档\n   欢迎贡献！  欢迎您在 GitHub 上提交 PR 贡献自己的聪明才智。我们一直期待您的加入！\n更多 …\n   实现原理!  有关 Merbridge 的实现细节，请参阅博客\n更多 …\n     Merbridge 是一个云原生基金会 (CNCF) 沙箱项目    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/","tags":"","title":"Merbridge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Result"},{"body":"  #td-cover-block-0 { background-image: url(/zh/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/about/featured-background_hub62e3d9f0a13001faca0b2843f77d622_262863_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  Merbridge 使用 eBPF 技术代替 iptables 加速服务网格        以 Istio 为首的服务网格技术正在被越来越多的企业所关注。服务网格目前使用 Sidecar 借助 iptables 技术实现流量拦截，但是这样会损失不少性能。近两年，由于 eBPF 技术的兴起，不少围绕 eBPF 的项目应声而出。\n    借助 eBPF 的 sockops 和 redir 能力，可以高效地处理数据包。因此，可以结合实际场景，使用 eBPF 代替 iptables 为 Istio 实现加速。      现在，我们开源了 Merbridge 项目。您只需在 Istio 集群执行一条命令，即可直接使用 eBPF 代替 iptables 实现网络加速！     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/about/","tags":"","title":"关于 Merbridge"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/search/","tags":"","title":"搜索结果"}]